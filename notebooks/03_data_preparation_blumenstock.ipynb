{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Data Preparation: Blumenstock et al. (2022) Replication\n",
    "\n",
    "This notebook prepares data following the methodology from:\n",
    "\n",
    "> Blumenstock, G., Lessmann, S., & Seow, H-V. (2022). Deep learning for survival and competing risk modelling. *Journal of the Operational Research Society*, 73(1), 26-38.\n",
    "\n",
    "## Dataset 2: Post-Crisis Period (2010-2025)\n",
    "\n",
    "**Variables from Table 2:**\n",
    "\n",
    "### Loan-Level Variables (9)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `int_rate` | Initial interest rate |\n",
    "| `orig_upb` | Original unpaid balance |\n",
    "| `fico_score` | Initial FICO score |\n",
    "| `dti_r` | Initial debt-to-income ratio |\n",
    "| `ltv_r` | Initial loan-to-value ratio |\n",
    "| `bal_repaid` | Current repaid balance in percent |\n",
    "| `t_act_12m` | No. of times not being delinquent in last 12 months |\n",
    "| `t_del_30d_12m` | No. of times being 30 days delinquent in last 12 months |\n",
    "| `t_del_60d_12m` | No. of times being 60 days delinquent in last 12 months |\n",
    "\n",
    "### Macroeconomic Variables (13 for Dataset 2)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `hpi_st_d_t_o` | HPI difference between origination and today (state) |\n",
    "| `ppi_c_FRMA` | Current prepayment incentive |\n",
    "| `TB10Y_d_t_o` | Treasury rate difference |\n",
    "| `FRMA30Y_d_t_o` | 30Y FRM difference |\n",
    "| `ppi_o_FRMA` | Prepayment incentive at origination |\n",
    "| `hpi_st_log12m` | HPI 12-month log return (state) |\n",
    "| `hpi_r_st_us` | Ratio of state HPI to national HPI |\n",
    "| `st_unemp_r12m` | Unemployment 12-month log return (state) |\n",
    "| `st_unemp_r3m` | Unemployment 3-month log return (state) |\n",
    "| `TB10Y_r12m` | Treasury rate 12-month return |\n",
    "| `T10Y3MM` | Yield spread (10Y - 3M) |\n",
    "| `T10Y3MM_r12m` | Yield spread 12-month return |\n",
    "\n",
    "### Event Definitions\n",
    "- **Default (k=2)**: Loan turning 3-month delinquent for the first time\n",
    "- **Prepayment (k=1)**: Loan repaid completely and unexpectedly\n",
    "- **Censored (k=0)**: Active loan without event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import column definitions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.data.columns import (\n",
    "    ORIGINATION_COLUMNS, ORIGINATION_DTYPES,\n",
    "    PERFORMANCE_COLUMNS, PERFORMANCE_DTYPES,\n",
    "    ZERO_BALANCE_CODE_MAP\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following the paper's Dataset 2 setup (2010-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "EXTERNAL_DATA_DIR = Path('../data/external')\n",
    "\n",
    "# Dataset 2: Post-crisis period (2010-2025)\n",
    "VINTAGES = list(range(2010, 2026))\n",
    "\n",
    "# Sampling strategy from paper:\n",
    "# - 11 random subsamples of 10,000 each\n",
    "# - 10 for cross-validation, 1 for hyperparameter tuning\n",
    "SAMPLE_SIZE_PER_FOLD = 10000\n",
    "N_FOLDS = 11\n",
    "\n",
    "# Default definition: 3-month delinquent for the first time\n",
    "DEFAULT_DELINQUENCY_THRESHOLD = 3\n",
    "\n",
    "print(f\"Dataset 2 Period: {min(VINTAGES)}-{max(VINTAGES)}\")\n",
    "print(f\"Sample size per fold: {SAMPLE_SIZE_PER_FOLD:,}\")\n",
    "print(f\"Number of folds: {N_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-macro-header",
   "metadata": {},
   "source": [
    "## Step 1: Load Macroeconomic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load national macro data\n",
    "macro_national = pd.read_parquet(EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet')\n",
    "macro_national.index.name = 'date'\n",
    "macro_national = macro_national.reset_index()\n",
    "macro_national['date'] = pd.to_datetime(macro_national['date'])\n",
    "macro_national['year_month'] = macro_national['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate additional variables needed for paper\n",
    "# TB10Y_r12m: 10-year treasury rate 12-month return\n",
    "macro_national['TB10Y_r12m'] = macro_national['DGS10'].pct_change(12)\n",
    "\n",
    "# T10Y3MM: Yield spread (need 3-month rate)\n",
    "# Using T10Y2Y as proxy if 3M not available\n",
    "if 'DGS3MO' in macro_national.columns:\n",
    "    macro_national['T10Y3MM'] = macro_national['DGS10'] - macro_national['DGS3MO']\n",
    "else:\n",
    "    # Calculate from T10Y2Y and DGS2\n",
    "    macro_national['T10Y3MM'] = macro_national['T10Y2Y'] + macro_national['DGS2'] - macro_national['DGS10']\n",
    "    # Fallback to just using T10Y2Y\n",
    "    if macro_national['T10Y3MM'].isna().all():\n",
    "        macro_national['T10Y3MM'] = macro_national['T10Y2Y']\n",
    "\n",
    "# T10Y3MM_r12m: Yield spread 12-month return\n",
    "macro_national['T10Y3MM_r12m'] = macro_national['T10Y3MM'].diff(12)\n",
    "\n",
    "print(f\"National macro data: {macro_national.shape}\")\n",
    "print(f\"Date range: {macro_national['date'].min()} to {macro_national['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-state-unemp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state-level unemployment\n",
    "state_unemp = pd.read_parquet(EXTERNAL_DATA_DIR / 'state_unemployment.parquet')\n",
    "state_unemp.index.name = 'date'\n",
    "state_unemp = state_unemp.reset_index()\n",
    "state_unemp['date'] = pd.to_datetime(state_unemp['date'])\n",
    "state_unemp['year_month'] = state_unemp['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate returns for each state\n",
    "state_unemp_returns = state_unemp.copy()\n",
    "for col in state_unemp.columns:\n",
    "    if '_unemployment' in col:\n",
    "        state = col.replace('_unemployment', '')\n",
    "        # 12-month log return\n",
    "        state_unemp_returns[f'{state}_unemp_r12m'] = np.log(\n",
    "            state_unemp[col] / state_unemp[col].shift(12)\n",
    "        )\n",
    "        # 3-month log return\n",
    "        state_unemp_returns[f'{state}_unemp_r3m'] = np.log(\n",
    "            state_unemp[col] / state_unemp[col].shift(3)\n",
    "        )\n",
    "\n",
    "# Melt to long format\n",
    "state_cols = [c for c in state_unemp.columns if '_unemployment' in c]\n",
    "state_unemp_long = state_unemp.melt(\n",
    "    id_vars=['date', 'year_month'],\n",
    "    value_vars=state_cols,\n",
    "    var_name='state_col',\n",
    "    value_name='state_unemployment'\n",
    ")\n",
    "state_unemp_long['property_state'] = state_unemp_long['state_col'].str.replace('_unemployment', '')\n",
    "\n",
    "# Add returns\n",
    "for col in [c for c in state_unemp_returns.columns if '_unemp_r12m' in c or '_unemp_r3m' in c]:\n",
    "    state = col.split('_')[0]\n",
    "    return_type = 'st_unemp_r12m' if 'r12m' in col else 'st_unemp_r3m'\n",
    "    temp = state_unemp_returns[['year_month', col]].copy()\n",
    "    temp['property_state'] = state\n",
    "    temp = temp.rename(columns={col: return_type})\n",
    "    state_unemp_long = state_unemp_long.merge(\n",
    "        temp, on=['year_month', 'property_state'], how='left'\n",
    "    )\n",
    "\n",
    "state_unemp_long = state_unemp_long[['year_month', 'property_state', 'state_unemployment', \n",
    "                                      'st_unemp_r12m', 'st_unemp_r3m']].drop_duplicates()\n",
    "\n",
    "print(f\"State unemployment: {state_unemp_long.shape}\")\n",
    "print(state_unemp_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-state-hpi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state-level HPI\n",
    "state_hpi = pd.read_parquet(EXTERNAL_DATA_DIR / 'state_hpi.parquet')\n",
    "state_hpi.index.name = 'date'\n",
    "state_hpi = state_hpi.reset_index()\n",
    "state_hpi['date'] = pd.to_datetime(state_hpi['date'])\n",
    "state_hpi['year_month'] = state_hpi['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate 12-month log return for each state\n",
    "for col in state_hpi.columns:\n",
    "    if '_hpi' in col and 'yoy' not in col:\n",
    "        state = col.replace('_hpi', '')\n",
    "        state_hpi[f'{state}_hpi_log12m'] = np.log(\n",
    "            state_hpi[col] / state_hpi[col].shift(12)\n",
    "        )\n",
    "\n",
    "# Calculate national HPI (average across states or use FHFA national)\n",
    "hpi_cols = [c for c in state_hpi.columns if c.endswith('_hpi') and len(c) <= 6]\n",
    "state_hpi['national_hpi'] = state_hpi[hpi_cols].mean(axis=1)\n",
    "\n",
    "# Melt to long format\n",
    "state_hpi_long = state_hpi.melt(\n",
    "    id_vars=['date', 'year_month', 'national_hpi'],\n",
    "    value_vars=hpi_cols,\n",
    "    var_name='state_col',\n",
    "    value_name='state_hpi'\n",
    ")\n",
    "state_hpi_long['property_state'] = state_hpi_long['state_col'].str.replace('_hpi', '')\n",
    "\n",
    "# Add log returns\n",
    "log_cols = [c for c in state_hpi.columns if '_hpi_log12m' in c]\n",
    "for col in log_cols:\n",
    "    state = col.replace('_hpi_log12m', '')\n",
    "    temp = state_hpi[['year_month', col]].copy()\n",
    "    temp['property_state'] = state\n",
    "    temp = temp.rename(columns={col: 'hpi_st_log12m'})\n",
    "    state_hpi_long = state_hpi_long.merge(\n",
    "        temp, on=['year_month', 'property_state'], how='left'\n",
    "    )\n",
    "\n",
    "# Calculate ratio of state HPI to national HPI\n",
    "state_hpi_long['hpi_r_st_us'] = state_hpi_long['state_hpi'] / state_hpi_long['national_hpi']\n",
    "\n",
    "state_hpi_long = state_hpi_long[['year_month', 'property_state', 'state_hpi', 'national_hpi',\n",
    "                                  'hpi_st_log12m', 'hpi_r_st_us']].drop_duplicates()\n",
    "\n",
    "print(f\"State HPI: {state_hpi_long.shape}\")\n",
    "print(state_hpi_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-loans-header",
   "metadata": {},
   "source": [
    "## Step 2: Load and Process Loan Data\n",
    "\n",
    "Process Freddie Mac data with paper's variable definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_origination_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load origination data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_orig_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=ORIGINATION_COLUMNS,\n",
    "        dtype=ORIGINATION_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_performance_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load performance (monthly) data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_svcg_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=PERFORMANCE_COLUMNS,\n",
    "        dtype=PERFORMANCE_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Load functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vintage_blumenstock(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a vintage following Blumenstock et al. (2022) methodology.\n",
    "    \n",
    "    Creates loan-level survival data with:\n",
    "    - Terminal record per loan\n",
    "    - Behavioral features from last 12 months\n",
    "    - Paper's variable definitions\n",
    "    \"\"\"\n",
    "    print(f\"Processing vintage {vintage}...\")\n",
    "    \n",
    "    # Load data\n",
    "    orig_df = load_origination_data(vintage)\n",
    "    perf_df = load_performance_data(vintage)\n",
    "    \n",
    "    if orig_df.empty or perf_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse reporting period\n",
    "    perf_df['reporting_date'] = pd.to_datetime(\n",
    "        perf_df['monthly_reporting_period'].astype(str), format='%Y%m'\n",
    "    )\n",
    "    perf_df['year_month'] = perf_df['reporting_date'].dt.to_period('M')\n",
    "    \n",
    "    # Parse delinquency status\n",
    "    perf_df['delinquency_status'] = pd.to_numeric(\n",
    "        perf_df['current_loan_delinquency_status'].replace({'X': '0', 'XX': '0'}),\n",
    "        errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    # Sort by loan and time\n",
    "    perf_df = perf_df.sort_values(['loan_sequence_number', 'loan_age'])\n",
    "    \n",
    "    # === Calculate behavioral variables (rolling 12-month) ===\n",
    "    perf_df['is_current'] = (perf_df['delinquency_status'] == 0).astype(int)\n",
    "    perf_df['is_30d_del'] = (perf_df['delinquency_status'] == 1).astype(int)\n",
    "    perf_df['is_60d_del'] = (perf_df['delinquency_status'] == 2).astype(int)\n",
    "    \n",
    "    # Rolling counts over last 12 months\n",
    "    grouped = perf_df.groupby('loan_sequence_number')\n",
    "    perf_df['t_act_12m'] = grouped['is_current'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_30d_12m'] = grouped['is_30d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_60d_12m'] = grouped['is_60d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    \n",
    "    # === Determine event type ===\n",
    "    # Default: first time reaching 90+ days delinquent\n",
    "    perf_df['is_default'] = (perf_df['delinquency_status'] >= DEFAULT_DELINQUENCY_THRESHOLD).astype(int)\n",
    "    perf_df['first_default'] = grouped['is_default'].transform(\n",
    "        lambda x: (x.cumsum() == 1) & (x == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Prepayment: zero balance code = 01 (not at maturity)\n",
    "    perf_df['is_prepay'] = (\n",
    "        (perf_df['zero_balance_code'] == '01') & \n",
    "        (perf_df['loan_age'] < perf_df['remaining_months_to_maturity'].fillna(360) + perf_df['loan_age'] - 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # === Calculate balance repaid ===\n",
    "    orig_upb = orig_df.set_index('loan_sequence_number')['orig_upb']\n",
    "    perf_df['orig_upb_lookup'] = perf_df['loan_sequence_number'].map(orig_upb)\n",
    "    perf_df['bal_repaid'] = (\n",
    "        (perf_df['orig_upb_lookup'] - perf_df['current_actual_upb'].fillna(0)) / \n",
    "        perf_df['orig_upb_lookup']\n",
    "    ) * 100\n",
    "    perf_df['bal_repaid'] = perf_df['bal_repaid'].clip(0, 100)\n",
    "    \n",
    "    # === Get terminal record for each loan ===\n",
    "    # Determine event at terminal record\n",
    "    def get_terminal_event(group):\n",
    "        \"\"\"Get terminal record with event type.\"\"\"\n",
    "        last_row = group.iloc[-1].copy()\n",
    "        \n",
    "        # Check for default (first 90+ delinquency)\n",
    "        default_rows = group[group['first_default'] == 1]\n",
    "        if len(default_rows) > 0:\n",
    "            last_row = default_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 2  # Default\n",
    "            return last_row\n",
    "        \n",
    "        # Check for prepayment\n",
    "        prepay_rows = group[group['is_prepay'] == 1]\n",
    "        if len(prepay_rows) > 0:\n",
    "            last_row = prepay_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 1  # Prepay\n",
    "            return last_row\n",
    "        \n",
    "        # Censored\n",
    "        last_row['event_code'] = 0\n",
    "        return last_row\n",
    "    \n",
    "    print(f\"  Getting terminal records...\")\n",
    "    terminal_df = perf_df.groupby('loan_sequence_number').apply(get_terminal_event)\n",
    "    terminal_df = terminal_df.reset_index(drop=True)\n",
    "    \n",
    "    # === Merge with origination data ===\n",
    "    orig_cols = [\n",
    "        'loan_sequence_number', 'credit_score', 'orig_ltv', 'orig_dti',\n",
    "        'orig_upb', 'orig_interest_rate', 'orig_loan_term',\n",
    "        'first_payment_date', 'property_state'\n",
    "    ]\n",
    "    orig_subset = orig_df[[c for c in orig_cols if c in orig_df.columns]].copy()\n",
    "    orig_subset['vintage_year'] = vintage\n",
    "    \n",
    "    # Parse origination date\n",
    "    orig_subset['first_payment_date'] = pd.to_datetime(\n",
    "        orig_subset['first_payment_date'].astype(str), format='%Y%m', errors='coerce'\n",
    "    )\n",
    "    orig_subset['orig_year_month'] = orig_subset['first_payment_date'].dt.to_period('M')\n",
    "    \n",
    "    terminal_df = terminal_df.merge(orig_subset, on='loan_sequence_number', how='left')\n",
    "    \n",
    "    print(f\"  Loans: {len(terminal_df):,}\")\n",
    "    print(f\"  Events: Prepay={sum(terminal_df['event_code']==1):,}, \"\n",
    "          f\"Default={sum(terminal_df['event_code']==2):,}, \"\n",
    "          f\"Censored={sum(terminal_df['event_code']==0):,}\")\n",
    "    \n",
    "    return terminal_df\n",
    "\n",
    "\n",
    "print(\"Process function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all vintages in Dataset 2\n",
    "all_loans = []\n",
    "\n",
    "for vintage in VINTAGES:\n",
    "    df = process_vintage_blumenstock(vintage)\n",
    "    if not df.empty:\n",
    "        all_loans.append(df)\n",
    "\n",
    "# Combine\n",
    "print(\"\\nCombining all vintages...\")\n",
    "loans_df = pd.concat(all_loans, ignore_index=True)\n",
    "print(f\"Total loans: {len(loans_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-macro-header",
   "metadata": {},
   "source": [
    "## Step 3: Merge Macroeconomic Variables\n",
    "\n",
    "Add paper's macroeconomic variables at the observation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge state unemployment data\n",
    "print(\"Merging state unemployment...\")\n",
    "loans_df = loans_df.merge(\n",
    "    state_unemp_long,\n",
    "    on=['year_month', 'property_state'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"  Coverage: {loans_df['st_unemp_r12m'].notna().mean():.1%}\")\n",
    "\n",
    "# Merge state HPI data  \n",
    "print(\"Merging state HPI...\")\n",
    "loans_df = loans_df.merge(\n",
    "    state_hpi_long,\n",
    "    on=['year_month', 'property_state'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"  Coverage: {loans_df['hpi_st_log12m'].notna().mean():.1%}\")\n",
    "\n",
    "# Merge national macro data\n",
    "print(\"Merging national macro...\")\n",
    "macro_cols = ['year_month', 'MORTGAGE30US', 'DGS10', 'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m']\n",
    "macro_subset = macro_national[[c for c in macro_cols if c in macro_national.columns]].copy()\n",
    "loans_df = loans_df.merge(macro_subset, on='year_month', how='left')\n",
    "print(f\"  Coverage: {loans_df['MORTGAGE30US'].notna().mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-orig-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get origination-time values for difference calculations\n",
    "print(\"Calculating origination-time differences...\")\n",
    "\n",
    "# Get origination-time state HPI\n",
    "orig_hpi = state_hpi_long[['year_month', 'property_state', 'state_hpi']].rename(\n",
    "    columns={'year_month': 'orig_year_month', 'state_hpi': 'orig_state_hpi'}\n",
    ")\n",
    "loans_df = loans_df.merge(orig_hpi, on=['orig_year_month', 'property_state'], how='left')\n",
    "\n",
    "# hpi_st_d_t_o: Difference of HPI between origination and today (state-level)\n",
    "loans_df['hpi_st_d_t_o'] = loans_df['state_hpi'] - loans_df['orig_state_hpi']\n",
    "\n",
    "# Get origination-time macro rates\n",
    "orig_macro = macro_national[['year_month', 'MORTGAGE30US', 'DGS10']].rename(\n",
    "    columns={'year_month': 'orig_year_month', 'MORTGAGE30US': 'orig_MORTGAGE30US', 'DGS10': 'orig_DGS10'}\n",
    ")\n",
    "loans_df = loans_df.merge(orig_macro, on='orig_year_month', how='left')\n",
    "\n",
    "# ppi_c_FRMA: Current prepayment incentive\n",
    "loans_df['ppi_c_FRMA'] = loans_df['orig_interest_rate'] - loans_df['MORTGAGE30US']\n",
    "\n",
    "# ppi_o_FRMA: Prepayment incentive at origination\n",
    "loans_df['ppi_o_FRMA'] = loans_df['orig_interest_rate'] - loans_df['orig_MORTGAGE30US']\n",
    "\n",
    "# TB10Y_d_t_o: Difference of 10-year treasury rate\n",
    "loans_df['TB10Y_d_t_o'] = loans_df['DGS10'] - loans_df['orig_DGS10']\n",
    "\n",
    "# FRMA30Y_d_t_o: Difference of 30-year FRM average\n",
    "loans_df['FRMA30Y_d_t_o'] = loans_df['MORTGAGE30US'] - loans_df['orig_MORTGAGE30US']\n",
    "\n",
    "print(\"Difference variables calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rename-header",
   "metadata": {},
   "source": [
    "## Step 4: Rename Variables to Paper's Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rename-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to paper's variable names\n",
    "rename_map = {\n",
    "    # Loan-level\n",
    "    'orig_interest_rate': 'int_rate',\n",
    "    'credit_score': 'fico_score',\n",
    "    'orig_dti': 'dti_r',\n",
    "    'orig_ltv': 'ltv_r',\n",
    "    # Duration\n",
    "    'loan_age': 'duration',\n",
    "}\n",
    "\n",
    "loans_df = loans_df.rename(columns=rename_map)\n",
    "\n",
    "# Define final variable sets (from paper Table 2)\n",
    "LOAN_LEVEL_VARS = [\n",
    "    'int_rate',           # Initial interest rate\n",
    "    'orig_upb',           # Original unpaid balance\n",
    "    'fico_score',         # Initial FICO score\n",
    "    'dti_r',              # Initial debt-to-income ratio\n",
    "    'ltv_r',              # Initial loan-to-value ratio\n",
    "    'bal_repaid',         # Current repaid balance in percent\n",
    "    't_act_12m',          # Times not delinquent in last 12 months\n",
    "    't_del_30d_12m',      # Times 30 days delinquent in last 12 months\n",
    "    't_del_60d_12m',      # Times 60 days delinquent in last 12 months\n",
    "]\n",
    "\n",
    "MACRO_VARS = [\n",
    "    'hpi_st_d_t_o',       # HPI difference (state)\n",
    "    'ppi_c_FRMA',         # Current prepayment incentive\n",
    "    'TB10Y_d_t_o',        # Treasury rate difference\n",
    "    'FRMA30Y_d_t_o',      # 30Y FRM difference\n",
    "    'ppi_o_FRMA',         # Prepayment incentive at origination\n",
    "    'hpi_st_log12m',      # HPI 12-month log return (state)\n",
    "    'hpi_r_st_us',        # Ratio of state HPI to national HPI\n",
    "    'st_unemp_r12m',      # Unemployment 12-month log return (state)\n",
    "    'st_unemp_r3m',       # Unemployment 3-month log return (state)\n",
    "    'TB10Y_r12m',         # Treasury rate 12-month return\n",
    "    'T10Y3MM',            # Yield spread (10Y - 3M)\n",
    "    'T10Y3MM_r12m',       # Yield spread 12-month return\n",
    "]\n",
    "\n",
    "ALL_VARS = LOAN_LEVEL_VARS + MACRO_VARS\n",
    "\n",
    "print(f\"Loan-level variables: {len(LOAN_LEVEL_VARS)}\")\n",
    "print(f\"Macro variables: {len(MACRO_VARS)}\")\n",
    "print(f\"Total: {len(ALL_VARS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variable coverage\n",
    "print(\"=== Variable Coverage ===\")\n",
    "for var in ALL_VARS:\n",
    "    if var in loans_df.columns:\n",
    "        coverage = loans_df[var].notna().mean()\n",
    "        print(f\"{var}: {coverage:.1%}\")\n",
    "    else:\n",
    "        print(f\"{var}: MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-header",
   "metadata": {},
   "source": [
    "## Step 5: Create Subsamples for Cross-Validation\n",
    "\n",
    "Following paper: 11 random subsamples of 10,000 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-complete",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to complete cases\n",
    "required_cols = ['duration', 'event_code'] + [v for v in ALL_VARS if v in loans_df.columns]\n",
    "loans_complete = loans_df.dropna(subset=[c for c in required_cols if c in loans_df.columns])\n",
    "\n",
    "print(f\"Complete cases: {len(loans_complete):,} / {len(loans_df):,} ({len(loans_complete)/len(loans_df):.1%})\")\n",
    "\n",
    "# Event distribution\n",
    "print(\"\\nEvent distribution:\")\n",
    "print(loans_complete['event_code'].value_counts().sort_index())\n",
    "print(\"\\n0=Censored, 1=Prepay, 2=Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified subsamples\n",
    "# Need to ensure each sample has both event types\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Filter to just prepay (1) and default (2) for stratification\n",
    "# Include some censored as well\n",
    "loans_for_sampling = loans_complete.copy()\n",
    "\n",
    "# Create 11 subsamples\n",
    "n_samples = min(N_FOLDS * SAMPLE_SIZE_PER_FOLD, len(loans_for_sampling))\n",
    "if n_samples < len(loans_for_sampling):\n",
    "    # Sample from the data\n",
    "    sampled_df = loans_for_sampling.sample(n=n_samples, random_state=42)\n",
    "else:\n",
    "    sampled_df = loans_for_sampling\n",
    "\n",
    "# Assign fold numbers\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "sampled_df['fold'] = sampled_df.index % N_FOLDS\n",
    "\n",
    "print(f\"Total sampled: {len(sampled_df):,}\")\n",
    "print(f\"\\nSamples per fold:\")\n",
    "print(sampled_df['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fold-events",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check event distribution per fold\n",
    "print(\"=== Event Distribution per Fold ===\")\n",
    "for fold in range(N_FOLDS):\n",
    "    fold_data = sampled_df[sampled_df['fold'] == fold]\n",
    "    prepay = (fold_data['event_code'] == 1).sum()\n",
    "    default = (fold_data['event_code'] == 2).sum()\n",
    "    censored = (fold_data['event_code'] == 0).sum()\n",
    "    print(f\"Fold {fold}: n={len(fold_data):,}, prepay={prepay:,}, default={default:,}, censored={censored:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Step 6: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns\n",
    "final_cols = [\n",
    "    # Identifiers\n",
    "    'loan_sequence_number', 'vintage_year', 'fold',\n",
    "    # Survival data\n",
    "    'duration', 'event_code',\n",
    "    # Loan-level variables\n",
    "] + [v for v in LOAN_LEVEL_VARS if v in sampled_df.columns] + [\n",
    "    # Macro variables  \n",
    "] + [v for v in MACRO_VARS if v in sampled_df.columns] + [\n",
    "    # Additional useful columns\n",
    "    'property_state', 'year_month'\n",
    "]\n",
    "\n",
    "# Remove duplicates and filter\n",
    "final_cols = list(dict.fromkeys(final_cols))\n",
    "final_cols = [c for c in final_cols if c in sampled_df.columns]\n",
    "\n",
    "final_df = sampled_df[final_cols].copy()\n",
    "print(f\"Final dataset: {len(final_df):,} rows, {len(final_cols)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "output_path = PROCESSED_DATA_DIR / 'blumenstock_dataset2.parquet'\n",
    "final_df.to_parquet(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Also save variable lists for reference\n",
    "var_config = {\n",
    "    'loan_level_vars': LOAN_LEVEL_VARS,\n",
    "    'macro_vars': MACRO_VARS,\n",
    "    'all_vars': ALL_VARS,\n",
    "}\n",
    "import json\n",
    "with open(PROCESSED_DATA_DIR / 'blumenstock_variables.json', 'w') as f:\n",
    "    json.dump(var_config, f, indent=2)\n",
    "print(\"Variable config saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BLUMENSTOCK DATASET 2 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPeriod: {final_df['vintage_year'].min()}-{final_df['vintage_year'].max()}\")\n",
    "print(f\"Total observations: {len(final_df):,}\")\n",
    "print(f\"Number of folds: {final_df['fold'].nunique()}\")\n",
    "\n",
    "print(f\"\\n=== Event Distribution ===\")\n",
    "event_counts = final_df['event_code'].value_counts().sort_index()\n",
    "for code, count in event_counts.items():\n",
    "    event_name = {0: 'Censored', 1: 'Prepayment', 2: 'Default'}.get(code, 'Other')\n",
    "    pct = count / len(final_df) * 100\n",
    "    print(f\"  {event_name} (k={code}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== Variable Summary ===\")\n",
    "print(f\"Loan-level variables: {len([v for v in LOAN_LEVEL_VARS if v in final_df.columns])}\")\n",
    "print(f\"Macro variables: {len([v for v in MACRO_VARS if v in final_df.columns])}\")\n",
    "\n",
    "print(f\"\\n=== Duration Statistics ===\")\n",
    "print(final_df['duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Data is ready for experiments:\n",
    "\n",
    "1. **Notebook 05**: Cause-Specific Cox (CSC)\n",
    "2. **Notebook 06**: Fine-Gray Model (FGR)\n",
    "3. **Notebook 07**: Random Survival Forest (RSF)\n",
    "4. **Notebook 08**: Model Comparison\n",
    "\n",
    "### Experiments (from paper)\n",
    "\n",
    "| Experiment | Variables | File |\n",
    "|------------|-----------|------|\n",
    "| Exp 4.1 | Loan-level only | Use `LOAN_LEVEL_VARS` |\n",
    "| Exp 4.2 | Macro only | Use `MACRO_VARS` |\n",
    "| Exp 4.3 | All variables | Use `ALL_VARS` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
