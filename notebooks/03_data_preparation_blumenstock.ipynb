{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Data Preparation: Blumenstock et al. (2022) Replication\n",
    "\n",
    "This notebook prepares data following the methodology from:\n",
    "\n",
    "> Blumenstock, G., Lessmann, S., & Seow, H-V. (2022). Deep learning for survival and competing risk modelling. *Journal of the Operational Research Society*, 73(1), 26-38.\n",
    "\n",
    "## Dataset 2: Post-Crisis Period (2010-2025)\n",
    "\n",
    "**Variables from Table 2:**\n",
    "\n",
    "### Loan-Level Variables (9)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `int_rate` | Initial interest rate |\n",
    "| `orig_upb` | Original unpaid balance |\n",
    "| `fico_score` | Initial FICO score |\n",
    "| `dti_r` | Initial debt-to-income ratio |\n",
    "| `ltv_r` | Initial loan-to-value ratio |\n",
    "| `bal_repaid` | Current repaid balance in percent |\n",
    "| `t_act_12m` | No. of times not being delinquent in last 12 months |\n",
    "| `t_del_30d_12m` | No. of times being 30 days delinquent in last 12 months |\n",
    "| `t_del_60d_12m` | No. of times being 60 days delinquent in last 12 months |\n",
    "\n",
    "### Macroeconomic Variables (13 for Dataset 2)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `hpi_st_d_t_o` | HPI difference between origination and today (state) |\n",
    "| `ppi_c_FRMA` | Current prepayment incentive |\n",
    "| `TB10Y_d_t_o` | Treasury rate difference |\n",
    "| `FRMA30Y_d_t_o` | 30Y FRM difference |\n",
    "| `ppi_o_FRMA` | Prepayment incentive at origination |\n",
    "| `hpi_st_log12m` | HPI 12-month log return (state) |\n",
    "| `hpi_r_st_us` | Ratio of state HPI to national HPI |\n",
    "| `st_unemp_r12m` | Unemployment 12-month log return (state) |\n",
    "| `st_unemp_r3m` | Unemployment 3-month log return (state) |\n",
    "| `TB10Y_r12m` | Treasury rate 12-month return |\n",
    "| `T10Y3MM` | Yield spread (10Y - 3M) |\n",
    "| `T10Y3MM_r12m` | Yield spread 12-month return |\n",
    "\n",
    "### Event Definitions\n",
    "- **Default (k=2)**: Loan turning 3-month delinquent for the first time\n",
    "- **Prepayment (k=1)**: Loan repaid completely and unexpectedly\n",
    "- **Censored (k=0)**: Active loan without event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import column definitions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.data.columns import (\n",
    "    ORIGINATION_COLUMNS, ORIGINATION_DTYPES,\n",
    "    PERFORMANCE_COLUMNS, PERFORMANCE_DTYPES,\n",
    "    ZERO_BALANCE_CODE_MAP\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following the paper's Dataset 2 setup (2010-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 Period: 2010-2025\n",
      "Sample size per fold: 10,000\n",
      "Number of folds: 11\n",
      "\n",
      "=== Checking Required Files ===\n",
      "  ✓ National macro data: ../data/external/fred_monthly_panel.parquet\n",
      "  ✓ State unemployment: ../data/external/state_unemployment.parquet\n",
      "  ✓ State HPI: ../data/external/state_hpi.parquet\n",
      "\n",
      "✓ All required files present.\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "EXTERNAL_DATA_DIR = Path('../data/external')\n",
    "\n",
    "# Dataset 2: Post-crisis period (2010-2025)\n",
    "VINTAGES = list(range(2010, 2026))\n",
    "\n",
    "# Sampling strategy from paper:\n",
    "# - 11 random subsamples of 10,000 each\n",
    "# - 10 for cross-validation, 1 for hyperparameter tuning\n",
    "SAMPLE_SIZE_PER_FOLD = 10000\n",
    "N_FOLDS = 11\n",
    "\n",
    "# Default definition: 3-month delinquent for the first time\n",
    "DEFAULT_DELINQUENCY_THRESHOLD = 3\n",
    "\n",
    "print(f\"Dataset 2 Period: {min(VINTAGES)}-{max(VINTAGES)}\")\n",
    "print(f\"Sample size per fold: {SAMPLE_SIZE_PER_FOLD:,}\")\n",
    "print(f\"Number of folds: {N_FOLDS}\")\n",
    "\n",
    "# === VERIFY REQUIRED FILES EXIST ===\n",
    "print(\"\\n=== Checking Required Files ===\")\n",
    "required_files = {\n",
    "    'National macro data': EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet',\n",
    "    'State unemployment': EXTERNAL_DATA_DIR / 'state_unemployment.parquet',\n",
    "    'State HPI': EXTERNAL_DATA_DIR / 'state_hpi.parquet',\n",
    "}\n",
    "\n",
    "missing_files = []\n",
    "for name, path in required_files.items():\n",
    "    if path.exists():\n",
    "        print(f\"  ✓ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {name}: MISSING - {path}\")\n",
    "        missing_files.append(name)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n⚠️  WARNING: {len(missing_files)} required file(s) missing!\")\n",
    "    print(\"   Run: python -m src.data.download_fred --include-states\")\n",
    "    print(\"   Some macro variables will not be calculated.\")\n",
    "else:\n",
    "    print(\"\\n✓ All required files present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-macro-header",
   "metadata": {},
   "source": [
    "## Step 1: Load Macroeconomic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "load-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ National macro data loaded: (337, 38)\n",
      "  Date range: 1998-01-01 00:00:00 to 2026-01-01 00:00:00\n",
      "  Columns: ['date', 'UNRATE', 'MORTGAGE30US', 'CSUSHPINSA', 'USSTHPI', 'FEDFUNDS', 'T10Y2Y', 'DGS3MO', 'DGS10', 'DGS5']...\n"
     ]
    }
   ],
   "source": [
    "# Load national macro data\n",
    "macro_path = EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet'\n",
    "\n",
    "if macro_path.exists():\n",
    "    macro_national = pd.read_parquet(macro_path)\n",
    "    macro_national.index.name = 'date'\n",
    "    macro_national = macro_national.reset_index()\n",
    "    macro_national['date'] = pd.to_datetime(macro_national['date'])\n",
    "    macro_national['year_month'] = macro_national['date'].dt.to_period('M')\n",
    "\n",
    "    # Verify required columns exist\n",
    "    required_macro_cols = ['MORTGAGE30US', 'DGS10', 'DGS3MO']\n",
    "    missing_cols = [c for c in required_macro_cols if c not in macro_national.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"⚠️  WARNING: Missing columns in macro data: {missing_cols}\")\n",
    "    \n",
    "    # Calculate additional variables needed for paper\n",
    "    # TB10Y_r12m: 10-year treasury rate 12-month return\n",
    "    if 'DGS10' in macro_national.columns:\n",
    "        macro_national['TB10Y_r12m'] = macro_national['DGS10'].pct_change(12)\n",
    "\n",
    "    # T10Y3MM: Yield spread (need 3-month rate)\n",
    "    if 'DGS10' in macro_national.columns and 'DGS3MO' in macro_national.columns:\n",
    "        macro_national['T10Y3MM'] = macro_national['DGS10'] - macro_national['DGS3MO']\n",
    "        # T10Y3MM_r12m: Yield spread 12-month return\n",
    "        macro_national['T10Y3MM_r12m'] = macro_national['T10Y3MM'].pct_change(12)\n",
    "\n",
    "    print(f\"✓ National macro data loaded: {macro_national.shape}\")\n",
    "    print(f\"  Date range: {macro_national['date'].min()} to {macro_national['date'].max()}\")\n",
    "    print(f\"  Columns: {list(macro_national.columns)[:10]}...\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: National macro data not found at {macro_path}\")\n",
    "    print(\"  Run: python -m src.data.download_fred\")\n",
    "    macro_national = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "load-state-unemp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ State unemployment loaded: (17085, 5)\n",
      "  States: 51\n",
      "  Sample: ['AK', 'AL', 'AR', 'AZ', 'CA']...\n"
     ]
    }
   ],
   "source": [
    "# Load state-level unemployment\n",
    "unemp_path = EXTERNAL_DATA_DIR / 'state_unemployment.parquet'\n",
    "\n",
    "if unemp_path.exists():\n",
    "    state_unemp = pd.read_parquet(unemp_path)\n",
    "    state_unemp.index.name = 'date'\n",
    "    state_unemp = state_unemp.reset_index()\n",
    "    state_unemp['date'] = pd.to_datetime(state_unemp['date'])\n",
    "    state_unemp['year_month'] = state_unemp['date'].dt.to_period('M')\n",
    "\n",
    "    # Melt to long format\n",
    "    state_cols = [c for c in state_unemp.columns if '_unemployment' in c]\n",
    "    \n",
    "    if len(state_cols) == 0:\n",
    "        print(\"⚠️  WARNING: No unemployment columns found in state data\")\n",
    "        state_unemp_long = pd.DataFrame()\n",
    "    else:\n",
    "        state_unemp_long = state_unemp.melt(\n",
    "            id_vars=['date', 'year_month'],\n",
    "            value_vars=state_cols,\n",
    "            var_name='state_col',\n",
    "            value_name='state_unemployment'\n",
    "        )\n",
    "        state_unemp_long['property_state'] = state_unemp_long['state_col'].str.replace('_unemployment', '')\n",
    "\n",
    "        # Calculate returns by state (need to sort first)\n",
    "        state_unemp_long = state_unemp_long.sort_values(['property_state', 'year_month'])\n",
    "        \n",
    "        # Group by state and calculate rolling returns\n",
    "        def calc_state_returns(group):\n",
    "            group = group.copy()\n",
    "            group['st_unemp_r12m'] = np.log(group['state_unemployment'] / group['state_unemployment'].shift(12))\n",
    "            group['st_unemp_r3m'] = np.log(group['state_unemployment'] / group['state_unemployment'].shift(3))\n",
    "            return group\n",
    "        \n",
    "        state_unemp_long = state_unemp_long.groupby('property_state', group_keys=False).apply(calc_state_returns)\n",
    "        \n",
    "        state_unemp_long = state_unemp_long[['year_month', 'property_state', 'state_unemployment', \n",
    "                                              'st_unemp_r12m', 'st_unemp_r3m']].drop_duplicates()\n",
    "\n",
    "        print(f\"✓ State unemployment loaded: {state_unemp_long.shape}\")\n",
    "        print(f\"  States: {state_unemp_long['property_state'].nunique()}\")\n",
    "        print(f\"  Sample: {state_unemp_long['property_state'].unique()[:5].tolist()}...\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: State unemployment data not found at {unemp_path}\")\n",
    "    print(\"  Run: python -m src.data.download_fred --include-states\")\n",
    "    state_unemp_long = pd.DataFrame(columns=['year_month', 'property_state', 'st_unemp_r12m', 'st_unemp_r3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "load-state-hpi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ State HPI loaded: (16881, 6)\n",
      "  States: 51\n",
      "  Sample: ['AK', 'AL', 'AR', 'AZ', 'CA']...\n"
     ]
    }
   ],
   "source": [
    "# Load state-level HPI\n",
    "hpi_path = EXTERNAL_DATA_DIR / 'state_hpi.parquet'\n",
    "\n",
    "if hpi_path.exists():\n",
    "    state_hpi = pd.read_parquet(hpi_path)\n",
    "    state_hpi.index.name = 'date'\n",
    "    state_hpi = state_hpi.reset_index()\n",
    "    state_hpi['date'] = pd.to_datetime(state_hpi['date'])\n",
    "    state_hpi['year_month'] = state_hpi['date'].dt.to_period('M')\n",
    "\n",
    "    # Find HPI columns (2-letter state code + _hpi)\n",
    "    hpi_cols = [c for c in state_hpi.columns if c.endswith('_hpi') and len(c) <= 6]\n",
    "    \n",
    "    if len(hpi_cols) == 0:\n",
    "        print(\"⚠️  WARNING: No HPI columns found in state data\")\n",
    "        state_hpi_long = pd.DataFrame()\n",
    "    else:\n",
    "        # Calculate national HPI (average across states)\n",
    "        state_hpi['national_hpi'] = state_hpi[hpi_cols].mean(axis=1)\n",
    "\n",
    "        # Melt to long format\n",
    "        state_hpi_long = state_hpi.melt(\n",
    "            id_vars=['date', 'year_month', 'national_hpi'],\n",
    "            value_vars=hpi_cols,\n",
    "            var_name='state_col',\n",
    "            value_name='state_hpi'\n",
    "        )\n",
    "        state_hpi_long['property_state'] = state_hpi_long['state_col'].str.replace('_hpi', '')\n",
    "\n",
    "        # Sort by state and time for proper rolling calculations\n",
    "        state_hpi_long = state_hpi_long.sort_values(['property_state', 'year_month'])\n",
    "\n",
    "        # Calculate log returns by state\n",
    "        def calc_hpi_returns(group):\n",
    "            group = group.copy()\n",
    "            group['hpi_st_log12m'] = np.log(group['state_hpi'] / group['state_hpi'].shift(12))\n",
    "            return group\n",
    "        \n",
    "        state_hpi_long = state_hpi_long.groupby('property_state', group_keys=False).apply(calc_hpi_returns)\n",
    "\n",
    "        # Calculate ratio of state HPI to national HPI\n",
    "        state_hpi_long['hpi_r_st_us'] = state_hpi_long['state_hpi'] / state_hpi_long['national_hpi']\n",
    "\n",
    "        state_hpi_long = state_hpi_long[['year_month', 'property_state', 'state_hpi', 'national_hpi',\n",
    "                                          'hpi_st_log12m', 'hpi_r_st_us']].drop_duplicates()\n",
    "\n",
    "        print(f\"✓ State HPI loaded: {state_hpi_long.shape}\")\n",
    "        print(f\"  States: {state_hpi_long['property_state'].nunique()}\")\n",
    "        print(f\"  Sample: {state_hpi_long['property_state'].unique()[:5].tolist()}...\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: State HPI data not found at {hpi_path}\")\n",
    "    print(\"  Run: python -m src.data.download_fred --include-states\")\n",
    "    state_hpi_long = pd.DataFrame(columns=['year_month', 'property_state', 'state_hpi', 'national_hpi', \n",
    "                                            'hpi_st_log12m', 'hpi_r_st_us'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-loans-header",
   "metadata": {},
   "source": [
    "## Step 2: Load and Process Loan Data\n",
    "\n",
    "Process Freddie Mac data with paper's variable definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_origination_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load origination data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_orig_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=ORIGINATION_COLUMNS,\n",
    "        dtype=ORIGINATION_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_performance_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load performance (monthly) data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_svcg_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=PERFORMANCE_COLUMNS,\n",
    "        dtype=PERFORMANCE_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Load functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "process-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process function defined.\n"
     ]
    }
   ],
   "source": [
    "def process_vintage_blumenstock(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a vintage following Blumenstock et al. (2022) methodology.\n",
    "    \n",
    "    Creates loan-level survival data with:\n",
    "    - Terminal record per loan\n",
    "    - Behavioral features from last 12 months\n",
    "    - Paper's variable definitions\n",
    "    \"\"\"\n",
    "    print(f\"Processing vintage {vintage}...\")\n",
    "    \n",
    "    # Load data\n",
    "    orig_df = load_origination_data(vintage)\n",
    "    perf_df = load_performance_data(vintage)\n",
    "    \n",
    "    if orig_df.empty or perf_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse reporting period\n",
    "    perf_df['reporting_date'] = pd.to_datetime(\n",
    "        perf_df['monthly_reporting_period'].astype(str), format='%Y%m'\n",
    "    )\n",
    "    perf_df['year_month'] = perf_df['reporting_date'].dt.to_period('M')\n",
    "    \n",
    "    # Parse delinquency status\n",
    "    perf_df['delinquency_status'] = pd.to_numeric(\n",
    "        perf_df['current_loan_delinquency_status'].replace({'X': '0', 'XX': '0'}),\n",
    "        errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    # Sort by loan and time\n",
    "    perf_df = perf_df.sort_values(['loan_sequence_number', 'loan_age'])\n",
    "    \n",
    "    # === Calculate behavioral variables (rolling 12-month) ===\n",
    "    perf_df['is_current'] = (perf_df['delinquency_status'] == 0).astype(int)\n",
    "    perf_df['is_30d_del'] = (perf_df['delinquency_status'] == 1).astype(int)\n",
    "    perf_df['is_60d_del'] = (perf_df['delinquency_status'] == 2).astype(int)\n",
    "    \n",
    "    # Rolling counts over last 12 months\n",
    "    grouped = perf_df.groupby('loan_sequence_number')\n",
    "    perf_df['t_act_12m'] = grouped['is_current'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_30d_12m'] = grouped['is_30d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_60d_12m'] = grouped['is_60d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    \n",
    "    # === Determine event type ===\n",
    "    # Default: first time reaching 90+ days delinquent\n",
    "    perf_df['is_default'] = (perf_df['delinquency_status'] >= DEFAULT_DELINQUENCY_THRESHOLD).astype(int)\n",
    "    perf_df['first_default'] = grouped['is_default'].transform(\n",
    "        lambda x: (x.cumsum() == 1) & (x == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Prepayment: zero balance code = 01 (not at maturity)\n",
    "    perf_df['is_prepay'] = (\n",
    "        (perf_df['zero_balance_code'] == '01') & \n",
    "        (perf_df['loan_age'] < perf_df['remaining_months_to_maturity'].fillna(360) + perf_df['loan_age'] - 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # === Calculate balance repaid ===\n",
    "    orig_upb = orig_df.set_index('loan_sequence_number')['orig_upb']\n",
    "    perf_df['orig_upb_lookup'] = perf_df['loan_sequence_number'].map(orig_upb)\n",
    "    perf_df['bal_repaid'] = (\n",
    "        (perf_df['orig_upb_lookup'] - perf_df['current_actual_upb'].fillna(0)) / \n",
    "        perf_df['orig_upb_lookup']\n",
    "    ) * 100\n",
    "    perf_df['bal_repaid'] = perf_df['bal_repaid'].clip(0, 100)\n",
    "    \n",
    "    # === Get terminal record for each loan ===\n",
    "    # Determine event at terminal record\n",
    "    def get_terminal_event(group):\n",
    "        \"\"\"Get terminal record with event type.\"\"\"\n",
    "        last_row = group.iloc[-1].copy()\n",
    "        \n",
    "        # Check for default (first 90+ delinquency)\n",
    "        default_rows = group[group['first_default'] == 1]\n",
    "        if len(default_rows) > 0:\n",
    "            last_row = default_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 2  # Default\n",
    "            return last_row\n",
    "        \n",
    "        # Check for prepayment\n",
    "        prepay_rows = group[group['is_prepay'] == 1]\n",
    "        if len(prepay_rows) > 0:\n",
    "            last_row = prepay_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 1  # Prepay\n",
    "            return last_row\n",
    "        \n",
    "        # Censored\n",
    "        last_row['event_code'] = 0\n",
    "        return last_row\n",
    "    \n",
    "    print(f\"  Getting terminal records...\")\n",
    "    terminal_df = perf_df.groupby('loan_sequence_number').apply(get_terminal_event)\n",
    "    terminal_df = terminal_df.reset_index(drop=True)\n",
    "    \n",
    "    # === Merge with origination data ===\n",
    "    orig_cols = [\n",
    "        'loan_sequence_number', 'credit_score', 'orig_ltv', 'orig_dti',\n",
    "        'orig_upb', 'orig_interest_rate', 'orig_loan_term',\n",
    "        'first_payment_date', 'property_state'\n",
    "    ]\n",
    "    orig_subset = orig_df[[c for c in orig_cols if c in orig_df.columns]].copy()\n",
    "    orig_subset['vintage_year'] = vintage\n",
    "    \n",
    "    # Parse origination date\n",
    "    orig_subset['first_payment_date'] = pd.to_datetime(\n",
    "        orig_subset['first_payment_date'].astype(str), format='%Y%m', errors='coerce'\n",
    "    )\n",
    "    orig_subset['orig_year_month'] = orig_subset['first_payment_date'].dt.to_period('M')\n",
    "    \n",
    "    terminal_df = terminal_df.merge(orig_subset, on='loan_sequence_number', how='left')\n",
    "    \n",
    "    print(f\"  Loans: {len(terminal_df):,}\")\n",
    "    print(f\"  Events: Prepay={sum(terminal_df['event_code']==1):,}, \"\n",
    "          f\"Default={sum(terminal_df['event_code']==2):,}, \"\n",
    "          f\"Censored={sum(terminal_df['event_code']==0):,}\")\n",
    "    \n",
    "    return terminal_df\n",
    "\n",
    "\n",
    "print(\"Process function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "process-all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vintage 2010...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=44,437, Default=1,614, Censored=3,949\n",
      "Processing vintage 2011...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=42,938, Default=1,528, Censored=5,534\n",
      "Processing vintage 2012...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,822, Default=1,698, Censored=10,480\n",
      "Processing vintage 2013...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,060, Default=1,882, Censored=11,058\n",
      "Processing vintage 2014...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=39,244, Default=1,919, Censored=8,837\n",
      "Processing vintage 2015...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=36,516, Default=2,023, Censored=11,461\n",
      "Processing vintage 2016...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=33,910, Default=2,345, Censored=13,745\n",
      "Processing vintage 2017...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=35,261, Default=2,877, Censored=11,862\n",
      "Processing vintage 2018...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,971, Default=2,670, Censored=9,359\n",
      "Processing vintage 2019...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=33,517, Default=2,725, Censored=13,758\n",
      "Processing vintage 2020...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=17,834, Default=1,022, Censored=31,144\n",
      "Processing vintage 2021...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=8,029, Default=850, Censored=41,121\n",
      "Processing vintage 2022...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=7,061, Default=1,368, Censored=41,571\n",
      "Processing vintage 2023...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=7,684, Default=789, Censored=41,527\n",
      "Processing vintage 2024...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=3,319, Default=215, Censored=46,466\n",
      "Processing vintage 2025...\n",
      "  Getting terminal records...\n",
      "  Loans: 24,950\n",
      "  Events: Prepay=199, Default=2, Censored=24,749\n",
      "\n",
      "Combining all vintages...\n",
      "Total loans: 774,950\n"
     ]
    }
   ],
   "source": [
    "# Process all vintages in Dataset 2\n",
    "all_loans = []\n",
    "\n",
    "for vintage in VINTAGES:\n",
    "    df = process_vintage_blumenstock(vintage)\n",
    "    if not df.empty:\n",
    "        all_loans.append(df)\n",
    "\n",
    "# Combine\n",
    "print(\"\\nCombining all vintages...\")\n",
    "loans_df = pd.concat(all_loans, ignore_index=True)\n",
    "print(f\"Total loans: {len(loans_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-macro-header",
   "metadata": {},
   "source": [
    "## Step 3: Merge Macroeconomic Variables\n",
    "\n",
    "Add paper's macroeconomic variables at the observation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "merge-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Merging Macro Data at Observation Time ===\n",
      "\n",
      "Merging state unemployment...\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'st_unemp_r12m_x', 'st_unemp_r3m_x', 'state_unemployment_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_unemp_long\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerging state unemployment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     loans_df \u001b[38;5;241m=\u001b[39m loans_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m     10\u001b[0m         state_unemp_long,\n\u001b[1;32m     11\u001b[0m         on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperty_state\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m         how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     coverage \u001b[38;5;241m=\u001b[39m loans_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_unemp_r12m\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✓ st_unemp_r12m coverage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m  10833\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10834\u001b[0m         right,\n\u001b[1;32m  10835\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[1;32m  10836\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m  10837\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[1;32m  10838\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[1;32m  10839\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[1;32m  10840\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[1;32m  10841\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m  10842\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[1;32m  10843\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m  10844\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[1;32m  10845\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m  10846\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    171\u001b[0m         left_df,\n\u001b[1;32m    172\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[1;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[1;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[0;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m _items_overlap_with_suffix(\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffixes\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    849\u001b[0m         join_index,\n\u001b[1;32m    850\u001b[0m         left_indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[0;34m(left, right, suffixes)\u001b[0m\n\u001b[1;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[0;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2760\u001b[0m     )\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[0;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'st_unemp_r12m_x', 'st_unemp_r3m_x', 'state_unemployment_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "# Merge macroeconomic data at observation time\n",
    "print(\"=== Merging Macro Data at Observation Time ===\\n\")\n",
    "\n",
    "n_before = len(loans_df)\n",
    "\n",
    "# Merge state unemployment data\n",
    "if not state_unemp_long.empty:\n",
    "    print(\"Merging state unemployment...\")\n",
    "    loans_df = loans_df.merge(\n",
    "        state_unemp_long,\n",
    "        on=['year_month', 'property_state'],\n",
    "        how='left'\n",
    "    )\n",
    "    coverage = loans_df['st_unemp_r12m'].notna().mean()\n",
    "    print(f\"  ✓ st_unemp_r12m coverage: {coverage:.1%}\")\n",
    "    print(f\"  ✓ st_unemp_r3m coverage: {loans_df['st_unemp_r3m'].notna().mean():.1%}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping state unemployment merge (data not loaded)\")\n",
    "    loans_df['st_unemp_r12m'] = np.nan\n",
    "    loans_df['st_unemp_r3m'] = np.nan\n",
    "\n",
    "# Merge state HPI data  \n",
    "if not state_hpi_long.empty:\n",
    "    print(\"\\nMerging state HPI...\")\n",
    "    loans_df = loans_df.merge(\n",
    "        state_hpi_long,\n",
    "        on=['year_month', 'property_state'],\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"  ✓ state_hpi coverage: {loans_df['state_hpi'].notna().mean():.1%}\")\n",
    "    print(f\"  ✓ hpi_st_log12m coverage: {loans_df['hpi_st_log12m'].notna().mean():.1%}\")\n",
    "    print(f\"  ✓ hpi_r_st_us coverage: {loans_df['hpi_r_st_us'].notna().mean():.1%}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping state HPI merge (data not loaded)\")\n",
    "    loans_df['state_hpi'] = np.nan\n",
    "    loans_df['national_hpi'] = np.nan\n",
    "    loans_df['hpi_st_log12m'] = np.nan\n",
    "    loans_df['hpi_r_st_us'] = np.nan\n",
    "\n",
    "# Merge national macro data\n",
    "if macro_national is not None:\n",
    "    print(\"\\nMerging national macro...\")\n",
    "    macro_cols = ['year_month', 'MORTGAGE30US', 'DGS10', 'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m']\n",
    "    macro_cols = [c for c in macro_cols if c in macro_national.columns]\n",
    "    macro_subset = macro_national[macro_cols].copy()\n",
    "    loans_df = loans_df.merge(macro_subset, on='year_month', how='left')\n",
    "    print(f\"  ✓ MORTGAGE30US coverage: {loans_df['MORTGAGE30US'].notna().mean():.1%}\")\n",
    "    print(f\"  ✓ DGS10 coverage: {loans_df['DGS10'].notna().mean():.1%}\")\n",
    "    if 'TB10Y_r12m' in loans_df.columns:\n",
    "        print(f\"  ✓ TB10Y_r12m coverage: {loans_df['TB10Y_r12m'].notna().mean():.1%}\")\n",
    "    if 'T10Y3MM' in loans_df.columns:\n",
    "        print(f\"  ✓ T10Y3MM coverage: {loans_df['T10Y3MM'].notna().mean():.1%}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping national macro merge (data not loaded)\")\n",
    "    loans_df['MORTGAGE30US'] = np.nan\n",
    "    loans_df['DGS10'] = np.nan\n",
    "    loans_df['TB10Y_r12m'] = np.nan\n",
    "    loans_df['T10Y3MM'] = np.nan\n",
    "    loans_df['T10Y3MM_r12m'] = np.nan\n",
    "\n",
    "# Verify no rows were lost\n",
    "n_after = len(loans_df)\n",
    "if n_after != n_before:\n",
    "    print(f\"\\n⚠️  WARNING: Row count changed from {n_before:,} to {n_after:,} during merge!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Row count unchanged: {n_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-orig-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get origination-time values for difference calculations\n",
    "print(\"=== Calculating Origination-Time Differences ===\\n\")\n",
    "\n",
    "# Track which variables were successfully created\n",
    "created_vars = []\n",
    "failed_vars = []\n",
    "\n",
    "# === 1. HPI difference (state-level) ===\n",
    "if not state_hpi_long.empty and 'state_hpi' in loans_df.columns:\n",
    "    print(\"Calculating hpi_st_d_t_o (HPI difference)...\")\n",
    "    \n",
    "    # Get origination-time state HPI\n",
    "    orig_hpi = state_hpi_long[['year_month', 'property_state', 'state_hpi']].rename(\n",
    "        columns={'year_month': 'orig_year_month', 'state_hpi': 'orig_state_hpi'}\n",
    "    )\n",
    "    loans_df = loans_df.merge(orig_hpi, on=['orig_year_month', 'property_state'], how='left')\n",
    "    \n",
    "    # hpi_st_d_t_o: Difference of HPI between origination and today (state-level)\n",
    "    loans_df['hpi_st_d_t_o'] = loans_df['state_hpi'] - loans_df['orig_state_hpi']\n",
    "    \n",
    "    coverage = loans_df['hpi_st_d_t_o'].notna().mean()\n",
    "    print(f\"  ✓ hpi_st_d_t_o coverage: {coverage:.1%}\")\n",
    "    if coverage > 0.9:\n",
    "        created_vars.append('hpi_st_d_t_o')\n",
    "    else:\n",
    "        failed_vars.append(('hpi_st_d_t_o', f'{coverage:.1%} coverage'))\n",
    "else:\n",
    "    print(\"⚠️  Cannot calculate hpi_st_d_t_o (state HPI not available)\")\n",
    "    loans_df['hpi_st_d_t_o'] = np.nan\n",
    "    failed_vars.append(('hpi_st_d_t_o', 'state HPI not loaded'))\n",
    "\n",
    "# === 2. Interest rate and mortgage differences ===\n",
    "if macro_national is not None and 'MORTGAGE30US' in loans_df.columns:\n",
    "    print(\"\\nCalculating prepayment incentives and rate differences...\")\n",
    "    \n",
    "    # Get origination-time macro rates\n",
    "    orig_macro = macro_national[['year_month', 'MORTGAGE30US', 'DGS10']].rename(\n",
    "        columns={'year_month': 'orig_year_month', 'MORTGAGE30US': 'orig_MORTGAGE30US', 'DGS10': 'orig_DGS10'}\n",
    "    )\n",
    "    loans_df = loans_df.merge(orig_macro, on='orig_year_month', how='left')\n",
    "    \n",
    "    # Check if we have the required columns\n",
    "    has_int_rate = 'orig_interest_rate' in loans_df.columns\n",
    "    has_mortgage = 'MORTGAGE30US' in loans_df.columns and 'orig_MORTGAGE30US' in loans_df.columns\n",
    "    has_treasury = 'DGS10' in loans_df.columns and 'orig_DGS10' in loans_df.columns\n",
    "    \n",
    "    # ppi_c_FRMA: Current prepayment incentive (loan rate - current mortgage rate)\n",
    "    if has_int_rate and has_mortgage:\n",
    "        loans_df['ppi_c_FRMA'] = loans_df['orig_interest_rate'] - loans_df['MORTGAGE30US']\n",
    "        coverage = loans_df['ppi_c_FRMA'].notna().mean()\n",
    "        print(f\"  ✓ ppi_c_FRMA coverage: {coverage:.1%}\")\n",
    "        created_vars.append('ppi_c_FRMA') if coverage > 0.9 else failed_vars.append(('ppi_c_FRMA', f'{coverage:.1%}'))\n",
    "    else:\n",
    "        loans_df['ppi_c_FRMA'] = np.nan\n",
    "        failed_vars.append(('ppi_c_FRMA', 'missing required columns'))\n",
    "\n",
    "    # ppi_o_FRMA: Prepayment incentive at origination (loan rate - orig mortgage rate)\n",
    "    if has_int_rate and has_mortgage:\n",
    "        loans_df['ppi_o_FRMA'] = loans_df['orig_interest_rate'] - loans_df['orig_MORTGAGE30US']\n",
    "        coverage = loans_df['ppi_o_FRMA'].notna().mean()\n",
    "        print(f\"  ✓ ppi_o_FRMA coverage: {coverage:.1%}\")\n",
    "        created_vars.append('ppi_o_FRMA') if coverage > 0.9 else failed_vars.append(('ppi_o_FRMA', f'{coverage:.1%}'))\n",
    "    else:\n",
    "        loans_df['ppi_o_FRMA'] = np.nan\n",
    "        failed_vars.append(('ppi_o_FRMA', 'missing required columns'))\n",
    "\n",
    "    # TB10Y_d_t_o: Difference of 10-year treasury rate (today - origination)\n",
    "    if has_treasury:\n",
    "        loans_df['TB10Y_d_t_o'] = loans_df['DGS10'] - loans_df['orig_DGS10']\n",
    "        coverage = loans_df['TB10Y_d_t_o'].notna().mean()\n",
    "        print(f\"  ✓ TB10Y_d_t_o coverage: {coverage:.1%}\")\n",
    "        created_vars.append('TB10Y_d_t_o') if coverage > 0.9 else failed_vars.append(('TB10Y_d_t_o', f'{coverage:.1%}'))\n",
    "    else:\n",
    "        loans_df['TB10Y_d_t_o'] = np.nan\n",
    "        failed_vars.append(('TB10Y_d_t_o', 'missing DGS10'))\n",
    "\n",
    "    # FRMA30Y_d_t_o: Difference of 30-year FRM average (today - origination)\n",
    "    if has_mortgage:\n",
    "        loans_df['FRMA30Y_d_t_o'] = loans_df['MORTGAGE30US'] - loans_df['orig_MORTGAGE30US']\n",
    "        coverage = loans_df['FRMA30Y_d_t_o'].notna().mean()\n",
    "        print(f\"  ✓ FRMA30Y_d_t_o coverage: {coverage:.1%}\")\n",
    "        created_vars.append('FRMA30Y_d_t_o') if coverage > 0.9 else failed_vars.append(('FRMA30Y_d_t_o', f'{coverage:.1%}'))\n",
    "    else:\n",
    "        loans_df['FRMA30Y_d_t_o'] = np.nan\n",
    "        failed_vars.append(('FRMA30Y_d_t_o', 'missing MORTGAGE30US'))\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Cannot calculate rate differences (macro data not available)\")\n",
    "    for var in ['ppi_c_FRMA', 'ppi_o_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o']:\n",
    "        loans_df[var] = np.nan\n",
    "        failed_vars.append((var, 'macro data not loaded'))\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n=== Derived Variables Summary ===\")\n",
    "print(f\"✓ Successfully created: {len(created_vars)}\")\n",
    "for var in created_vars:\n",
    "    print(f\"    - {var}\")\n",
    "\n",
    "if failed_vars:\n",
    "    print(f\"\\n⚠️  Failed or low coverage: {len(failed_vars)}\")\n",
    "    for var, reason in failed_vars:\n",
    "        print(f\"    - {var}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rename-header",
   "metadata": {},
   "source": [
    "## Step 4: Rename Variables to Paper's Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "rename-vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan-level variables: 9\n",
      "Macro variables: 12\n",
      "Total: 21\n"
     ]
    }
   ],
   "source": [
    "# Rename to paper's variable names\n",
    "rename_map = {\n",
    "    # Loan-level\n",
    "    'orig_interest_rate': 'int_rate',\n",
    "    'credit_score': 'fico_score',\n",
    "    'orig_dti': 'dti_r',\n",
    "    'orig_ltv': 'ltv_r',\n",
    "    # Duration\n",
    "    'loan_age': 'duration',\n",
    "}\n",
    "\n",
    "loans_df = loans_df.rename(columns=rename_map)\n",
    "\n",
    "# Define final variable sets (from paper Table 2)\n",
    "LOAN_LEVEL_VARS = [\n",
    "    'int_rate',           # Initial interest rate\n",
    "    'orig_upb',           # Original unpaid balance\n",
    "    'fico_score',         # Initial FICO score\n",
    "    'dti_r',              # Initial debt-to-income ratio\n",
    "    'ltv_r',              # Initial loan-to-value ratio\n",
    "    'bal_repaid',         # Current repaid balance in percent\n",
    "    't_act_12m',          # Times not delinquent in last 12 months\n",
    "    't_del_30d_12m',      # Times 30 days delinquent in last 12 months\n",
    "    't_del_60d_12m',      # Times 60 days delinquent in last 12 months\n",
    "]\n",
    "\n",
    "MACRO_VARS = [\n",
    "    'hpi_st_d_t_o',       # HPI difference (state)\n",
    "    'ppi_c_FRMA',         # Current prepayment incentive\n",
    "    'TB10Y_d_t_o',        # Treasury rate difference\n",
    "    'FRMA30Y_d_t_o',      # 30Y FRM difference\n",
    "    'ppi_o_FRMA',         # Prepayment incentive at origination\n",
    "    'hpi_st_log12m',      # HPI 12-month log return (state)\n",
    "    'hpi_r_st_us',        # Ratio of state HPI to national HPI\n",
    "    'st_unemp_r12m',      # Unemployment 12-month log return (state)\n",
    "    'st_unemp_r3m',       # Unemployment 3-month log return (state)\n",
    "    'TB10Y_r12m',         # Treasury rate 12-month return\n",
    "    'T10Y3MM',            # Yield spread (10Y - 3M)\n",
    "    'T10Y3MM_r12m',       # Yield spread 12-month return\n",
    "]\n",
    "\n",
    "ALL_VARS = LOAN_LEVEL_VARS + MACRO_VARS\n",
    "\n",
    "print(f\"Loan-level variables: {len(LOAN_LEVEL_VARS)}\")\n",
    "print(f\"Macro variables: {len(MACRO_VARS)}\")\n",
    "print(f\"Total: {len(ALL_VARS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "check-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VARIABLE COVERAGE CHECK\n",
      "======================================================================\n",
      "\n",
      "=== Loan-Level Variables (9) ===\n",
      "  ✓ int_rate: 100.0%\n",
      "  ✓ orig_upb: 100.0%\n",
      "  ✓ fico_score: 100.0%\n",
      "  ✓ dti_r: 100.0%\n",
      "  ✓ ltv_r: 100.0%\n",
      "  ✓ bal_repaid: 100.0%\n",
      "  ✓ t_act_12m: 100.0%\n",
      "  ✓ t_del_30d_12m: 100.0%\n",
      "  ✓ t_del_60d_12m: 100.0%\n",
      "\n",
      "=== Macro Variables (12) ===\n",
      "  ✓ hpi_st_d_t_o: 100.0% [CRITICAL]\n",
      "      Sample values: [-9.27, -2.41, 36.08]\n",
      "  ✓ ppi_c_FRMA: 100.0% [CRITICAL]\n",
      "      Sample values: [0.577, 0.546, 0.73]\n",
      "  ✓ TB10Y_d_t_o: 100.0% [CRITICAL]\n",
      "      Sample values: [-1.617, -1.555, -1.685]\n",
      "  ✓ FRMA30Y_d_t_o: 100.0% [CRITICAL]\n",
      "      Sample values: [-1.09, -1.014, -1.198]\n",
      "  ✓ ppi_o_FRMA: 100.0% [CRITICAL]\n",
      "      Sample values: [-0.513, -0.468, -0.468]\n",
      "  ✓ hpi_st_log12m: 100.0%\n",
      "  ✓ hpi_r_st_us: 100.0%\n",
      "  ✓ st_unemp_r12m: 100.0%\n",
      "  ✓ st_unemp_r3m: 100.0%\n",
      "  ✓ TB10Y_r12m: 100.0%\n",
      "  ✓ T10Y3MM: 100.0%\n",
      "  ✓ T10Y3MM_r12m: 100.0%\n",
      "\n",
      "======================================================================\n",
      "✓ ALL VARIABLES OK (coverage >= 90%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check variable coverage with detailed status\n",
    "print(\"=\" * 70)\n",
    "print(\"VARIABLE COVERAGE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_ok = True\n",
    "coverage_threshold = 0.90  # Warn if below 90%\n",
    "\n",
    "print(\"\\n=== Loan-Level Variables (9) ===\")\n",
    "for var in LOAN_LEVEL_VARS:\n",
    "    if var in loans_df.columns:\n",
    "        coverage = loans_df[var].notna().mean()\n",
    "        status = \"✓\" if coverage >= coverage_threshold else \"⚠️\"\n",
    "        if coverage < coverage_threshold:\n",
    "            all_ok = False\n",
    "        print(f\"  {status} {var}: {coverage:.1%}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {var}: MISSING\")\n",
    "        all_ok = False\n",
    "\n",
    "print(\"\\n=== Macro Variables (12) ===\")\n",
    "# Mark critical derived variables\n",
    "critical_vars = ['hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o', 'ppi_o_FRMA']\n",
    "\n",
    "for var in MACRO_VARS:\n",
    "    critical_marker = \" [CRITICAL]\" if var in critical_vars else \"\"\n",
    "    if var in loans_df.columns:\n",
    "        coverage = loans_df[var].notna().mean()\n",
    "        status = \"✓\" if coverage >= coverage_threshold else \"⚠️\"\n",
    "        if coverage < coverage_threshold:\n",
    "            all_ok = False\n",
    "        \n",
    "        # Show sample values for critical variables\n",
    "        if var in critical_vars and coverage > 0:\n",
    "            sample_vals = loans_df[var].dropna().head(3).tolist()\n",
    "            print(f\"  {status} {var}: {coverage:.1%}{critical_marker}\")\n",
    "            print(f\"      Sample values: {[round(v, 3) for v in sample_vals]}\")\n",
    "        else:\n",
    "            print(f\"  {status} {var}: {coverage:.1%}{critical_marker}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {var}: MISSING{critical_marker}\")\n",
    "        all_ok = False\n",
    "\n",
    "# Final status\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_ok:\n",
    "    print(\"✓ ALL VARIABLES OK (coverage >= 90%)\")\n",
    "else:\n",
    "    print(\"⚠️  SOME VARIABLES HAVE ISSUES - Check warnings above\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "id": "iy2vjuzapgg",
   "source": "# Save full loans_df (before subsampling) for analysis\nprint(\"=== Saving Full Dataset ===\")\n\n# Save to parquet (efficient)\nfull_output_path = PROCESSED_DATA_DIR / 'survival_data_blumenstock.parquet'\nloans_df.to_parquet(full_output_path, index=False)\nprint(f\"✓ Saved parquet: {full_output_path}\")\nprint(f\"  Shape: {loans_df.shape}\")\n\n# Save to CSV (portable)\ncsv_output_path = PROCESSED_DATA_DIR / 'survival_data_blumenstock.csv'\nloans_df.to_csv(csv_output_path, index=False)\nprint(f\"✓ Saved CSV: {csv_output_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5khn0nijci",
   "source": "# Run validation tests on the saved dataset\nimport subprocess\nprint(\"=== Running Validation Tests ===\\n\")\n\nresult = subprocess.run(\n    ['python', '-m', 'tests.test_survival_data_blumenstock', '--num-records', '10', '--num-vintages', '3'],\n    cwd=str(Path('..').resolve()),\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.returncode != 0:\n    print(\"STDERR:\", result.stderr)\n    print(f\"\\n⚠️  Tests returned non-zero exit code: {result.returncode}\")\nelse:\n    print(\"✓ All validation tests passed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sample-header",
   "metadata": {},
   "source": [
    "## Step 5: Create Subsamples for Cross-Validation\n",
    "\n",
    "Following paper: 11 random subsamples of 10,000 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "filter-complete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete cases: 774,608 / 774,950 (100.0%)\n",
      "\n",
      "Event distribution:\n",
      "event_code\n",
      "0    326436\n",
      "1    422695\n",
      "2     25477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "0=Censored, 1=Prepay, 2=Default\n"
     ]
    }
   ],
   "source": [
    "# Filter to complete cases\n",
    "required_cols = ['duration', 'event_code'] + [v for v in ALL_VARS if v in loans_df.columns]\n",
    "loans_complete = loans_df.dropna(subset=[c for c in required_cols if c in loans_df.columns])\n",
    "\n",
    "print(f\"Complete cases: {len(loans_complete):,} / {len(loans_df):,} ({len(loans_complete)/len(loans_df):.1%})\")\n",
    "\n",
    "# Event distribution\n",
    "print(\"\\nEvent distribution:\")\n",
    "print(loans_complete['event_code'].value_counts().sort_index())\n",
    "print(\"\\n0=Censored, 1=Prepay, 2=Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "create-samples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted loans available: 25,477\n",
      "Non-defaulted loans available: 749,131\n",
      "\n",
      "Total sampled: 110,000\n",
      "Number of folds: 11\n",
      "\n",
      "Samples per fold:\n",
      "fold\n",
      "0     10000\n",
      "1     10000\n",
      "2     10000\n",
      "3     10000\n",
      "4     10000\n",
      "5     10000\n",
      "6     10000\n",
      "7     10000\n",
      "8     10000\n",
      "9     10000\n",
      "10    10000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create subsamples with controlled default ratio\n",
    "# Target: 100 defaulted loans (1%) + 9,900 non-defaulted loans (99%) per fold\n",
    "# Sampling WITHOUT replacement\n",
    "\n",
    "DEFAULT_SAMPLES_PER_FOLD = 100\n",
    "NON_DEFAULT_SAMPLES_PER_FOLD = 9900\n",
    "\n",
    "# Separate defaulted and non-defaulted loans\n",
    "defaulted = loans_complete[loans_complete['event_code'] == 2].copy()\n",
    "non_defaulted = loans_complete[loans_complete['event_code'] != 2].copy()\n",
    "\n",
    "print(f\"Defaulted loans available: {len(defaulted):,}\")\n",
    "print(f\"Non-defaulted loans available: {len(non_defaulted):,}\")\n",
    "\n",
    "# Check if we have enough data for all folds (sampling WITHOUT replacement)\n",
    "total_defaults_needed = DEFAULT_SAMPLES_PER_FOLD * N_FOLDS\n",
    "total_non_defaults_needed = NON_DEFAULT_SAMPLES_PER_FOLD * N_FOLDS\n",
    "\n",
    "if len(defaulted) < total_defaults_needed:\n",
    "    print(f\"\\n⚠️  Not enough defaulted loans for {N_FOLDS} folds without replacement\")\n",
    "    print(f\"   Need: {total_defaults_needed:,}, Have: {len(defaulted):,}\")\n",
    "    N_FOLDS_ACTUAL = len(defaulted) // DEFAULT_SAMPLES_PER_FOLD\n",
    "    print(f\"   Reducing to {N_FOLDS_ACTUAL} folds\")\n",
    "else:\n",
    "    N_FOLDS_ACTUAL = N_FOLDS\n",
    "\n",
    "if len(non_defaulted) < NON_DEFAULT_SAMPLES_PER_FOLD * N_FOLDS_ACTUAL:\n",
    "    print(f\"\\n⚠️  Not enough non-defaulted loans for {N_FOLDS_ACTUAL} folds without replacement\")\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(42)\n",
    "defaulted_shuffled = defaulted.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "non_defaulted_shuffled = non_defaulted.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Create folds by taking sequential chunks (no replacement)\n",
    "fold_dfs = []\n",
    "for fold in range(N_FOLDS_ACTUAL):\n",
    "    # Get defaulted samples for this fold\n",
    "    default_start = fold * DEFAULT_SAMPLES_PER_FOLD\n",
    "    default_end = default_start + DEFAULT_SAMPLES_PER_FOLD\n",
    "    fold_defaults = defaulted_shuffled.iloc[default_start:default_end].copy()\n",
    "    \n",
    "    # Get non-defaulted samples for this fold\n",
    "    non_default_start = fold * NON_DEFAULT_SAMPLES_PER_FOLD\n",
    "    non_default_end = non_default_start + NON_DEFAULT_SAMPLES_PER_FOLD\n",
    "    fold_non_defaults = non_defaulted_shuffled.iloc[non_default_start:non_default_end].copy()\n",
    "    \n",
    "    # Combine and assign fold number\n",
    "    fold_df = pd.concat([fold_defaults, fold_non_defaults], ignore_index=True)\n",
    "    fold_df['fold'] = fold\n",
    "    fold_dfs.append(fold_df)\n",
    "\n",
    "# Combine all folds\n",
    "sampled_df = pd.concat(fold_dfs, ignore_index=True)\n",
    "\n",
    "# Shuffle within the combined dataset to mix defaults and non-defaults\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTotal sampled: {len(sampled_df):,}\")\n",
    "print(f\"Number of folds: {N_FOLDS_ACTUAL}\")\n",
    "print(f\"\\nSamples per fold:\")\n",
    "print(sampled_df['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fold-events",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Event Distribution per Fold ===\n",
      "Target: 100 defaults (1%) + 9900 non-defaults per fold\n",
      "\n",
      "Fold 0: n=10,000, default=100 (1.0%), prepay=5,571, censored=4,329\n",
      "Fold 1: n=10,000, default=100 (1.0%), prepay=5,622, censored=4,278\n",
      "Fold 2: n=10,000, default=100 (1.0%), prepay=5,527, censored=4,373\n",
      "Fold 3: n=10,000, default=100 (1.0%), prepay=5,547, censored=4,353\n",
      "Fold 4: n=10,000, default=100 (1.0%), prepay=5,647, censored=4,253\n",
      "Fold 5: n=10,000, default=100 (1.0%), prepay=5,654, censored=4,246\n",
      "Fold 6: n=10,000, default=100 (1.0%), prepay=5,576, censored=4,324\n",
      "Fold 7: n=10,000, default=100 (1.0%), prepay=5,595, censored=4,305\n",
      "Fold 8: n=10,000, default=100 (1.0%), prepay=5,589, censored=4,311\n",
      "Fold 9: n=10,000, default=100 (1.0%), prepay=5,576, censored=4,324\n",
      "Fold 10: n=10,000, default=100 (1.0%), prepay=5,610, censored=4,290\n",
      "\n",
      "Total defaults: 1,100 (1.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check event distribution per fold\n",
    "print(\"=== Event Distribution per Fold ===\")\n",
    "print(f\"Target: {DEFAULT_SAMPLES_PER_FOLD} defaults (1%) + {NON_DEFAULT_SAMPLES_PER_FOLD} non-defaults per fold\\n\")\n",
    "\n",
    "for fold in range(N_FOLDS_ACTUAL):\n",
    "    fold_data = sampled_df[sampled_df['fold'] == fold]\n",
    "    prepay = (fold_data['event_code'] == 1).sum()\n",
    "    default = (fold_data['event_code'] == 2).sum()\n",
    "    censored = (fold_data['event_code'] == 0).sum()\n",
    "    default_pct = default / len(fold_data) * 100\n",
    "    print(f\"Fold {fold}: n={len(fold_data):,}, default={default:,} ({default_pct:.1f}%), prepay={prepay:,}, censored={censored:,}\")\n",
    "\n",
    "# Verify overall\n",
    "total_defaults = (sampled_df['event_code'] == 2).sum()\n",
    "print(f\"\\nTotal defaults: {total_defaults:,} ({total_defaults/len(sampled_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Step 6: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "select-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: 110,000 rows, 28 columns\n"
     ]
    }
   ],
   "source": [
    "# Select final columns\n",
    "final_cols = [\n",
    "    # Identifiers\n",
    "    'loan_sequence_number', 'vintage_year', 'fold',\n",
    "    # Survival data\n",
    "    'duration', 'event_code',\n",
    "    # Loan-level variables\n",
    "] + [v for v in LOAN_LEVEL_VARS if v in sampled_df.columns] + [\n",
    "    # Macro variables  \n",
    "] + [v for v in MACRO_VARS if v in sampled_df.columns] + [\n",
    "    # Additional useful columns\n",
    "    'property_state', 'year_month'\n",
    "]\n",
    "\n",
    "# Remove duplicates and filter\n",
    "final_cols = list(dict.fromkeys(final_cols))\n",
    "final_cols = [c for c in final_cols if c in sampled_df.columns]\n",
    "\n",
    "final_df = sampled_df[final_cols].copy()\n",
    "print(f\"Final dataset: {len(final_df):,} rows, {len(final_cols)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "save-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/processed/blumenstock_dataset2.parquet\n",
      "Variable config saved.\n"
     ]
    }
   ],
   "source": [
    "# Save to parquet\n",
    "output_path = PROCESSED_DATA_DIR / 'blumenstock_dataset2.parquet'\n",
    "final_df.to_parquet(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Also save variable lists for reference\n",
    "var_config = {\n",
    "    'loan_level_vars': LOAN_LEVEL_VARS,\n",
    "    'macro_vars': MACRO_VARS,\n",
    "    'all_vars': ALL_VARS,\n",
    "}\n",
    "import json\n",
    "with open(PROCESSED_DATA_DIR / 'blumenstock_variables.json', 'w') as f:\n",
    "    json.dump(var_config, f, indent=2)\n",
    "print(\"Variable config saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43ijjx4a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL VERIFICATION: Checking Saved Dataset\n",
      "======================================================================\n",
      "\n",
      "Reloaded dataset: 110,000 rows, 28 columns\n",
      "\n",
      "=== Critical Variable Verification ===\n",
      "  ✓ hpi_st_d_t_o: 100.0% coverage, mean=126.751, std=132.813\n",
      "  ✓ ppi_c_FRMA: 100.0% coverage, mean=-0.674, std=1.831\n",
      "  ✓ TB10Y_d_t_o: 100.0% coverage, mean=0.477, std=1.506\n",
      "  ✓ FRMA30Y_d_t_o: 100.0% coverage, mean=0.703, std=1.706\n",
      "  ✓ ppi_o_FRMA: 100.0% coverage, mean=0.029, std=0.617\n",
      "\n",
      "=== Quick Statistics ===\n",
      "Total variables: 28\n",
      "Loan-level vars present: 9/9\n",
      "Macro vars present: 12/12\n",
      "\n",
      "======================================================================\n",
      "✓ VERIFICATION PASSED: All critical variables present with good coverage\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === FINAL VERIFICATION: Reload and verify saved dataset ===\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL VERIFICATION: Checking Saved Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reload the saved dataset\n",
    "verify_df = pd.read_parquet(output_path)\n",
    "print(f\"\\nReloaded dataset: {len(verify_df):,} rows, {len(verify_df.columns)} columns\")\n",
    "\n",
    "# Check all required variables are present and have good coverage\n",
    "print(\"\\n=== Critical Variable Verification ===\")\n",
    "\n",
    "critical_macro_vars = ['hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o', 'ppi_o_FRMA']\n",
    "all_critical_ok = True\n",
    "\n",
    "for var in critical_macro_vars:\n",
    "    if var in verify_df.columns:\n",
    "        coverage = verify_df[var].notna().mean()\n",
    "        mean_val = verify_df[var].mean()\n",
    "        std_val = verify_df[var].std()\n",
    "        \n",
    "        if coverage < 0.9:\n",
    "            print(f\"  ⚠️  {var}: {coverage:.1%} coverage (LOW!)\")\n",
    "            all_critical_ok = False\n",
    "        elif verify_df[var].isna().all():\n",
    "            print(f\"  ✗ {var}: ALL NaN VALUES!\")\n",
    "            all_critical_ok = False\n",
    "        else:\n",
    "            print(f\"  ✓ {var}: {coverage:.1%} coverage, mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {var}: MISSING FROM SAVED DATASET!\")\n",
    "        all_critical_ok = False\n",
    "\n",
    "# Summary statistics for all variables\n",
    "print(\"\\n=== Quick Statistics ===\")\n",
    "print(f\"Total variables: {len(verify_df.columns)}\")\n",
    "print(f\"Loan-level vars present: {len([v for v in LOAN_LEVEL_VARS if v in verify_df.columns])}/{len(LOAN_LEVEL_VARS)}\")\n",
    "print(f\"Macro vars present: {len([v for v in MACRO_VARS if v in verify_df.columns])}/{len(MACRO_VARS)}\")\n",
    "\n",
    "# Final status\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_critical_ok:\n",
    "    print(\"✓ VERIFICATION PASSED: All critical variables present with good coverage\")\n",
    "else:\n",
    "    print(\"✗ VERIFICATION FAILED: Some critical variables missing or have low coverage\")\n",
    "    print(\"  Check the warnings above and re-run data preparation if needed.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLUMENSTOCK DATASET 2 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Period: 2010-2025\n",
      "Total observations: 110,000\n",
      "Number of folds: 11\n",
      "Per fold: 100 defaults + 9900 non-defaults\n",
      "\n",
      "=== Event Distribution ===\n",
      "  Censored (k=0): 47,386 (43.1%)\n",
      "  Prepayment (k=1): 61,514 (55.9%)\n",
      "  Default (k=2): 1,100 (1.0%)\n",
      "\n",
      "=== Variable Summary ===\n",
      "Loan-level variables: 9\n",
      "Macro variables: 12\n",
      "\n",
      "=== Duration Statistics ===\n",
      "count    110000.000000\n",
      "mean         49.447473\n",
      "std          39.810387\n",
      "min           0.000000\n",
      "25%          19.000000\n",
      "50%          39.000000\n",
      "75%          68.000000\n",
      "max         184.000000\n",
      "Name: duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BLUMENSTOCK DATASET 2 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPeriod: {final_df['vintage_year'].min()}-{final_df['vintage_year'].max()}\")\n",
    "print(f\"Total observations: {len(final_df):,}\")\n",
    "print(f\"Number of folds: {final_df['fold'].nunique()}\")\n",
    "print(f\"Per fold: {DEFAULT_SAMPLES_PER_FOLD} defaults + {NON_DEFAULT_SAMPLES_PER_FOLD} non-defaults\")\n",
    "\n",
    "print(f\"\\n=== Event Distribution ===\")\n",
    "event_counts = final_df['event_code'].value_counts().sort_index()\n",
    "for code, count in event_counts.items():\n",
    "    event_name = {0: 'Censored', 1: 'Prepayment', 2: 'Default'}.get(code, 'Other')\n",
    "    pct = count / len(final_df) * 100\n",
    "    print(f\"  {event_name} (k={code}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== Variable Summary ===\")\n",
    "print(f\"Loan-level variables: {len([v for v in LOAN_LEVEL_VARS if v in final_df.columns])}\")\n",
    "print(f\"Macro variables: {len([v for v in MACRO_VARS if v in final_df.columns])}\")\n",
    "\n",
    "print(f\"\\n=== Duration Statistics ===\")\n",
    "print(final_df['duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Data is ready for experiments:\n",
    "\n",
    "1. **Notebook 05**: Cause-Specific Cox (CSC)\n",
    "2. **Notebook 06**: Fine-Gray Model (FGR)\n",
    "3. **Notebook 07**: Random Survival Forest (RSF)\n",
    "4. **Notebook 08**: Model Comparison\n",
    "\n",
    "### Experiments (from paper)\n",
    "\n",
    "| Experiment | Variables | File |\n",
    "|------------|-----------|------|\n",
    "| Exp 4.1 | Loan-level only | Use `LOAN_LEVEL_VARS` |\n",
    "| Exp 4.2 | Macro only | Use `MACRO_VARS` |\n",
    "| Exp 4.3 | All variables | Use `ALL_VARS` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}