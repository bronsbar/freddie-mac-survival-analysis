{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Data Preparation: Blumenstock et al. (2022) Replication\n",
    "\n",
    "This notebook prepares data following the methodology from:\n",
    "\n",
    "> Blumenstock, G., Lessmann, S., & Seow, H-V. (2022). Deep learning for survival and competing risk modelling. *Journal of the Operational Research Society*, 73(1), 26-38.\n",
    "\n",
    "## Dataset 2: Post-Crisis Period (2010-2025)\n",
    "\n",
    "**Variables from Table 2:**\n",
    "\n",
    "### Loan-Level Variables (9)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `int_rate` | Initial interest rate |\n",
    "| `orig_upb` | Original unpaid balance |\n",
    "| `fico_score` | Initial FICO score |\n",
    "| `dti_r` | Initial debt-to-income ratio |\n",
    "| `ltv_r` | Initial loan-to-value ratio |\n",
    "| `bal_repaid` | Current repaid balance in percent |\n",
    "| `t_act_12m` | No. of times not being delinquent in last 12 months |\n",
    "| `t_del_30d_12m` | No. of times being 30 days delinquent in last 12 months |\n",
    "| `t_del_60d_12m` | No. of times being 60 days delinquent in last 12 months |\n",
    "\n",
    "### Macroeconomic Variables (13 for Dataset 2)\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `hpi_st_d_t_o` | HPI difference between origination and today (state) |\n",
    "| `ppi_c_FRMA` | Current prepayment incentive |\n",
    "| `TB10Y_d_t_o` | Treasury rate difference |\n",
    "| `FRMA30Y_d_t_o` | 30Y FRM difference |\n",
    "| `ppi_o_FRMA` | Prepayment incentive at origination |\n",
    "| `hpi_st_log12m` | HPI 12-month log return (state) |\n",
    "| `hpi_r_st_us` | Ratio of state HPI to national HPI |\n",
    "| `st_unemp_r12m` | Unemployment 12-month log return (state) |\n",
    "| `st_unemp_r3m` | Unemployment 3-month log return (state) |\n",
    "| `TB10Y_r12m` | Treasury rate 12-month return |\n",
    "| `T10Y3MM` | Yield spread (10Y - 3M) |\n",
    "| `T10Y3MM_r12m` | Yield spread 12-month return |\n",
    "\n",
    "### Event Definitions\n",
    "- **Default (k=2)**: Loan turning 3-month delinquent for the first time\n",
    "- **Prepayment (k=1)**: Loan repaid completely and unexpectedly\n",
    "- **Censored (k=0)**: Active loan without event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import column definitions\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.data.columns import (\n",
    "    ORIGINATION_COLUMNS, ORIGINATION_DTYPES,\n",
    "    PERFORMANCE_COLUMNS, PERFORMANCE_DTYPES,\n",
    "    ZERO_BALANCE_CODE_MAP\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following the paper's Dataset 2 setup (2010-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Data paths\nRAW_DATA_DIR = Path('../data/raw')\nPROCESSED_DATA_DIR = Path('../data/processed')\nEXTERNAL_DATA_DIR = Path('../data/external')\n\n# Dataset 2: Post-crisis period (2010-2025)\nVINTAGES = list(range(2010, 2026))\n\n# Sampling strategy from paper:\n# - 11 random subsamples of 10,000 each\n# - 10 for cross-validation, 1 for hyperparameter tuning\nSAMPLE_SIZE_PER_FOLD = 10000\nN_FOLDS = 11\n\n# Default definition: 3-month delinquent for the first time\nDEFAULT_DELINQUENCY_THRESHOLD = 3\n\nprint(f\"Dataset 2 Period: {min(VINTAGES)}-{max(VINTAGES)}\")\nprint(f\"Sample size per fold: {SAMPLE_SIZE_PER_FOLD:,}\")\nprint(f\"Number of folds: {N_FOLDS}\")\n\n# === VERIFY REQUIRED FILES EXIST ===\nprint(\"\\n=== Checking Required Files ===\")\nrequired_files = {\n    'National macro data': EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet',\n    'State unemployment': EXTERNAL_DATA_DIR / 'state_unemployment.parquet',\n    'State HPI': EXTERNAL_DATA_DIR / 'state_hpi.parquet',\n}\n\nmissing_files = []\nfor name, path in required_files.items():\n    if path.exists():\n        print(f\"  ✓ {name}: {path}\")\n    else:\n        print(f\"  ✗ {name}: MISSING - {path}\")\n        missing_files.append(name)\n\nif missing_files:\n    print(f\"\\n⚠️  WARNING: {len(missing_files)} required file(s) missing!\")\n    print(\"   Run: python -m src.data.download_fred --include-states\")\n    print(\"   Some macro variables will not be calculated.\")\nelse:\n    print(\"\\n✓ All required files present.\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-macro-header",
   "metadata": {},
   "source": [
    "## Step 1: Load Macroeconomic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-macro",
   "metadata": {},
   "outputs": [],
   "source": "# Load national macro data\nmacro_path = EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet'\n\nif macro_path.exists():\n    macro_national = pd.read_parquet(macro_path)\n    macro_national.index.name = 'date'\n    macro_national = macro_national.reset_index()\n    macro_national['date'] = pd.to_datetime(macro_national['date'])\n    macro_national['year_month'] = macro_national['date'].dt.to_period('M')\n\n    # Verify required columns exist\n    required_macro_cols = ['MORTGAGE30US', 'DGS10', 'DGS3MO']\n    missing_cols = [c for c in required_macro_cols if c not in macro_national.columns]\n    if missing_cols:\n        print(f\"⚠️  WARNING: Missing columns in macro data: {missing_cols}\")\n    \n    # Calculate additional variables needed for paper\n    # TB10Y_r12m: 10-year treasury rate 12-month return\n    if 'DGS10' in macro_national.columns:\n        macro_national['TB10Y_r12m'] = macro_national['DGS10'].pct_change(12)\n\n    # T10Y3MM: Yield spread (need 3-month rate)\n    if 'DGS10' in macro_national.columns and 'DGS3MO' in macro_national.columns:\n        macro_national['T10Y3MM'] = macro_national['DGS10'] - macro_national['DGS3MO']\n        # T10Y3MM_r12m: Yield spread 12-month return\n        macro_national['T10Y3MM_r12m'] = macro_national['T10Y3MM'].pct_change(12)\n\n    print(f\"✓ National macro data loaded: {macro_national.shape}\")\n    print(f\"  Date range: {macro_national['date'].min()} to {macro_national['date'].max()}\")\n    print(f\"  Columns: {list(macro_national.columns)[:10]}...\")\nelse:\n    print(f\"✗ ERROR: National macro data not found at {macro_path}\")\n    print(\"  Run: python -m src.data.download_fred\")\n    macro_national = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-state-unemp",
   "metadata": {},
   "outputs": [],
   "source": "# Load state-level unemployment\nunemp_path = EXTERNAL_DATA_DIR / 'state_unemployment.parquet'\n\nif unemp_path.exists():\n    state_unemp = pd.read_parquet(unemp_path)\n    state_unemp.index.name = 'date'\n    state_unemp = state_unemp.reset_index()\n    state_unemp['date'] = pd.to_datetime(state_unemp['date'])\n    state_unemp['year_month'] = state_unemp['date'].dt.to_period('M')\n\n    # Melt to long format\n    state_cols = [c for c in state_unemp.columns if '_unemployment' in c]\n    \n    if len(state_cols) == 0:\n        print(\"⚠️  WARNING: No unemployment columns found in state data\")\n        state_unemp_long = pd.DataFrame()\n    else:\n        state_unemp_long = state_unemp.melt(\n            id_vars=['date', 'year_month'],\n            value_vars=state_cols,\n            var_name='state_col',\n            value_name='state_unemployment'\n        )\n        state_unemp_long['property_state'] = state_unemp_long['state_col'].str.replace('_unemployment', '')\n\n        # Calculate returns by state (need to sort first)\n        state_unemp_long = state_unemp_long.sort_values(['property_state', 'year_month'])\n        \n        # Group by state and calculate rolling returns\n        def calc_state_returns(group):\n            group = group.copy()\n            group['st_unemp_r12m'] = np.log(group['state_unemployment'] / group['state_unemployment'].shift(12))\n            group['st_unemp_r3m'] = np.log(group['state_unemployment'] / group['state_unemployment'].shift(3))\n            return group\n        \n        state_unemp_long = state_unemp_long.groupby('property_state', group_keys=False).apply(calc_state_returns)\n        \n        state_unemp_long = state_unemp_long[['year_month', 'property_state', 'state_unemployment', \n                                              'st_unemp_r12m', 'st_unemp_r3m']].drop_duplicates()\n\n        print(f\"✓ State unemployment loaded: {state_unemp_long.shape}\")\n        print(f\"  States: {state_unemp_long['property_state'].nunique()}\")\n        print(f\"  Sample: {state_unemp_long['property_state'].unique()[:5].tolist()}...\")\nelse:\n    print(f\"✗ ERROR: State unemployment data not found at {unemp_path}\")\n    print(\"  Run: python -m src.data.download_fred --include-states\")\n    state_unemp_long = pd.DataFrame(columns=['year_month', 'property_state', 'st_unemp_r12m', 'st_unemp_r3m'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-state-hpi",
   "metadata": {},
   "outputs": [],
   "source": "# Load state-level HPI\nhpi_path = EXTERNAL_DATA_DIR / 'state_hpi.parquet'\n\nif hpi_path.exists():\n    state_hpi = pd.read_parquet(hpi_path)\n    state_hpi.index.name = 'date'\n    state_hpi = state_hpi.reset_index()\n    state_hpi['date'] = pd.to_datetime(state_hpi['date'])\n    state_hpi['year_month'] = state_hpi['date'].dt.to_period('M')\n\n    # Find HPI columns (2-letter state code + _hpi)\n    hpi_cols = [c for c in state_hpi.columns if c.endswith('_hpi') and len(c) <= 6]\n    \n    if len(hpi_cols) == 0:\n        print(\"⚠️  WARNING: No HPI columns found in state data\")\n        state_hpi_long = pd.DataFrame()\n    else:\n        # Calculate national HPI (average across states)\n        state_hpi['national_hpi'] = state_hpi[hpi_cols].mean(axis=1)\n\n        # Melt to long format\n        state_hpi_long = state_hpi.melt(\n            id_vars=['date', 'year_month', 'national_hpi'],\n            value_vars=hpi_cols,\n            var_name='state_col',\n            value_name='state_hpi'\n        )\n        state_hpi_long['property_state'] = state_hpi_long['state_col'].str.replace('_hpi', '')\n\n        # Sort by state and time for proper rolling calculations\n        state_hpi_long = state_hpi_long.sort_values(['property_state', 'year_month'])\n\n        # Calculate log returns by state\n        def calc_hpi_returns(group):\n            group = group.copy()\n            group['hpi_st_log12m'] = np.log(group['state_hpi'] / group['state_hpi'].shift(12))\n            return group\n        \n        state_hpi_long = state_hpi_long.groupby('property_state', group_keys=False).apply(calc_hpi_returns)\n\n        # Calculate ratio of state HPI to national HPI\n        state_hpi_long['hpi_r_st_us'] = state_hpi_long['state_hpi'] / state_hpi_long['national_hpi']\n\n        state_hpi_long = state_hpi_long[['year_month', 'property_state', 'state_hpi', 'national_hpi',\n                                          'hpi_st_log12m', 'hpi_r_st_us']].drop_duplicates()\n\n        print(f\"✓ State HPI loaded: {state_hpi_long.shape}\")\n        print(f\"  States: {state_hpi_long['property_state'].nunique()}\")\n        print(f\"  Sample: {state_hpi_long['property_state'].unique()[:5].tolist()}...\")\nelse:\n    print(f\"✗ ERROR: State HPI data not found at {hpi_path}\")\n    print(\"  Run: python -m src.data.download_fred --include-states\")\n    state_hpi_long = pd.DataFrame(columns=['year_month', 'property_state', 'state_hpi', 'national_hpi', \n                                            'hpi_st_log12m', 'hpi_r_st_us'])"
  },
  {
   "cell_type": "markdown",
   "id": "load-loans-header",
   "metadata": {},
   "source": [
    "## Step 2: Load and Process Loan Data\n",
    "\n",
    "Process Freddie Mac data with paper's variable definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_origination_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load origination data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_orig_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=ORIGINATION_COLUMNS,\n",
    "        dtype=ORIGINATION_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_performance_data(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"Load performance (monthly) data for a vintage.\"\"\"\n",
    "    pattern = f'sample_{vintage}/sample_svcg_{vintage}.txt'\n",
    "    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        files[0], sep='|', names=PERFORMANCE_COLUMNS,\n",
    "        dtype=PERFORMANCE_DTYPES, na_values=['', ' ']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Load functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "process-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process function defined.\n"
     ]
    }
   ],
   "source": [
    "def process_vintage_blumenstock(vintage: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a vintage following Blumenstock et al. (2022) methodology.\n",
    "    \n",
    "    Creates loan-level survival data with:\n",
    "    - Terminal record per loan\n",
    "    - Behavioral features from last 12 months\n",
    "    - Paper's variable definitions\n",
    "    \"\"\"\n",
    "    print(f\"Processing vintage {vintage}...\")\n",
    "    \n",
    "    # Load data\n",
    "    orig_df = load_origination_data(vintage)\n",
    "    perf_df = load_performance_data(vintage)\n",
    "    \n",
    "    if orig_df.empty or perf_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse reporting period\n",
    "    perf_df['reporting_date'] = pd.to_datetime(\n",
    "        perf_df['monthly_reporting_period'].astype(str), format='%Y%m'\n",
    "    )\n",
    "    perf_df['year_month'] = perf_df['reporting_date'].dt.to_period('M')\n",
    "    \n",
    "    # Parse delinquency status\n",
    "    perf_df['delinquency_status'] = pd.to_numeric(\n",
    "        perf_df['current_loan_delinquency_status'].replace({'X': '0', 'XX': '0'}),\n",
    "        errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    # Sort by loan and time\n",
    "    perf_df = perf_df.sort_values(['loan_sequence_number', 'loan_age'])\n",
    "    \n",
    "    # === Calculate behavioral variables (rolling 12-month) ===\n",
    "    perf_df['is_current'] = (perf_df['delinquency_status'] == 0).astype(int)\n",
    "    perf_df['is_30d_del'] = (perf_df['delinquency_status'] == 1).astype(int)\n",
    "    perf_df['is_60d_del'] = (perf_df['delinquency_status'] == 2).astype(int)\n",
    "    \n",
    "    # Rolling counts over last 12 months\n",
    "    grouped = perf_df.groupby('loan_sequence_number')\n",
    "    perf_df['t_act_12m'] = grouped['is_current'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_30d_12m'] = grouped['is_30d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    perf_df['t_del_60d_12m'] = grouped['is_60d_del'].transform(\n",
    "        lambda x: x.rolling(12, min_periods=1).sum()\n",
    "    )\n",
    "    \n",
    "    # === Determine event type ===\n",
    "    # Default: first time reaching 90+ days delinquent\n",
    "    perf_df['is_default'] = (perf_df['delinquency_status'] >= DEFAULT_DELINQUENCY_THRESHOLD).astype(int)\n",
    "    perf_df['first_default'] = grouped['is_default'].transform(\n",
    "        lambda x: (x.cumsum() == 1) & (x == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Prepayment: zero balance code = 01 (not at maturity)\n",
    "    perf_df['is_prepay'] = (\n",
    "        (perf_df['zero_balance_code'] == '01') & \n",
    "        (perf_df['loan_age'] < perf_df['remaining_months_to_maturity'].fillna(360) + perf_df['loan_age'] - 6)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # === Calculate balance repaid ===\n",
    "    orig_upb = orig_df.set_index('loan_sequence_number')['orig_upb']\n",
    "    perf_df['orig_upb_lookup'] = perf_df['loan_sequence_number'].map(orig_upb)\n",
    "    perf_df['bal_repaid'] = (\n",
    "        (perf_df['orig_upb_lookup'] - perf_df['current_actual_upb'].fillna(0)) / \n",
    "        perf_df['orig_upb_lookup']\n",
    "    ) * 100\n",
    "    perf_df['bal_repaid'] = perf_df['bal_repaid'].clip(0, 100)\n",
    "    \n",
    "    # === Get terminal record for each loan ===\n",
    "    # Determine event at terminal record\n",
    "    def get_terminal_event(group):\n",
    "        \"\"\"Get terminal record with event type.\"\"\"\n",
    "        last_row = group.iloc[-1].copy()\n",
    "        \n",
    "        # Check for default (first 90+ delinquency)\n",
    "        default_rows = group[group['first_default'] == 1]\n",
    "        if len(default_rows) > 0:\n",
    "            last_row = default_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 2  # Default\n",
    "            return last_row\n",
    "        \n",
    "        # Check for prepayment\n",
    "        prepay_rows = group[group['is_prepay'] == 1]\n",
    "        if len(prepay_rows) > 0:\n",
    "            last_row = prepay_rows.iloc[0].copy()\n",
    "            last_row['event_code'] = 1  # Prepay\n",
    "            return last_row\n",
    "        \n",
    "        # Censored\n",
    "        last_row['event_code'] = 0\n",
    "        return last_row\n",
    "    \n",
    "    print(f\"  Getting terminal records...\")\n",
    "    terminal_df = perf_df.groupby('loan_sequence_number').apply(get_terminal_event)\n",
    "    terminal_df = terminal_df.reset_index(drop=True)\n",
    "    \n",
    "    # === Merge with origination data ===\n",
    "    orig_cols = [\n",
    "        'loan_sequence_number', 'credit_score', 'orig_ltv', 'orig_dti',\n",
    "        'orig_upb', 'orig_interest_rate', 'orig_loan_term',\n",
    "        'first_payment_date', 'property_state'\n",
    "    ]\n",
    "    orig_subset = orig_df[[c for c in orig_cols if c in orig_df.columns]].copy()\n",
    "    orig_subset['vintage_year'] = vintage\n",
    "    \n",
    "    # Parse origination date\n",
    "    orig_subset['first_payment_date'] = pd.to_datetime(\n",
    "        orig_subset['first_payment_date'].astype(str), format='%Y%m', errors='coerce'\n",
    "    )\n",
    "    orig_subset['orig_year_month'] = orig_subset['first_payment_date'].dt.to_period('M')\n",
    "    \n",
    "    terminal_df = terminal_df.merge(orig_subset, on='loan_sequence_number', how='left')\n",
    "    \n",
    "    print(f\"  Loans: {len(terminal_df):,}\")\n",
    "    print(f\"  Events: Prepay={sum(terminal_df['event_code']==1):,}, \"\n",
    "          f\"Default={sum(terminal_df['event_code']==2):,}, \"\n",
    "          f\"Censored={sum(terminal_df['event_code']==0):,}\")\n",
    "    \n",
    "    return terminal_df\n",
    "\n",
    "\n",
    "print(\"Process function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "process-all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vintage 2010...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=44,437, Default=1,614, Censored=3,949\n",
      "Processing vintage 2011...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=42,938, Default=1,528, Censored=5,534\n",
      "Processing vintage 2012...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,822, Default=1,698, Censored=10,480\n",
      "Processing vintage 2013...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,060, Default=1,882, Censored=11,058\n",
      "Processing vintage 2014...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=39,244, Default=1,919, Censored=8,837\n",
      "Processing vintage 2015...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=36,516, Default=2,023, Censored=11,461\n",
      "Processing vintage 2016...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=33,910, Default=2,345, Censored=13,745\n",
      "Processing vintage 2017...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=35,261, Default=2,877, Censored=11,862\n",
      "Processing vintage 2018...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=37,971, Default=2,670, Censored=9,359\n",
      "Processing vintage 2019...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=33,517, Default=2,725, Censored=13,758\n",
      "Processing vintage 2020...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=17,834, Default=1,022, Censored=31,144\n",
      "Processing vintage 2021...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=8,029, Default=850, Censored=41,121\n",
      "Processing vintage 2022...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=7,061, Default=1,368, Censored=41,571\n",
      "Processing vintage 2023...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=7,684, Default=789, Censored=41,527\n",
      "Processing vintage 2024...\n",
      "  Getting terminal records...\n",
      "  Loans: 50,000\n",
      "  Events: Prepay=3,319, Default=215, Censored=46,466\n",
      "Processing vintage 2025...\n",
      "  Getting terminal records...\n",
      "  Loans: 24,950\n",
      "  Events: Prepay=199, Default=2, Censored=24,749\n",
      "\n",
      "Combining all vintages...\n",
      "Total loans: 774,950\n"
     ]
    }
   ],
   "source": [
    "# Process all vintages in Dataset 2\n",
    "all_loans = []\n",
    "\n",
    "for vintage in VINTAGES:\n",
    "    df = process_vintage_blumenstock(vintage)\n",
    "    if not df.empty:\n",
    "        all_loans.append(df)\n",
    "\n",
    "# Combine\n",
    "print(\"\\nCombining all vintages...\")\n",
    "loans_df = pd.concat(all_loans, ignore_index=True)\n",
    "print(f\"Total loans: {len(loans_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-macro-header",
   "metadata": {},
   "source": [
    "## Step 3: Merge Macroeconomic Variables\n",
    "\n",
    "Add paper's macroeconomic variables at the observation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-macro",
   "metadata": {},
   "outputs": [],
   "source": "# Merge macroeconomic data at observation time\nprint(\"=== Merging Macro Data at Observation Time ===\\n\")\n\nn_before = len(loans_df)\n\n# Merge state unemployment data\nif not state_unemp_long.empty:\n    print(\"Merging state unemployment...\")\n    loans_df = loans_df.merge(\n        state_unemp_long,\n        on=['year_month', 'property_state'],\n        how='left'\n    )\n    coverage = loans_df['st_unemp_r12m'].notna().mean()\n    print(f\"  ✓ st_unemp_r12m coverage: {coverage:.1%}\")\n    print(f\"  ✓ st_unemp_r3m coverage: {loans_df['st_unemp_r3m'].notna().mean():.1%}\")\nelse:\n    print(\"⚠️  Skipping state unemployment merge (data not loaded)\")\n    loans_df['st_unemp_r12m'] = np.nan\n    loans_df['st_unemp_r3m'] = np.nan\n\n# Merge state HPI data  \nif not state_hpi_long.empty:\n    print(\"\\nMerging state HPI...\")\n    loans_df = loans_df.merge(\n        state_hpi_long,\n        on=['year_month', 'property_state'],\n        how='left'\n    )\n    print(f\"  ✓ state_hpi coverage: {loans_df['state_hpi'].notna().mean():.1%}\")\n    print(f\"  ✓ hpi_st_log12m coverage: {loans_df['hpi_st_log12m'].notna().mean():.1%}\")\n    print(f\"  ✓ hpi_r_st_us coverage: {loans_df['hpi_r_st_us'].notna().mean():.1%}\")\nelse:\n    print(\"⚠️  Skipping state HPI merge (data not loaded)\")\n    loans_df['state_hpi'] = np.nan\n    loans_df['national_hpi'] = np.nan\n    loans_df['hpi_st_log12m'] = np.nan\n    loans_df['hpi_r_st_us'] = np.nan\n\n# Merge national macro data\nif macro_national is not None:\n    print(\"\\nMerging national macro...\")\n    macro_cols = ['year_month', 'MORTGAGE30US', 'DGS10', 'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m']\n    macro_cols = [c for c in macro_cols if c in macro_national.columns]\n    macro_subset = macro_national[macro_cols].copy()\n    loans_df = loans_df.merge(macro_subset, on='year_month', how='left')\n    print(f\"  ✓ MORTGAGE30US coverage: {loans_df['MORTGAGE30US'].notna().mean():.1%}\")\n    print(f\"  ✓ DGS10 coverage: {loans_df['DGS10'].notna().mean():.1%}\")\n    if 'TB10Y_r12m' in loans_df.columns:\n        print(f\"  ✓ TB10Y_r12m coverage: {loans_df['TB10Y_r12m'].notna().mean():.1%}\")\n    if 'T10Y3MM' in loans_df.columns:\n        print(f\"  ✓ T10Y3MM coverage: {loans_df['T10Y3MM'].notna().mean():.1%}\")\nelse:\n    print(\"⚠️  Skipping national macro merge (data not loaded)\")\n    loans_df['MORTGAGE30US'] = np.nan\n    loans_df['DGS10'] = np.nan\n    loans_df['TB10Y_r12m'] = np.nan\n    loans_df['T10Y3MM'] = np.nan\n    loans_df['T10Y3MM_r12m'] = np.nan\n\n# Verify no rows were lost\nn_after = len(loans_df)\nif n_after != n_before:\n    print(f\"\\n⚠️  WARNING: Row count changed from {n_before:,} to {n_after:,} during merge!\")\nelse:\n    print(f\"\\n✓ Row count unchanged: {n_after:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-orig-macro",
   "metadata": {},
   "outputs": [],
   "source": "# Get origination-time values for difference calculations\nprint(\"=== Calculating Origination-Time Differences ===\\n\")\n\n# Track which variables were successfully created\ncreated_vars = []\nfailed_vars = []\n\n# === 1. HPI difference (state-level) ===\nif not state_hpi_long.empty and 'state_hpi' in loans_df.columns:\n    print(\"Calculating hpi_st_d_t_o (HPI difference)...\")\n    \n    # Get origination-time state HPI\n    orig_hpi = state_hpi_long[['year_month', 'property_state', 'state_hpi']].rename(\n        columns={'year_month': 'orig_year_month', 'state_hpi': 'orig_state_hpi'}\n    )\n    loans_df = loans_df.merge(orig_hpi, on=['orig_year_month', 'property_state'], how='left')\n    \n    # hpi_st_d_t_o: Difference of HPI between origination and today (state-level)\n    loans_df['hpi_st_d_t_o'] = loans_df['state_hpi'] - loans_df['orig_state_hpi']\n    \n    coverage = loans_df['hpi_st_d_t_o'].notna().mean()\n    print(f\"  ✓ hpi_st_d_t_o coverage: {coverage:.1%}\")\n    if coverage > 0.9:\n        created_vars.append('hpi_st_d_t_o')\n    else:\n        failed_vars.append(('hpi_st_d_t_o', f'{coverage:.1%} coverage'))\nelse:\n    print(\"⚠️  Cannot calculate hpi_st_d_t_o (state HPI not available)\")\n    loans_df['hpi_st_d_t_o'] = np.nan\n    failed_vars.append(('hpi_st_d_t_o', 'state HPI not loaded'))\n\n# === 2. Interest rate and mortgage differences ===\nif macro_national is not None and 'MORTGAGE30US' in loans_df.columns:\n    print(\"\\nCalculating prepayment incentives and rate differences...\")\n    \n    # Get origination-time macro rates\n    orig_macro = macro_national[['year_month', 'MORTGAGE30US', 'DGS10']].rename(\n        columns={'year_month': 'orig_year_month', 'MORTGAGE30US': 'orig_MORTGAGE30US', 'DGS10': 'orig_DGS10'}\n    )\n    loans_df = loans_df.merge(orig_macro, on='orig_year_month', how='left')\n    \n    # Check if we have the required columns\n    has_int_rate = 'orig_interest_rate' in loans_df.columns\n    has_mortgage = 'MORTGAGE30US' in loans_df.columns and 'orig_MORTGAGE30US' in loans_df.columns\n    has_treasury = 'DGS10' in loans_df.columns and 'orig_DGS10' in loans_df.columns\n    \n    # ppi_c_FRMA: Current prepayment incentive (loan rate - current mortgage rate)\n    if has_int_rate and has_mortgage:\n        loans_df['ppi_c_FRMA'] = loans_df['orig_interest_rate'] - loans_df['MORTGAGE30US']\n        coverage = loans_df['ppi_c_FRMA'].notna().mean()\n        print(f\"  ✓ ppi_c_FRMA coverage: {coverage:.1%}\")\n        created_vars.append('ppi_c_FRMA') if coverage > 0.9 else failed_vars.append(('ppi_c_FRMA', f'{coverage:.1%}'))\n    else:\n        loans_df['ppi_c_FRMA'] = np.nan\n        failed_vars.append(('ppi_c_FRMA', 'missing required columns'))\n\n    # ppi_o_FRMA: Prepayment incentive at origination (loan rate - orig mortgage rate)\n    if has_int_rate and has_mortgage:\n        loans_df['ppi_o_FRMA'] = loans_df['orig_interest_rate'] - loans_df['orig_MORTGAGE30US']\n        coverage = loans_df['ppi_o_FRMA'].notna().mean()\n        print(f\"  ✓ ppi_o_FRMA coverage: {coverage:.1%}\")\n        created_vars.append('ppi_o_FRMA') if coverage > 0.9 else failed_vars.append(('ppi_o_FRMA', f'{coverage:.1%}'))\n    else:\n        loans_df['ppi_o_FRMA'] = np.nan\n        failed_vars.append(('ppi_o_FRMA', 'missing required columns'))\n\n    # TB10Y_d_t_o: Difference of 10-year treasury rate (today - origination)\n    if has_treasury:\n        loans_df['TB10Y_d_t_o'] = loans_df['DGS10'] - loans_df['orig_DGS10']\n        coverage = loans_df['TB10Y_d_t_o'].notna().mean()\n        print(f\"  ✓ TB10Y_d_t_o coverage: {coverage:.1%}\")\n        created_vars.append('TB10Y_d_t_o') if coverage > 0.9 else failed_vars.append(('TB10Y_d_t_o', f'{coverage:.1%}'))\n    else:\n        loans_df['TB10Y_d_t_o'] = np.nan\n        failed_vars.append(('TB10Y_d_t_o', 'missing DGS10'))\n\n    # FRMA30Y_d_t_o: Difference of 30-year FRM average (today - origination)\n    if has_mortgage:\n        loans_df['FRMA30Y_d_t_o'] = loans_df['MORTGAGE30US'] - loans_df['orig_MORTGAGE30US']\n        coverage = loans_df['FRMA30Y_d_t_o'].notna().mean()\n        print(f\"  ✓ FRMA30Y_d_t_o coverage: {coverage:.1%}\")\n        created_vars.append('FRMA30Y_d_t_o') if coverage > 0.9 else failed_vars.append(('FRMA30Y_d_t_o', f'{coverage:.1%}'))\n    else:\n        loans_df['FRMA30Y_d_t_o'] = np.nan\n        failed_vars.append(('FRMA30Y_d_t_o', 'missing MORTGAGE30US'))\n        \nelse:\n    print(\"⚠️  Cannot calculate rate differences (macro data not available)\")\n    for var in ['ppi_c_FRMA', 'ppi_o_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o']:\n        loans_df[var] = np.nan\n        failed_vars.append((var, 'macro data not loaded'))\n\n# === Summary ===\nprint(f\"\\n=== Derived Variables Summary ===\")\nprint(f\"✓ Successfully created: {len(created_vars)}\")\nfor var in created_vars:\n    print(f\"    - {var}\")\n\nif failed_vars:\n    print(f\"\\n⚠️  Failed or low coverage: {len(failed_vars)}\")\n    for var, reason in failed_vars:\n        print(f\"    - {var}: {reason}\")"
  },
  {
   "cell_type": "markdown",
   "id": "rename-header",
   "metadata": {},
   "source": [
    "## Step 4: Rename Variables to Paper's Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rename-vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan-level variables: 9\n",
      "Macro variables: 12\n",
      "Total: 21\n"
     ]
    }
   ],
   "source": [
    "# Rename to paper's variable names\n",
    "rename_map = {\n",
    "    # Loan-level\n",
    "    'orig_interest_rate': 'int_rate',\n",
    "    'credit_score': 'fico_score',\n",
    "    'orig_dti': 'dti_r',\n",
    "    'orig_ltv': 'ltv_r',\n",
    "    # Duration\n",
    "    'loan_age': 'duration',\n",
    "}\n",
    "\n",
    "loans_df = loans_df.rename(columns=rename_map)\n",
    "\n",
    "# Define final variable sets (from paper Table 2)\n",
    "LOAN_LEVEL_VARS = [\n",
    "    'int_rate',           # Initial interest rate\n",
    "    'orig_upb',           # Original unpaid balance\n",
    "    'fico_score',         # Initial FICO score\n",
    "    'dti_r',              # Initial debt-to-income ratio\n",
    "    'ltv_r',              # Initial loan-to-value ratio\n",
    "    'bal_repaid',         # Current repaid balance in percent\n",
    "    't_act_12m',          # Times not delinquent in last 12 months\n",
    "    't_del_30d_12m',      # Times 30 days delinquent in last 12 months\n",
    "    't_del_60d_12m',      # Times 60 days delinquent in last 12 months\n",
    "]\n",
    "\n",
    "MACRO_VARS = [\n",
    "    'hpi_st_d_t_o',       # HPI difference (state)\n",
    "    'ppi_c_FRMA',         # Current prepayment incentive\n",
    "    'TB10Y_d_t_o',        # Treasury rate difference\n",
    "    'FRMA30Y_d_t_o',      # 30Y FRM difference\n",
    "    'ppi_o_FRMA',         # Prepayment incentive at origination\n",
    "    'hpi_st_log12m',      # HPI 12-month log return (state)\n",
    "    'hpi_r_st_us',        # Ratio of state HPI to national HPI\n",
    "    'st_unemp_r12m',      # Unemployment 12-month log return (state)\n",
    "    'st_unemp_r3m',       # Unemployment 3-month log return (state)\n",
    "    'TB10Y_r12m',         # Treasury rate 12-month return\n",
    "    'T10Y3MM',            # Yield spread (10Y - 3M)\n",
    "    'T10Y3MM_r12m',       # Yield spread 12-month return\n",
    "]\n",
    "\n",
    "ALL_VARS = LOAN_LEVEL_VARS + MACRO_VARS\n",
    "\n",
    "print(f\"Loan-level variables: {len(LOAN_LEVEL_VARS)}\")\n",
    "print(f\"Macro variables: {len(MACRO_VARS)}\")\n",
    "print(f\"Total: {len(ALL_VARS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-coverage",
   "metadata": {},
   "outputs": [],
   "source": "# Check variable coverage with detailed status\nprint(\"=\" * 70)\nprint(\"VARIABLE COVERAGE CHECK\")\nprint(\"=\" * 70)\n\nall_ok = True\ncoverage_threshold = 0.90  # Warn if below 90%\n\nprint(\"\\n=== Loan-Level Variables (9) ===\")\nfor var in LOAN_LEVEL_VARS:\n    if var in loans_df.columns:\n        coverage = loans_df[var].notna().mean()\n        status = \"✓\" if coverage >= coverage_threshold else \"⚠️\"\n        if coverage < coverage_threshold:\n            all_ok = False\n        print(f\"  {status} {var}: {coverage:.1%}\")\n    else:\n        print(f\"  ✗ {var}: MISSING\")\n        all_ok = False\n\nprint(\"\\n=== Macro Variables (12) ===\")\n# Mark critical derived variables\ncritical_vars = ['hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o', 'ppi_o_FRMA']\n\nfor var in MACRO_VARS:\n    critical_marker = \" [CRITICAL]\" if var in critical_vars else \"\"\n    if var in loans_df.columns:\n        coverage = loans_df[var].notna().mean()\n        status = \"✓\" if coverage >= coverage_threshold else \"⚠️\"\n        if coverage < coverage_threshold:\n            all_ok = False\n        \n        # Show sample values for critical variables\n        if var in critical_vars and coverage > 0:\n            sample_vals = loans_df[var].dropna().head(3).tolist()\n            print(f\"  {status} {var}: {coverage:.1%}{critical_marker}\")\n            print(f\"      Sample values: {[round(v, 3) for v in sample_vals]}\")\n        else:\n            print(f\"  {status} {var}: {coverage:.1%}{critical_marker}\")\n    else:\n        print(f\"  ✗ {var}: MISSING{critical_marker}\")\n        all_ok = False\n\n# Final status\nprint(\"\\n\" + \"=\" * 70)\nif all_ok:\n    print(\"✓ ALL VARIABLES OK (coverage >= 90%)\")\nelse:\n    print(\"⚠️  SOME VARIABLES HAVE ISSUES - Check warnings above\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "sample-header",
   "metadata": {},
   "source": [
    "## Step 5: Create Subsamples for Cross-Validation\n",
    "\n",
    "Following paper: 11 random subsamples of 10,000 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "filter-complete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete cases: 774,608 / 774,950 (100.0%)\n",
      "\n",
      "Event distribution:\n",
      "event_code\n",
      "0    326436\n",
      "1    422695\n",
      "2     25477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "0=Censored, 1=Prepay, 2=Default\n"
     ]
    }
   ],
   "source": [
    "# Filter to complete cases\n",
    "required_cols = ['duration', 'event_code'] + [v for v in ALL_VARS if v in loans_df.columns]\n",
    "loans_complete = loans_df.dropna(subset=[c for c in required_cols if c in loans_df.columns])\n",
    "\n",
    "print(f\"Complete cases: {len(loans_complete):,} / {len(loans_df):,} ({len(loans_complete)/len(loans_df):.1%})\")\n",
    "\n",
    "# Event distribution\n",
    "print(\"\\nEvent distribution:\")\n",
    "print(loans_complete['event_code'].value_counts().sort_index())\n",
    "print(\"\\n0=Censored, 1=Prepay, 2=Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "create-samples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled: 110,000\n",
      "\n",
      "Samples per fold:\n",
      "fold\n",
      "0     10000\n",
      "1     10000\n",
      "2     10000\n",
      "3     10000\n",
      "4     10000\n",
      "5     10000\n",
      "6     10000\n",
      "7     10000\n",
      "8     10000\n",
      "9     10000\n",
      "10    10000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create stratified subsamples\n",
    "# Need to ensure each sample has both event types\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Filter to just prepay (1) and default (2) for stratification\n",
    "# Include some censored as well\n",
    "loans_for_sampling = loans_complete.copy()\n",
    "\n",
    "# Create 11 subsamples\n",
    "n_samples = min(N_FOLDS * SAMPLE_SIZE_PER_FOLD, len(loans_for_sampling))\n",
    "if n_samples < len(loans_for_sampling):\n",
    "    # Sample from the data\n",
    "    sampled_df = loans_for_sampling.sample(n=n_samples, random_state=42)\n",
    "else:\n",
    "    sampled_df = loans_for_sampling\n",
    "\n",
    "# Assign fold numbers\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "sampled_df['fold'] = sampled_df.index % N_FOLDS\n",
    "\n",
    "print(f\"Total sampled: {len(sampled_df):,}\")\n",
    "print(f\"\\nSamples per fold:\")\n",
    "print(sampled_df['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fold-events",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Event Distribution per Fold ===\n",
      "Fold 0: n=10,000, prepay=5,470, default=304, censored=4,226\n",
      "Fold 1: n=10,000, prepay=5,334, default=328, censored=4,338\n",
      "Fold 2: n=10,000, prepay=5,483, default=326, censored=4,191\n",
      "Fold 3: n=10,000, prepay=5,430, default=334, censored=4,236\n",
      "Fold 4: n=10,000, prepay=5,397, default=337, censored=4,266\n",
      "Fold 5: n=10,000, prepay=5,513, default=300, censored=4,187\n",
      "Fold 6: n=10,000, prepay=5,384, default=343, censored=4,273\n",
      "Fold 7: n=10,000, prepay=5,487, default=315, censored=4,198\n",
      "Fold 8: n=10,000, prepay=5,450, default=313, censored=4,237\n",
      "Fold 9: n=10,000, prepay=5,480, default=318, censored=4,202\n",
      "Fold 10: n=10,000, prepay=5,408, default=331, censored=4,261\n"
     ]
    }
   ],
   "source": [
    "# Check event distribution per fold\n",
    "print(\"=== Event Distribution per Fold ===\")\n",
    "for fold in range(N_FOLDS):\n",
    "    fold_data = sampled_df[sampled_df['fold'] == fold]\n",
    "    prepay = (fold_data['event_code'] == 1).sum()\n",
    "    default = (fold_data['event_code'] == 2).sum()\n",
    "    censored = (fold_data['event_code'] == 0).sum()\n",
    "    print(f\"Fold {fold}: n={len(fold_data):,}, prepay={prepay:,}, default={default:,}, censored={censored:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Step 6: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "select-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: 110,000 rows, 28 columns\n"
     ]
    }
   ],
   "source": [
    "# Select final columns\n",
    "final_cols = [\n",
    "    # Identifiers\n",
    "    'loan_sequence_number', 'vintage_year', 'fold',\n",
    "    # Survival data\n",
    "    'duration', 'event_code',\n",
    "    # Loan-level variables\n",
    "] + [v for v in LOAN_LEVEL_VARS if v in sampled_df.columns] + [\n",
    "    # Macro variables  \n",
    "] + [v for v in MACRO_VARS if v in sampled_df.columns] + [\n",
    "    # Additional useful columns\n",
    "    'property_state', 'year_month'\n",
    "]\n",
    "\n",
    "# Remove duplicates and filter\n",
    "final_cols = list(dict.fromkeys(final_cols))\n",
    "final_cols = [c for c in final_cols if c in sampled_df.columns]\n",
    "\n",
    "final_df = sampled_df[final_cols].copy()\n",
    "print(f\"Final dataset: {len(final_df):,} rows, {len(final_cols)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "save-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/processed/blumenstock_dataset2.parquet\n",
      "Variable config saved.\n"
     ]
    }
   ],
   "source": [
    "# Save to parquet\n",
    "output_path = PROCESSED_DATA_DIR / 'blumenstock_dataset2.parquet'\n",
    "final_df.to_parquet(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n",
    "\n",
    "# Also save variable lists for reference\n",
    "var_config = {\n",
    "    'loan_level_vars': LOAN_LEVEL_VARS,\n",
    "    'macro_vars': MACRO_VARS,\n",
    "    'all_vars': ALL_VARS,\n",
    "}\n",
    "import json\n",
    "with open(PROCESSED_DATA_DIR / 'blumenstock_variables.json', 'w') as f:\n",
    "    json.dump(var_config, f, indent=2)\n",
    "print(\"Variable config saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "43ijjx4a471",
   "source": "# === FINAL VERIFICATION: Reload and verify saved dataset ===\nprint(\"=\" * 70)\nprint(\"FINAL VERIFICATION: Checking Saved Dataset\")\nprint(\"=\" * 70)\n\n# Reload the saved dataset\nverify_df = pd.read_parquet(output_path)\nprint(f\"\\nReloaded dataset: {len(verify_df):,} rows, {len(verify_df.columns)} columns\")\n\n# Check all required variables are present and have good coverage\nprint(\"\\n=== Critical Variable Verification ===\")\n\ncritical_macro_vars = ['hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o', 'ppi_o_FRMA']\nall_critical_ok = True\n\nfor var in critical_macro_vars:\n    if var in verify_df.columns:\n        coverage = verify_df[var].notna().mean()\n        mean_val = verify_df[var].mean()\n        std_val = verify_df[var].std()\n        \n        if coverage < 0.9:\n            print(f\"  ⚠️  {var}: {coverage:.1%} coverage (LOW!)\")\n            all_critical_ok = False\n        elif verify_df[var].isna().all():\n            print(f\"  ✗ {var}: ALL NaN VALUES!\")\n            all_critical_ok = False\n        else:\n            print(f\"  ✓ {var}: {coverage:.1%} coverage, mean={mean_val:.3f}, std={std_val:.3f}\")\n    else:\n        print(f\"  ✗ {var}: MISSING FROM SAVED DATASET!\")\n        all_critical_ok = False\n\n# Summary statistics for all variables\nprint(\"\\n=== Quick Statistics ===\")\nprint(f\"Total variables: {len(verify_df.columns)}\")\nprint(f\"Loan-level vars present: {len([v for v in LOAN_LEVEL_VARS if v in verify_df.columns])}/{len(LOAN_LEVEL_VARS)}\")\nprint(f\"Macro vars present: {len([v for v in MACRO_VARS if v in verify_df.columns])}/{len(MACRO_VARS)}\")\n\n# Final status\nprint(\"\\n\" + \"=\" * 70)\nif all_critical_ok:\n    print(\"✓ VERIFICATION PASSED: All critical variables present with good coverage\")\nelse:\n    print(\"✗ VERIFICATION FAILED: Some critical variables missing or have low coverage\")\n    print(\"  Check the warnings above and re-run data preparation if needed.\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLUMENSTOCK DATASET 2 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Period: 2010-2025\n",
      "Total observations: 110,000\n",
      "Number of folds: 11\n",
      "\n",
      "=== Event Distribution ===\n",
      "  Censored (k=0): 46,615 (42.4%)\n",
      "  Prepayment (k=1): 59,836 (54.4%)\n",
      "  Default (k=2): 3,549 (3.2%)\n",
      "\n",
      "=== Variable Summary ===\n",
      "Loan-level variables: 9\n",
      "Macro variables: 12\n",
      "\n",
      "=== Duration Statistics ===\n",
      "count    110000.000000\n",
      "mean         49.535464\n",
      "std          39.820797\n",
      "min           0.000000\n",
      "25%          19.000000\n",
      "50%          39.000000\n",
      "75%          68.000000\n",
      "max         184.000000\n",
      "Name: duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BLUMENSTOCK DATASET 2 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPeriod: {final_df['vintage_year'].min()}-{final_df['vintage_year'].max()}\")\n",
    "print(f\"Total observations: {len(final_df):,}\")\n",
    "print(f\"Number of folds: {final_df['fold'].nunique()}\")\n",
    "\n",
    "print(f\"\\n=== Event Distribution ===\")\n",
    "event_counts = final_df['event_code'].value_counts().sort_index()\n",
    "for code, count in event_counts.items():\n",
    "    event_name = {0: 'Censored', 1: 'Prepayment', 2: 'Default'}.get(code, 'Other')\n",
    "    pct = count / len(final_df) * 100\n",
    "    print(f\"  {event_name} (k={code}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== Variable Summary ===\")\n",
    "print(f\"Loan-level variables: {len([v for v in LOAN_LEVEL_VARS if v in final_df.columns])}\")\n",
    "print(f\"Macro variables: {len([v for v in MACRO_VARS if v in final_df.columns])}\")\n",
    "\n",
    "print(f\"\\n=== Duration Statistics ===\")\n",
    "print(final_df['duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Data is ready for experiments:\n",
    "\n",
    "1. **Notebook 05**: Cause-Specific Cox (CSC)\n",
    "2. **Notebook 06**: Fine-Gray Model (FGR)\n",
    "3. **Notebook 07**: Random Survival Forest (RSF)\n",
    "4. **Notebook 08**: Model Comparison\n",
    "\n",
    "### Experiments (from paper)\n",
    "\n",
    "| Experiment | Variables | File |\n",
    "|------------|-----------|------|\n",
    "| Exp 4.1 | Loan-level only | Use `LOAN_LEVEL_VARS` |\n",
    "| Exp 4.2 | Macro only | Use `MACRO_VARS` |\n",
    "| Exp 4.3 | All variables | Use `ALL_VARS` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}