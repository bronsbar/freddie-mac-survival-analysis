{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Create Loan-Month Panel Data for Cox Regression\n",
    "\n",
    "This notebook creates **loan-month panel data** for cause-specific Cox regression with time-varying covariates.\n",
    "\n",
    "## Methodology (Blumenstock et al. 2022)\n",
    "\n",
    "### Sampling Strategy\n",
    "- **11 folds** (10 for cross-validation, 1 for hyperparameter tuning)\n",
    "- **10,000 loans per fold** (100 defaults + 9,900 non-defaults)\n",
    "- **Sampling without replacement** to ensure independent folds\n",
    "\n",
    "### Data Structure\n",
    "- Each loan contributes multiple rows (one per month observed)\n",
    "- Time-varying covariates updated each month\n",
    "- Interval format `(start, stop)` for Cox regression\n",
    "\n",
    "## Phases\n",
    "1. **Phase 1**: Sample loans (this section)\n",
    "2. **Phase 2**: Create loan-month panel for sampled loans\n",
    "3. **Phase 3**: Merge time-varying covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Data paths\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "EXTERNAL_DATA_DIR = Path('../data/external')\n",
    "\n",
    "# Sampling configuration (Blumenstock et al.)\n",
    "N_FOLDS = 11\n",
    "DEFAULTS_PER_FOLD = 100\n",
    "NON_DEFAULTS_PER_FOLD = 9_900\n",
    "LOANS_PER_FOLD = DEFAULTS_PER_FOLD + NON_DEFAULTS_PER_FOLD  # 10,000\n",
    "\n",
    "# Total needed\n",
    "TOTAL_DEFAULTS_NEEDED = N_FOLDS * DEFAULTS_PER_FOLD      # 1,100\n",
    "TOTAL_NON_DEFAULTS_NEEDED = N_FOLDS * NON_DEFAULTS_PER_FOLD  # 108,900\n",
    "TOTAL_LOANS = N_FOLDS * LOANS_PER_FOLD  # 110,000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"=== Sampling Configuration ===\")\n",
    "print(f\"Number of folds: {N_FOLDS}\")\n",
    "print(f\"Loans per fold: {LOANS_PER_FOLD:,}\")\n",
    "print(f\"  - Defaults per fold: {DEFAULTS_PER_FOLD}\")\n",
    "print(f\"  - Non-defaults per fold: {NON_DEFAULTS_PER_FOLD:,}\")\n",
    "print(f\"\\nTotal loans to sample: {TOTAL_LOANS:,}\")\n",
    "print(f\"  - Total defaults needed: {TOTAL_DEFAULTS_NEEDED:,}\")\n",
    "print(f\"  - Total non-defaults needed: {TOTAL_NON_DEFAULTS_NEEDED:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Sample Loans\n",
    "\n",
    "Load terminal records and sample loans using stratified sampling:\n",
    "- 100 defaults per fold (without replacement)\n",
    "- 9,900 non-defaults per fold (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load terminal records to identify loan outcomes\n",
    "print(\"Loading terminal records...\")\n",
    "terminal_df = pd.read_parquet(PROCESSED_DATA_DIR / 'survival_data_blumenstock.parquet')\n",
    "\n",
    "print(f\"\\nLoaded {len(terminal_df):,} loans\")\n",
    "print(f\"Vintages: {terminal_df['vintage_year'].min()} - {terminal_df['vintage_year'].max()}\")\n",
    "\n",
    "print(\"\\nEvent distribution:\")\n",
    "event_names = {0: 'Censored', 1: 'Prepay', 2: 'Default'}\n",
    "for code, count in terminal_df['event_code'].value_counts().sort_index().items():\n",
    "    pct = count / len(terminal_df) * 100\n",
    "    print(f\"  {event_names.get(code, 'Other')} (k={code}): {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-events",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate loans by terminal event type\n",
    "defaulted_loans = terminal_df[terminal_df['event_code'] == 2]['loan_sequence_number'].unique()\n",
    "non_defaulted_loans = terminal_df[terminal_df['event_code'] != 2]['loan_sequence_number'].unique()\n",
    "\n",
    "print(\"=== Loan Pools ===\")\n",
    "print(f\"Defaulted loans available: {len(defaulted_loans):,}\")\n",
    "print(f\"Non-defaulted loans available: {len(non_defaulted_loans):,}\")\n",
    "\n",
    "# Verify we have enough loans\n",
    "print(\"\\n=== Verification ===\")\n",
    "if len(defaulted_loans) >= TOTAL_DEFAULTS_NEEDED:\n",
    "    print(f\"✓ Sufficient defaults: {len(defaulted_loans):,} >= {TOTAL_DEFAULTS_NEEDED:,} needed\")\n",
    "else:\n",
    "    print(f\"✗ INSUFFICIENT defaults: {len(defaulted_loans):,} < {TOTAL_DEFAULTS_NEEDED:,} needed\")\n",
    "    raise ValueError(\"Not enough defaulted loans for sampling\")\n",
    "\n",
    "if len(non_defaulted_loans) >= TOTAL_NON_DEFAULTS_NEEDED:\n",
    "    print(f\"✓ Sufficient non-defaults: {len(non_defaulted_loans):,} >= {TOTAL_NON_DEFAULTS_NEEDED:,} needed\")\n",
    "else:\n",
    "    print(f\"✗ INSUFFICIENT non-defaults: {len(non_defaulted_loans):,} < {TOTAL_NON_DEFAULTS_NEEDED:,} needed\")\n",
    "    raise ValueError(\"Not enough non-defaulted loans for sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-loans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle loans (without replacement sampling)\n",
    "print(\"Shuffling loan pools...\")\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "defaulted_shuffled = np.random.permutation(defaulted_loans)\n",
    "non_defaulted_shuffled = np.random.permutation(non_defaulted_loans)\n",
    "\n",
    "print(f\"  Defaulted pool shuffled: {len(defaulted_shuffled):,} loans\")\n",
    "print(f\"  Non-defaulted pool shuffled: {len(non_defaulted_shuffled):,} loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-folds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fold assignments by taking sequential chunks (without replacement)\n",
    "print(\"\\n=== Creating Fold Assignments ===\")\n",
    "\n",
    "fold_assignments = {}  # loan_id -> fold number\n",
    "fold_loan_lists = []   # List of loan IDs per fold (for verification)\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    # Defaults for this fold (sequential chunk)\n",
    "    d_start = fold * DEFAULTS_PER_FOLD\n",
    "    d_end = d_start + DEFAULTS_PER_FOLD\n",
    "    fold_defaults = defaulted_shuffled[d_start:d_end]\n",
    "    \n",
    "    # Non-defaults for this fold (sequential chunk)\n",
    "    nd_start = fold * NON_DEFAULTS_PER_FOLD\n",
    "    nd_end = nd_start + NON_DEFAULTS_PER_FOLD\n",
    "    fold_non_defaults = non_defaulted_shuffled[nd_start:nd_end]\n",
    "    \n",
    "    # Combine and assign fold number\n",
    "    fold_loans = np.concatenate([fold_defaults, fold_non_defaults])\n",
    "    fold_loan_lists.append(fold_loans)\n",
    "    \n",
    "    for loan_id in fold_loans:\n",
    "        fold_assignments[loan_id] = fold\n",
    "    \n",
    "    print(f\"Fold {fold:2d}: {len(fold_defaults):,} defaults + {len(fold_non_defaults):,} non-defaults = {len(fold_loans):,} loans\")\n",
    "\n",
    "# Collect all sampled loan IDs\n",
    "sampled_loan_ids = set(fold_assignments.keys())\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total loans sampled: {len(sampled_loan_ids):,}\")\n",
    "print(f\"Expected: {TOTAL_LOANS:,}\")\n",
    "assert len(sampled_loan_ids) == TOTAL_LOANS, \"Mismatch in total sampled loans!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-no-overlap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no overlap between folds (sampling without replacement)\n",
    "print(\"=== Verifying No Overlap Between Folds ===\")\n",
    "\n",
    "all_ok = True\n",
    "for i in range(N_FOLDS):\n",
    "    for j in range(i + 1, N_FOLDS):\n",
    "        set_i = set(fold_loan_lists[i])\n",
    "        set_j = set(fold_loan_lists[j])\n",
    "        overlap = set_i & set_j\n",
    "        if len(overlap) > 0:\n",
    "            print(f\"✗ Overlap between fold {i} and fold {j}: {len(overlap)} loans\")\n",
    "            all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"✓ No overlap between any folds - sampling without replacement verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-event-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify event distribution per fold\n",
    "print(\"=== Event Distribution per Fold ===\")\n",
    "print(f\"Target: {DEFAULTS_PER_FOLD} defaults (1%) + {NON_DEFAULTS_PER_FOLD:,} non-defaults per fold\\n\")\n",
    "\n",
    "# Create a DataFrame of sampled loans with their fold assignments\n",
    "sampled_df = terminal_df[terminal_df['loan_sequence_number'].isin(sampled_loan_ids)].copy()\n",
    "sampled_df['fold'] = sampled_df['loan_sequence_number'].map(fold_assignments)\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    fold_data = sampled_df[sampled_df['fold'] == fold]\n",
    "    n_defaults = (fold_data['event_code'] == 2).sum()\n",
    "    n_prepay = (fold_data['event_code'] == 1).sum()\n",
    "    n_censored = (fold_data['event_code'] == 0).sum()\n",
    "    default_pct = n_defaults / len(fold_data) * 100\n",
    "    \n",
    "    status = \"✓\" if n_defaults == DEFAULTS_PER_FOLD else \"✗\"\n",
    "    print(f\"{status} Fold {fold:2d}: n={len(fold_data):,}, defaults={n_defaults} ({default_pct:.1f}%), prepay={n_prepay:,}, censored={n_censored:,}\")\n",
    "\n",
    "# Overall summary\n",
    "total_defaults = (sampled_df['event_code'] == 2).sum()\n",
    "total_prepay = (sampled_df['event_code'] == 1).sum()\n",
    "total_censored = (sampled_df['event_code'] == 0).sum()\n",
    "\n",
    "print(f\"\\n=== Overall ===\")\n",
    "print(f\"Total defaults: {total_defaults:,} ({total_defaults/len(sampled_df)*100:.1f}%)\")\n",
    "print(f\"Total prepay: {total_prepay:,} ({total_prepay/len(sampled_df)*100:.1f}%)\")\n",
    "print(f\"Total censored: {total_censored:,} ({total_censored/len(sampled_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-vintage-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vintage distribution in sampled data\n",
    "print(\"=== Vintage Distribution in Sampled Data ===\")\n",
    "\n",
    "vintage_dist = sampled_df.groupby('vintage_year').agg({\n",
    "    'loan_sequence_number': 'count',\n",
    "    'event_code': lambda x: (x == 2).sum()  # Count defaults\n",
    "}).rename(columns={'loan_sequence_number': 'n_loans', 'event_code': 'n_defaults'})\n",
    "\n",
    "vintage_dist['default_rate'] = (vintage_dist['n_defaults'] / vintage_dist['n_loans'] * 100).round(2)\n",
    "\n",
    "print(vintage_dist.to_string())\n",
    "print(f\"\\nTotal: {vintage_dist['n_loans'].sum():,} loans, {vintage_dist['n_defaults'].sum():,} defaults\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled loans with fold assignments\n",
    "print(\"=== Saving Sampled Loan Data ===\")\n",
    "\n",
    "# Select key columns for the sampled loans reference file\n",
    "sample_cols = [\n",
    "    'loan_sequence_number', 'fold', 'vintage_year', 'property_state',\n",
    "    'event_code', 'duration',\n",
    "    # Static covariates\n",
    "    'int_rate', 'orig_upb', 'fico_score', 'dti_r', 'ltv_r',\n",
    "    # Origination info for macro merging\n",
    "    'first_payment_date', 'orig_year_month'\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "sample_cols = [c for c in sample_cols if c in sampled_df.columns]\n",
    "sampled_loans_df = sampled_df[sample_cols].copy()\n",
    "\n",
    "# Save to parquet\n",
    "output_path = PROCESSED_DATA_DIR / 'sampled_loans_blumenstock.parquet'\n",
    "sampled_loans_df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved sampled loans to: {output_path}\")\n",
    "print(f\"  Shape: {sampled_loans_df.shape}\")\n",
    "print(f\"  Columns: {list(sampled_loans_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-fold-assignments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fold assignments as a separate lookup file (for use in panel creation)\n",
    "import json\n",
    "\n",
    "# Convert to serializable format\n",
    "fold_assignments_serializable = {str(k): int(v) for k, v in fold_assignments.items()}\n",
    "\n",
    "# Save as JSON\n",
    "fold_path = PROCESSED_DATA_DIR / 'fold_assignments.json'\n",
    "with open(fold_path, 'w') as f:\n",
    "    json.dump(fold_assignments_serializable, f)\n",
    "\n",
    "print(f\"✓ Saved fold assignments to: {fold_path}\")\n",
    "print(f\"  Total entries: {len(fold_assignments_serializable):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1 Summary\n",
    "\n",
    "### Completed\n",
    "- ✅ Loaded terminal records (774,950 loans)\n",
    "- ✅ Separated defaulted (25,527) vs non-defaulted (749,423) loans\n",
    "- ✅ Sampled 100 defaults + 9,900 non-defaults per fold (without replacement)\n",
    "- ✅ Created 11 folds with 10,000 loans each (110,000 total)\n",
    "- ✅ Verified no overlap between folds\n",
    "- ✅ Saved sampled loans to `sampled_loans_blumenstock.parquet`\n",
    "- ✅ Saved fold assignments to `fold_assignments.json`\n",
    "\n",
    "### Output Files\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `sampled_loans_blumenstock.parquet` | 110,000 sampled loans with fold assignments and static covariates |\n",
    "| `fold_assignments.json` | Loan ID → Fold mapping for panel creation |\n",
    "\n",
    "### Next: Phase 2\n",
    "Create loan-month panel data for the sampled loans by expanding performance records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1-complete",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 1 COMPLETE: Loan Sampling\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSampled {len(sampled_loan_ids):,} loans across {N_FOLDS} folds\")\n",
    "print(f\"  - {DEFAULTS_PER_FOLD} defaults per fold ({TOTAL_DEFAULTS_NEEDED:,} total)\")\n",
    "print(f\"  - {NON_DEFAULTS_PER_FOLD:,} non-defaults per fold ({TOTAL_NON_DEFAULTS_NEEDED:,} total)\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'sampled_loans_blumenstock.parquet'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'fold_assignments.json'}\")\n",
    "print(f\"\\nReady for Phase 2: Create loan-month panel data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zgj5j431m3",
   "source": "---\n\n# Phase 2: Create Loan-Month Panel Data\n\nExpand the sampled loans into loan-month panel format:\n1. Load raw performance data for each vintage\n2. Filter to sampled loans (early filtering for memory efficiency)\n3. Calculate behavioral variables (rolling 12-month counts)\n4. Determine events per loan-month\n5. Create interval format (start, stop) for Cox regression",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "voth6goq42d",
   "source": "# Load sampled loan IDs and fold assignments from Phase 1\nimport json\n\nprint(\"=== Loading Phase 1 Outputs ===\")\n\n# Load sampled loans reference\nsampled_loans_df = pd.read_parquet(PROCESSED_DATA_DIR / 'sampled_loans_blumenstock.parquet')\nprint(f\"Loaded {len(sampled_loans_df):,} sampled loans\")\n\n# Load fold assignments\nwith open(PROCESSED_DATA_DIR / 'fold_assignments.json', 'r') as f:\n    fold_assignments = json.load(f)\nprint(f\"Loaded {len(fold_assignments):,} fold assignments\")\n\n# Create set of sampled loan IDs for fast lookup\nsampled_loan_ids = set(fold_assignments.keys())\nprint(f\"Sampled loan IDs: {len(sampled_loan_ids):,}\")\n\n# Vintages to process\nVINTAGES = list(range(2010, 2026))\nprint(f\"\\nVintages to process: {VINTAGES[0]} - {VINTAGES[-1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ntlp2yob3r",
   "source": "# Import column definitions\nimport sys\nsys.path.insert(0, '..')\nfrom src.data.columns import (\n    ORIGINATION_COLUMNS, ORIGINATION_DTYPES,\n    PERFORMANCE_COLUMNS, PERFORMANCE_DTYPES,\n)\n\n# Default definition: 3-month delinquent (90+ days)\nDEFAULT_DELINQUENCY_THRESHOLD = 3\n\ndef load_performance_data(vintage: int) -> pd.DataFrame:\n    \"\"\"Load performance (loan-month) data for a vintage.\"\"\"\n    pattern = f'sample_{vintage}/sample_svcg_{vintage}.txt'\n    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n    \n    if not files:\n        return pd.DataFrame()\n    \n    df = pd.read_csv(\n        files[0], sep='|', names=PERFORMANCE_COLUMNS,\n        dtype=PERFORMANCE_DTYPES, na_values=['', ' ']\n    )\n    return df\n\n\ndef load_origination_data(vintage: int) -> pd.DataFrame:\n    \"\"\"Load origination data for a vintage.\"\"\"\n    pattern = f'sample_{vintage}/sample_orig_{vintage}.txt'\n    files = list(RAW_DATA_DIR.glob(f'**/{pattern}'))\n    \n    if not files:\n        return pd.DataFrame()\n    \n    df = pd.read_csv(\n        files[0], sep='|', names=ORIGINATION_COLUMNS,\n        dtype=ORIGINATION_DTYPES, na_values=['', ' ']\n    )\n    return df\n\n\nprint(\"Data loading functions defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bmqr95syynl",
   "source": "def process_vintage_panel(vintage: int, sampled_loan_ids: set, fold_assignments: dict) -> pd.DataFrame:\n    \"\"\"\n    Process a vintage into loan-month panel format for sampled loans only.\n    \n    Returns DataFrame with one row per loan-month, including:\n    - Interval format (start, stop) for Cox regression\n    - Behavioral variables (rolling 12-month counts)\n    - Event indicators\n    \"\"\"\n    print(f\"\\nProcessing vintage {vintage}...\")\n    \n    # Load performance data\n    perf_df = load_performance_data(vintage)\n    if perf_df.empty:\n        print(f\"  No performance data found for vintage {vintage}\")\n        return pd.DataFrame()\n    \n    initial_rows = len(perf_df)\n    initial_loans = perf_df['loan_sequence_number'].nunique()\n    \n    # EARLY FILTER: Keep only sampled loans (critical for memory efficiency)\n    perf_df = perf_df[perf_df['loan_sequence_number'].isin(sampled_loan_ids)]\n    \n    if len(perf_df) == 0:\n        print(f\"  No sampled loans found in vintage {vintage}\")\n        return pd.DataFrame()\n    \n    filtered_loans = perf_df['loan_sequence_number'].nunique()\n    print(f\"  Filtered: {initial_loans:,} → {filtered_loans:,} loans ({len(perf_df):,} loan-months)\")\n    \n    # Add fold assignment\n    perf_df['fold'] = perf_df['loan_sequence_number'].map(fold_assignments)\n    \n    # Parse reporting period\n    perf_df['reporting_date'] = pd.to_datetime(\n        perf_df['monthly_reporting_period'].astype(str), format='%Y%m'\n    )\n    perf_df['year_month'] = perf_df['reporting_date'].dt.to_period('M')\n    \n    # Parse delinquency status\n    perf_df['delinquency_status'] = pd.to_numeric(\n        perf_df['current_loan_delinquency_status'].replace({'X': '0', 'XX': '0'}),\n        errors='coerce'\n    ).fillna(0).astype(int)\n    \n    # Sort by loan and time (required for rolling calculations)\n    perf_df = perf_df.sort_values(['loan_sequence_number', 'loan_age'])\n    \n    # === Calculate behavioral variables ===\n    # Binary indicators for each month\n    perf_df['is_current'] = (perf_df['delinquency_status'] == 0).astype(int)\n    perf_df['is_30d_del'] = (perf_df['delinquency_status'] == 1).astype(int)\n    perf_df['is_60d_del'] = (perf_df['delinquency_status'] == 2).astype(int)\n    \n    # Rolling 12-month counts (per loan)\n    grouped = perf_df.groupby('loan_sequence_number')\n    perf_df['t_act_12m'] = grouped['is_current'].transform(\n        lambda x: x.rolling(12, min_periods=1).sum()\n    )\n    perf_df['t_del_30d_12m'] = grouped['is_30d_del'].transform(\n        lambda x: x.rolling(12, min_periods=1).sum()\n    )\n    perf_df['t_del_60d_12m'] = grouped['is_60d_del'].transform(\n        lambda x: x.rolling(12, min_periods=1).sum()\n    )\n    \n    # === Determine events ===\n    # Default: first time reaching 90+ days delinquent\n    perf_df['is_default'] = (perf_df['delinquency_status'] >= DEFAULT_DELINQUENCY_THRESHOLD).astype(int)\n    perf_df['cumsum_default'] = grouped['is_default'].transform('cumsum')\n    perf_df['first_default'] = ((perf_df['cumsum_default'] == 1) & (perf_df['is_default'] == 1)).astype(int)\n    \n    # Prepayment: zero balance code = 01\n    perf_df['is_prepay'] = (perf_df['zero_balance_code'] == '01').astype(int)\n    \n    # === Create interval format (start, stop) ===\n    # Filter out loan_age <= 0 (origination month with no elapsed time)\n    # These rows cannot contribute to the risk set and would create invalid start < 0\n    n_before_filter = len(perf_df)\n    perf_df = perf_df[perf_df['loan_age'] > 0].copy()\n    n_dropped = n_before_filter - len(perf_df)\n    if n_dropped > 0:\n        print(f\"  Dropped {n_dropped:,} rows with loan_age <= 0\")\n    \n    perf_df['start'] = perf_df['loan_age'] - 1\n    perf_df['stop'] = perf_df['loan_age']\n    \n    # Event indicator (1 if event occurs at this stop time)\n    perf_df['event'] = 0\n    perf_df.loc[perf_df['first_default'] == 1, 'event'] = 1\n    perf_df.loc[perf_df['is_prepay'] == 1, 'event'] = 1\n    \n    # Event code (for cause-specific models)\n    perf_df['event_code'] = 0  # censored\n    perf_df.loc[perf_df['is_prepay'] == 1, 'event_code'] = 1  # prepay\n    perf_df.loc[perf_df['first_default'] == 1, 'event_code'] = 2  # default\n    \n    # === Remove observations AFTER the event ===\n    # Loan should not contribute to risk set after event\n    # Need to recompute grouped after filtering\n    grouped = perf_df.groupby('loan_sequence_number')\n    perf_df['cumsum_event'] = grouped['event'].transform('cumsum')\n    perf_df = perf_df[perf_df['cumsum_event'] <= 1]  # Keep up to and including first event\n    \n    # Add vintage year\n    perf_df['vintage_year'] = vintage\n    \n    print(f\"  Final: {perf_df['loan_sequence_number'].nunique():,} loans, {len(perf_df):,} loan-months\")\n    \n    return perf_df\n\n\nprint(\"Processing function defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qkpa7pkhhg",
   "source": "# Process all vintages and combine into panel\nprint(\"=\" * 60)\nprint(\"PROCESSING ALL VINTAGES\")\nprint(\"=\" * 60)\n\npanel_dfs = []\nvintage_stats = []\n\nfor vintage in VINTAGES:\n    panel_vintage = process_vintage_panel(vintage, sampled_loan_ids, fold_assignments)\n    \n    if not panel_vintage.empty:\n        n_loans = panel_vintage['loan_sequence_number'].nunique()\n        n_months = len(panel_vintage)\n        n_events = panel_vintage['event'].sum()\n        \n        vintage_stats.append({\n            'vintage': vintage,\n            'loans': n_loans,\n            'loan_months': n_months,\n            'events': n_events,\n            'avg_duration': n_months / n_loans if n_loans > 0 else 0\n        })\n        \n        panel_dfs.append(panel_vintage)\n\n# Combine all vintages\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMBINING VINTAGES\")\nprint(\"=\" * 60)\n\npanel_df = pd.concat(panel_dfs, ignore_index=True)\n\nprint(f\"\\nTotal loan-months: {len(panel_df):,}\")\nprint(f\"Total unique loans: {panel_df['loan_sequence_number'].nunique():,}\")\nprint(f\"Total events: {panel_df['event'].sum():,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z8afeo0tmhm",
   "source": "# Display vintage statistics\nprint(\"=== Vintage Statistics ===\")\nstats_df = pd.DataFrame(vintage_stats)\nstats_df['avg_duration'] = stats_df['avg_duration'].round(1)\nprint(stats_df.to_string(index=False))\n\nprint(f\"\\nTotal: {stats_df['loans'].sum():,} loans, {stats_df['loan_months'].sum():,} loan-months\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gidq6ukirhi",
   "source": "## Merge Origination Data (Static Covariates)\n\nAdd static loan characteristics from origination files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7ppm1211yg2",
   "source": "# Load and merge origination data for static covariates\nprint(\"=== Merging Origination Data ===\")\n\n# Load origination data for all vintages\norig_dfs = []\nfor vintage in VINTAGES:\n    orig_df = load_origination_data(vintage)\n    if not orig_df.empty:\n        # Filter to sampled loans\n        orig_df = orig_df[orig_df['loan_sequence_number'].isin(sampled_loan_ids)]\n        if not orig_df.empty:\n            orig_df['vintage_year'] = vintage\n            orig_dfs.append(orig_df)\n\norig_all = pd.concat(orig_dfs, ignore_index=True)\nprint(f\"Loaded origination data for {len(orig_all):,} loans\")\n\n# Select columns to merge\norig_cols = [\n    'loan_sequence_number',\n    'credit_score',      # fico_score\n    'orig_ltv',          # ltv_r\n    'orig_dti',          # dti_r\n    'orig_upb',          # orig_upb\n    'orig_interest_rate', # int_rate\n    'first_payment_date',\n    'property_state',\n]\n\n# Filter to available columns\norig_cols = [c for c in orig_cols if c in orig_all.columns]\norig_subset = orig_all[orig_cols].drop_duplicates(subset=['loan_sequence_number'])\n\nprint(f\"Origination columns to merge: {orig_cols}\")\n\n# Merge with panel\nn_before = len(panel_df)\npanel_df = panel_df.merge(orig_subset, on='loan_sequence_number', how='left')\nn_after = len(panel_df)\n\nprint(f\"Merged: {n_before:,} → {n_after:,} rows\")\n\n# Rename columns to match Blumenstock variable names\nrename_map = {\n    'credit_score': 'fico_score',\n    'orig_ltv': 'ltv_r',\n    'orig_dti': 'dti_r',\n    'orig_interest_rate': 'int_rate',\n}\npanel_df = panel_df.rename(columns=rename_map)\n\n# Parse origination date for macro merging\npanel_df['first_payment_date'] = pd.to_datetime(\n    panel_df['first_payment_date'].astype(str), format='%Y%m', errors='coerce'\n)\npanel_df['orig_year_month'] = panel_df['first_payment_date'].dt.to_period('M')\n\n# Calculate bal_repaid (percent of balance repaid)\npanel_df['bal_repaid'] = (\n    (panel_df['orig_upb'] - panel_df['current_actual_upb'].fillna(0)) / \n    panel_df['orig_upb']\n) * 100\npanel_df['bal_repaid'] = panel_df['bal_repaid'].clip(0, 100)\n\nprint(f\"\\n✓ Static covariates merged\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vnhphu15rs",
   "source": "## Merge Macroeconomic Data (Time-Varying)\n\nAdd time-varying macro variables:\n- National: Treasury rates, mortgage rates, yield spreads\n- State-level: HPI, unemployment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9zg619i24hr",
   "source": "# Load macroeconomic data\nprint(\"=== Loading Macro Data ===\")\n\n# National macro data\nmacro_path = EXTERNAL_DATA_DIR / 'fred_monthly_panel.parquet'\nif macro_path.exists():\n    macro_national = pd.read_parquet(macro_path)\n    macro_national.index.name = 'date'\n    macro_national = macro_national.reset_index()\n    macro_national['date'] = pd.to_datetime(macro_national['date'])\n    macro_national['year_month'] = macro_national['date'].dt.to_period('M')\n    \n    # Calculate derived variables\n    if 'DGS10' in macro_national.columns:\n        macro_national['TB10Y_r12m'] = macro_national['DGS10'].pct_change(12)\n    if 'DGS10' in macro_national.columns and 'DGS3MO' in macro_national.columns:\n        macro_national['T10Y3MM'] = macro_national['DGS10'] - macro_national['DGS3MO']\n        macro_national['T10Y3MM_r12m'] = macro_national['T10Y3MM'].pct_change(12)\n    \n    print(f\"✓ National macro: {len(macro_national)} months\")\nelse:\n    print(f\"✗ National macro not found: {macro_path}\")\n    macro_national = None\n\n# State unemployment\nunemp_path = EXTERNAL_DATA_DIR / 'state_unemployment.parquet'\nif unemp_path.exists():\n    state_unemp = pd.read_parquet(unemp_path)\n    state_unemp.index.name = 'date'\n    state_unemp = state_unemp.reset_index()\n    state_unemp['date'] = pd.to_datetime(state_unemp['date'])\n    state_unemp['year_month'] = state_unemp['date'].dt.to_period('M')\n    \n    # Melt to long format\n    state_cols = [c for c in state_unemp.columns if '_unemployment' in c]\n    state_unemp_long = state_unemp.melt(\n        id_vars=['date', 'year_month'],\n        value_vars=state_cols,\n        var_name='state_col',\n        value_name='state_unemployment'\n    )\n    state_unemp_long['property_state'] = state_unemp_long['state_col'].str.replace('_unemployment', '')\n    \n    # Calculate returns\n    state_unemp_long = state_unemp_long.sort_values(['property_state', 'year_month'])\n    state_unemp_long['st_unemp_r12m'] = state_unemp_long.groupby('property_state')['state_unemployment'].transform(\n        lambda x: np.log(x / x.shift(12))\n    )\n    state_unemp_long['st_unemp_r3m'] = state_unemp_long.groupby('property_state')['state_unemployment'].transform(\n        lambda x: np.log(x / x.shift(3))\n    )\n    \n    state_unemp_long = state_unemp_long[['year_month', 'property_state', 'state_unemployment', \n                                          'st_unemp_r12m', 'st_unemp_r3m']].drop_duplicates()\n    print(f\"✓ State unemployment: {state_unemp_long['property_state'].nunique()} states\")\nelse:\n    print(f\"✗ State unemployment not found: {unemp_path}\")\n    state_unemp_long = pd.DataFrame()\n\n# State HPI\nhpi_path = EXTERNAL_DATA_DIR / 'state_hpi.parquet'\nif hpi_path.exists():\n    state_hpi = pd.read_parquet(hpi_path)\n    state_hpi.index.name = 'date'\n    state_hpi = state_hpi.reset_index()\n    state_hpi['date'] = pd.to_datetime(state_hpi['date'])\n    state_hpi['year_month'] = state_hpi['date'].dt.to_period('M')\n    \n    # Melt to long format\n    hpi_cols = [c for c in state_hpi.columns if c.endswith('_hpi') and len(c) <= 6]\n    state_hpi['national_hpi'] = state_hpi[hpi_cols].mean(axis=1)\n    \n    state_hpi_long = state_hpi.melt(\n        id_vars=['date', 'year_month', 'national_hpi'],\n        value_vars=hpi_cols,\n        var_name='state_col',\n        value_name='state_hpi'\n    )\n    state_hpi_long['property_state'] = state_hpi_long['state_col'].str.replace('_hpi', '')\n    \n    # Calculate returns\n    state_hpi_long = state_hpi_long.sort_values(['property_state', 'year_month'])\n    state_hpi_long['hpi_st_log12m'] = state_hpi_long.groupby('property_state')['state_hpi'].transform(\n        lambda x: np.log(x / x.shift(12))\n    )\n    state_hpi_long['hpi_r_st_us'] = state_hpi_long['state_hpi'] / state_hpi_long['national_hpi']\n    \n    state_hpi_long = state_hpi_long[['year_month', 'property_state', 'state_hpi', 'national_hpi',\n                                      'hpi_st_log12m', 'hpi_r_st_us']].drop_duplicates()\n    print(f\"✓ State HPI: {state_hpi_long['property_state'].nunique()} states\")\nelse:\n    print(f\"✗ State HPI not found: {hpi_path}\")\n    state_hpi_long = pd.DataFrame()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7t64kssoix",
   "source": "# Merge macro data with panel (by observation year_month)\nprint(\"=== Merging Macro Data at Observation Time ===\")\nn_before = len(panel_df)\n\n# Merge national macro\nif macro_national is not None:\n    macro_cols = ['year_month', 'MORTGAGE30US', 'DGS10', 'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m']\n    macro_cols = [c for c in macro_cols if c in macro_national.columns]\n    panel_df = panel_df.merge(macro_national[macro_cols], on='year_month', how='left')\n    print(f\"  ✓ National macro merged: MORTGAGE30US coverage = {panel_df['MORTGAGE30US'].notna().mean():.1%}\")\n\n# Merge state unemployment\nif not state_unemp_long.empty:\n    panel_df = panel_df.merge(\n        state_unemp_long[['year_month', 'property_state', 'st_unemp_r12m', 'st_unemp_r3m']],\n        on=['year_month', 'property_state'],\n        how='left'\n    )\n    print(f\"  ✓ State unemployment merged: coverage = {panel_df['st_unemp_r12m'].notna().mean():.1%}\")\n\n# Merge state HPI\nif not state_hpi_long.empty:\n    panel_df = panel_df.merge(\n        state_hpi_long[['year_month', 'property_state', 'state_hpi', 'national_hpi', 'hpi_st_log12m', 'hpi_r_st_us']],\n        on=['year_month', 'property_state'],\n        how='left'\n    )\n    print(f\"  ✓ State HPI merged: coverage = {panel_df['state_hpi'].notna().mean():.1%}\")\n\nn_after = len(panel_df)\nprint(f\"\\nRows: {n_before:,} → {n_after:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "16lctbc32tw",
   "source": "# Calculate origination-relative differences\nprint(\"=== Calculating Origination-Time Differences ===\")\n\n# Get origination-time macro values\nif macro_national is not None:\n    orig_macro = macro_national[['year_month', 'MORTGAGE30US', 'DGS10']].rename(\n        columns={'year_month': 'orig_year_month', \n                 'MORTGAGE30US': 'orig_MORTGAGE30US', \n                 'DGS10': 'orig_DGS10'}\n    )\n    panel_df = panel_df.merge(orig_macro, on='orig_year_month', how='left')\n    \n    # ppi_c_FRMA: Current prepayment incentive (int_rate - current_mortgage_rate)\n    panel_df['ppi_c_FRMA'] = panel_df['int_rate'] - panel_df['MORTGAGE30US']\n    \n    # ppi_o_FRMA: Prepayment incentive at origination\n    panel_df['ppi_o_FRMA'] = panel_df['int_rate'] - panel_df['orig_MORTGAGE30US']\n    \n    # TB10Y_d_t_o: Treasury rate difference (today - origination)\n    panel_df['TB10Y_d_t_o'] = panel_df['DGS10'] - panel_df['orig_DGS10']\n    \n    # FRMA30Y_d_t_o: Mortgage rate difference (today - origination)\n    panel_df['FRMA30Y_d_t_o'] = panel_df['MORTGAGE30US'] - panel_df['orig_MORTGAGE30US']\n    \n    print(f\"  ✓ ppi_c_FRMA coverage: {panel_df['ppi_c_FRMA'].notna().mean():.1%}\")\n    print(f\"  ✓ TB10Y_d_t_o coverage: {panel_df['TB10Y_d_t_o'].notna().mean():.1%}\")\n\n# Get origination-time HPI\nif not state_hpi_long.empty:\n    orig_hpi = state_hpi_long[['year_month', 'property_state', 'state_hpi']].rename(\n        columns={'year_month': 'orig_year_month', 'state_hpi': 'orig_state_hpi'}\n    )\n    panel_df = panel_df.merge(orig_hpi, on=['orig_year_month', 'property_state'], how='left')\n    \n    # hpi_st_d_t_o: HPI difference (today - origination)\n    panel_df['hpi_st_d_t_o'] = panel_df['state_hpi'] - panel_df['orig_state_hpi']\n    \n    print(f\"  ✓ hpi_st_d_t_o coverage: {panel_df['hpi_st_d_t_o'].notna().mean():.1%}\")\n\nprint(\"\\n✓ Origination-relative differences calculated\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jxm13yddxt",
   "source": "## Select Final Columns and Verify Panel",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1r777v1gbyj",
   "source": "# Define final columns for the panel\nFINAL_COLUMNS = [\n    # Identifiers\n    'loan_sequence_number', 'fold', 'vintage_year', 'property_state',\n    \n    # Time indices\n    'loan_age', 'start', 'stop', 'year_month',\n    \n    # Event indicators\n    'event', 'event_code',\n    \n    # Static covariates\n    'int_rate', 'orig_upb', 'fico_score', 'dti_r', 'ltv_r',\n    \n    # Time-varying behavioral\n    'bal_repaid', 't_act_12m', 't_del_30d_12m', 't_del_60d_12m',\n    \n    # Time-varying macro\n    'hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o',\n    'ppi_o_FRMA', 'hpi_st_log12m', 'hpi_r_st_us',\n    'st_unemp_r12m', 'st_unemp_r3m', \n    'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m',\n]\n\n# Filter to available columns\navailable_cols = [c for c in FINAL_COLUMNS if c in panel_df.columns]\nmissing_cols = [c for c in FINAL_COLUMNS if c not in panel_df.columns]\n\nprint(\"=== Column Selection ===\")\nprint(f\"Requested: {len(FINAL_COLUMNS)} columns\")\nprint(f\"Available: {len(available_cols)} columns\")\nif missing_cols:\n    print(f\"Missing: {missing_cols}\")\n\n# Select final columns\npanel_final = panel_df[available_cols].copy()\nprint(f\"\\nFinal panel shape: {panel_final.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kkcroyhra9",
   "source": "# Verify panel integrity\nprint(\"=== Panel Verification ===\")\n\n# Check loan counts per fold\nprint(\"\\nLoans per fold:\")\nloans_per_fold = panel_final.groupby('fold')['loan_sequence_number'].nunique()\nfor fold, n_loans in loans_per_fold.items():\n    print(f\"  Fold {fold}: {n_loans:,} loans\")\n\n# Check events per fold\nprint(\"\\nEvents per fold:\")\nterminal_events = panel_final[panel_final['event'] == 1].groupby(['fold', 'event_code']).size().unstack(fill_value=0)\nprint(terminal_events)\n\n# Check variable coverage\nprint(\"\\n=== Variable Coverage ===\")\nLOAN_VARS = ['int_rate', 'orig_upb', 'fico_score', 'dti_r', 'ltv_r',\n             'bal_repaid', 't_act_12m', 't_del_30d_12m', 't_del_60d_12m']\nMACRO_VARS = ['hpi_st_d_t_o', 'ppi_c_FRMA', 'TB10Y_d_t_o', 'FRMA30Y_d_t_o',\n              'ppi_o_FRMA', 'hpi_st_log12m', 'hpi_r_st_us', \n              'st_unemp_r12m', 'st_unemp_r3m', 'TB10Y_r12m', 'T10Y3MM', 'T10Y3MM_r12m']\n\nprint(\"\\nLoan-level variables:\")\nfor var in LOAN_VARS:\n    if var in panel_final.columns:\n        coverage = panel_final[var].notna().mean()\n        print(f\"  {var}: {coverage:.1%}\")\n\nprint(\"\\nMacro variables:\")\nfor var in MACRO_VARS:\n    if var in panel_final.columns:\n        coverage = panel_final[var].notna().mean()\n        print(f\"  {var}: {coverage:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lh30sczamem",
   "source": "# Save the loan-month panel\nprint(\"=== Saving Loan-Month Panel ===\")\n\noutput_path = PROCESSED_DATA_DIR / 'loan_month_panel.parquet'\npanel_final.to_parquet(output_path, index=False)\n\nprint(f\"✓ Saved to: {output_path}\")\nprint(f\"  Shape: {panel_final.shape}\")\nprint(f\"  File size: {output_path.stat().st_size / 1e6:.1f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vy0769omb6e",
   "source": "---\n\n## Phase 2 Summary\n\n### Completed\n- ✅ Processed 16 vintages (2010-2025)\n- ✅ Filtered to sampled loans (early filtering for memory efficiency)\n- ✅ Calculated behavioral variables (rolling 12-month counts)\n- ✅ Created interval format (start, stop) for Cox regression\n- ✅ Merged static covariates from origination data\n- ✅ Merged time-varying macro data (national + state-level)\n- ✅ Calculated origination-relative differences\n- ✅ Saved loan-month panel\n\n### Output\n| File | Description |\n|------|-------------|\n| `loan_month_panel.parquet` | ~5.5M loan-month records with time-varying covariates |\n\n### Data Structure\nEach row represents one loan-month with:\n- **Interval format**: `(start, stop)` for Cox regression\n- **Event indicator**: 1 if event occurs at `stop`, 0 otherwise\n- **Static covariates**: Fixed at origination\n- **Time-varying covariates**: Updated each month\n\n### Next Steps\nUse `loan_month_panel.parquet` in notebook 05 for cause-specific Cox regression with time-varying covariates.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qm6f4e0pxud",
   "source": "print(\"=\" * 60)\nprint(\"PHASE 2 COMPLETE: Loan-Month Panel Created\")\nprint(\"=\" * 60)\nprint(f\"\\nPanel statistics:\")\nprint(f\"  Total loan-months: {len(panel_final):,}\")\nprint(f\"  Total loans: {panel_final['loan_sequence_number'].nunique():,}\")\nprint(f\"  Folds: {panel_final['fold'].nunique()}\")\nprint(f\"  Vintages: {panel_final['vintage_year'].min()} - {panel_final['vintage_year'].max()}\")\nprint(f\"\\nEvents:\")\nprint(f\"  Prepayments: {(panel_final[panel_final['event']==1]['event_code']==1).sum():,}\")\nprint(f\"  Defaults: {(panel_final[panel_final['event']==1]['event_code']==2).sum():,}\")\nprint(f\"\\nOutput file: {output_path}\")\nprint(f\"\\nReady for Phase 3 (notebook 05): Cause-Specific Cox with time-varying covariates\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}