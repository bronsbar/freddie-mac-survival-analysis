{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# DeepHit for Competing Risks\n\nThis notebook implements **DeepHit** (Lee et al., 2018) for competing risks analysis using **pure PyTorch**. DeepHit is a deep learning approach that directly models the probability mass function of survival times without assuming any particular form for the underlying stochastic process.\n\n## Methodology\n\n| Aspect | Description |\n|--------|-------------|\n| **Model** | DeepHit with competing risks |\n| **Implementation** | Pure PyTorch (custom network and loss) |\n| **Features** | Blumenstock et al. (2022) - 21 variables |\n| **Evaluation** | Time-dependent C-index at 24, 48, 72 months |\n\n## Key Advantages of DeepHit\n\n| Feature | Description |\n|---------|-------------|\n| **No PH assumption** | Does not assume proportional hazards |\n| **Competing risks** | Native support for multiple event types |\n| **Flexible** | Can capture complex non-linear relationships |\n| **Direct modeling** | Models PMF directly, not hazard function |\n\n## DeepHit Loss Function\n\nThe DeepHit loss combines two components:\n1. **Negative log-likelihood (L1)**: Maximizes probability of observed event at observed time\n2. **Ranking loss (L2)**: Ensures correct ordering of event times\n\n$$\\mathcal{L} = \\mathcal{L}_1 + \\alpha \\cdot \\mathcal{L}_2$$\n\n## References\n\n- Lee, C., Zame, W., Yoon, J., & van der Schaar, M. (2018). DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks. AAAI."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Deep learning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# pycox for evaluation metrics only\nfrom pycox.evaluation import EvalSurv\n\n# Survival analysis (for comparison metrics)\nfrom sksurv.util import Surv\nfrom sksurv.metrics import concordance_index_censored, concordance_index_ipcw\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Device setup (MPS for macOS, CUDA for Linux/Windows, CPU fallback)\nif torch.backends.mps.is_available():\n    DEVICE = torch.device('mps')\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\n    print(\"Using CUDA GPU\")\nelse:\n    DEVICE = torch.device('cpu')\n    print(\"Using CPU\")\n\n# Time horizons for evaluation (matching previous notebooks)\nTIME_HORIZONS = [24, 48, 72]\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"Time horizons for C-index evaluation: {TIME_HORIZONS} months\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURATION ===\nDATA_DIR = Path('../data/processed')\nFIGURES_DIR = Path('../reports/figures')\nMODELS_DIR = Path('../models')\n\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Cross-validation folds (Blumenstock methodology)\nTRAIN_FOLDS = list(range(10))  # Folds 0-9 for training\nVAL_FOLDS = [9]                 # Use fold 9 for validation (early stopping)\nTEST_FOLD = 10                  # Fold 10 for testing\n\n# Number of discrete time points\nNUM_DURATIONS = 200\n\n# DeepHit hyperparameters (Blumenstock et al. 2022)\nDEEPHIT_PARAMS = {\n    # Training\n    'batch_size': 256,\n    'epochs': 100,\n    'learning_rate': 0.01,\n    # Loss function\n    'alpha': 0.2,               # Weight for ranking loss\n    'sigma': 0.1,               # Smoothing parameter for ranking loss\n    # Regularization\n    'dropout': 0.2,\n    'batch_norm': True,\n}\n\n# Architecture is defined in DeepHitNetwork class:\n# - Shared: 3 layers x 300 nodes\n# - Heads: 5 layers x 200 nodes each\n\nprint(f\"Training folds: {[f for f in TRAIN_FOLDS if f not in VAL_FOLDS]}\")\nprint(f\"Validation fold: {VAL_FOLDS}\")\nprint(f\"Test fold: {TEST_FOLD}\")\nprint(f\"\\nTime discretization: {NUM_DURATIONS} bins\")\nprint(f\"\\nDeepHit parameters:\")\nfor k, v in DEEPHIT_PARAMS.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Loan-Month Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan-month panel data\n",
    "print(\"Loading loan-month panel data...\")\n",
    "panel_df = pd.read_parquet(DATA_DIR / 'loan_month_panel.parquet')\n",
    "\n",
    "print(f\"Loaded {len(panel_df):,} loan-months\")\n",
    "print(f\"Unique loans: {panel_df['loan_sequence_number'].nunique():,}\")\n",
    "print(f\"Folds: {sorted(panel_df['fold'].unique())}\")\n",
    "print(f\"Vintages: {panel_df['vintage_year'].min()} - {panel_df['vintage_year'].max()}\")\n",
    "\n",
    "print(\"\\nEvent distribution (terminal observations):\")\n",
    "event_names = {0: 'Censored', 1: 'Prepay', 2: 'Default'}\n",
    "terminal_events = panel_df[panel_df['event'] == 1].groupby('event_code').size()\n",
    "for code, count in terminal_events.items():\n",
    "    print(f\"  {event_names.get(code, 'Other')} (k={code}): {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define Features (Blumenstock et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups (Blumenstock et al. 2022, Table 2)\n",
    "# Matching previous notebooks exactly\n",
    "\n",
    "# Static covariates (fixed at origination) - 5 variables\n",
    "STATIC_FEATURES = [\n",
    "    'int_rate',      # Initial interest rate\n",
    "    'orig_upb',      # Original unpaid balance\n",
    "    'fico_score',    # Initial FICO score\n",
    "    'dti_r',         # Initial debt-to-income ratio\n",
    "    'ltv_r',         # Initial loan-to-value ratio\n",
    "]\n",
    "\n",
    "# Behavioral covariates (time-varying) - 4 variables\n",
    "BEHAVIORAL_FEATURES = [\n",
    "    'bal_repaid',      # Current repaid balance in percent\n",
    "    't_act_12m',       # No. of times not being delinquent in last 12 months\n",
    "    't_del_30d_12m',   # No. of times being 30 days delinquent in last 12 months\n",
    "    't_del_60d_12m',   # No. of times being 60 days delinquent in last 12 months\n",
    "]\n",
    "\n",
    "# Macro covariates (time-varying) - 12 variables\n",
    "MACRO_FEATURES = [\n",
    "    'hpi_st_d_t_o',    # HPI difference between origination and today (state)\n",
    "    'ppi_c_FRMA',      # Current prepayment incentive\n",
    "    'TB10Y_d_t_o',     # Treasury rate difference\n",
    "    'FRMA30Y_d_t_o',   # 30Y FRM difference\n",
    "    'ppi_o_FRMA',      # Prepayment incentive at origination\n",
    "    'hpi_st_log12m',   # HPI 12-month log return (state)\n",
    "    'hpi_r_st_us',     # Ratio of state HPI to national HPI\n",
    "    'st_unemp_r12m',   # Unemployment 12-month log return (state)\n",
    "    'st_unemp_r3m',    # Unemployment 3-month log return (state)\n",
    "    'TB10Y_r12m',      # Treasury rate 12-month return\n",
    "    'T10Y3MM',         # Yield spread (10Y - 3M)\n",
    "    'T10Y3MM_r12m',    # Yield spread 12-month return\n",
    "]\n",
    "\n",
    "ALL_FEATURES = STATIC_FEATURES + BEHAVIORAL_FEATURES + MACRO_FEATURES\n",
    "\n",
    "# Filter to available features\n",
    "feature_cols = [f for f in ALL_FEATURES if f in panel_df.columns]\n",
    "missing_features = [f for f in ALL_FEATURES if f not in panel_df.columns]\n",
    "\n",
    "print(\"=== Feature Groups (Blumenstock et al. 2022) ===\")\n",
    "print(f\"Static features: {len([f for f in STATIC_FEATURES if f in feature_cols])}/5\")\n",
    "print(f\"Behavioral features: {len([f for f in BEHAVIORAL_FEATURES if f in feature_cols])}/4\")\n",
    "print(f\"Macro features: {len([f for f in MACRO_FEATURES if f in feature_cols])}/12\")\n",
    "print(f\"\\nTotal available: {len(feature_cols)}/21\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nMissing features ({len(missing_features)}):\")\n",
    "    for f in missing_features:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare Data for DeepHit\n",
    "\n",
    "DeepHit requires terminal observations (one per loan) with discretized time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DeepHit, we need terminal observations (one per loan)\n",
    "# Get the last observation for each loan\n",
    "print(\"=== Preparing Terminal Observations ===\")\n",
    "\n",
    "time_col = 'loan_age'\n",
    "event_col = 'event_code'\n",
    "\n",
    "# Sort and get last observation per loan\n",
    "panel_df = panel_df.sort_values(['loan_sequence_number', time_col])\n",
    "terminal_df = panel_df.groupby('loan_sequence_number').last().reset_index()\n",
    "\n",
    "print(f\"Terminal observations: {len(terminal_df):,} loans\")\n",
    "\n",
    "# Lag bal_repaid to avoid data leakage (matching previous notebooks)\n",
    "if 'bal_repaid' in feature_cols:\n",
    "    print(\"\\nLagging bal_repaid to avoid data leakage...\")\n",
    "    \n",
    "    def get_lagged_bal_repaid(group):\n",
    "        if len(group) >= 2:\n",
    "            return group['bal_repaid'].iloc[-2]\n",
    "        else:\n",
    "            return group['bal_repaid'].iloc[-1]\n",
    "    \n",
    "    bal_repaid_lag = panel_df.groupby('loan_sequence_number').apply(get_lagged_bal_repaid)\n",
    "    terminal_df['bal_repaid_lag1'] = terminal_df['loan_sequence_number'].map(bal_repaid_lag)\n",
    "    \n",
    "    feature_cols = [f if f != 'bal_repaid' else 'bal_repaid_lag1' for f in feature_cols]\n",
    "    print(\"  Created bal_repaid_lag1\")\n",
    "\n",
    "# Log transform UPB\n",
    "if 'orig_upb' in terminal_df.columns:\n",
    "    terminal_df['log_upb'] = np.log(terminal_df['orig_upb'])\n",
    "    feature_cols = [f if f != 'orig_upb' else 'log_upb' for f in feature_cols]\n",
    "    print(\"  Created log_upb\")\n",
    "\n",
    "# Drop rows with missing features\n",
    "n_before = len(terminal_df)\n",
    "terminal_df = terminal_df.dropna(subset=feature_cols)\n",
    "n_after = len(terminal_df)\n",
    "print(f\"\\nAfter dropping NaN: {n_after:,} loans (dropped {n_before - n_after:,})\")\n",
    "\n",
    "print(f\"\\nFeatures ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by folds (matching previous notebooks)\n",
    "print(\"=== Splitting Data by Folds ===\")\n",
    "\n",
    "# Training uses folds 0-8, validation uses fold 9, test uses fold 10\n",
    "train_folds_actual = [f for f in TRAIN_FOLDS if f not in VAL_FOLDS]\n",
    "\n",
    "train_df = terminal_df[terminal_df['fold'].isin(train_folds_actual)].copy()\n",
    "val_df = terminal_df[terminal_df['fold'].isin(VAL_FOLDS)].copy()\n",
    "test_df = terminal_df[terminal_df['fold'] == TEST_FOLD].copy()\n",
    "\n",
    "print(f\"Training set (folds {train_folds_actual}): {len(train_df):,} loans\")\n",
    "print(f\"Validation set (fold {VAL_FOLDS}): {len(val_df):,} loans\")\n",
    "print(f\"Test set (fold {TEST_FOLD}): {len(test_df):,} loans\")\n",
    "\n",
    "# Event distribution\n",
    "print(\"\\nTraining set event distribution:\")\n",
    "for code, name in event_names.items():\n",
    "    count = (train_df[event_col] == code).sum()\n",
    "    print(f\"  {name}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for neural networks)\n",
    "print(\"=== Standardizing Features ===\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df[feature_cols]).astype('float32')\n",
    "X_val = scaler.transform(val_df[feature_cols]).astype('float32')\n",
    "X_test = scaler.transform(test_df[feature_cols]).astype('float32')\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Durations\n",
    "duration_train = train_df[time_col].values\n",
    "duration_val = val_df[time_col].values\n",
    "duration_test = test_df[time_col].values\n",
    "\n",
    "print(f\"\\nDuration range: {duration_train.min()} - {duration_train.max()} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acvv9byz8ge",
   "source": "---\n\n## Pure PyTorch DeepHit Implementation\n\nCustom implementation of DeepHit (Lee et al., 2018) without using pycox wrappers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vangr7w6wjo",
   "source": "# === DeepHit Model Architecture (Blumenstock et al. 2022) ===\n\nclass DeepHitNetwork(torch.nn.Module):\n    \"\"\"\n    DeepHit neural network for competing risks (Blumenstock et al. 2022).\n    \n    Architecture:\n    - Shared FFN: 3 layers, 300 nodes each\n    - Residual connection: shared output + input features\n    - Cause-specific heads: 2 heads (prepay, default), each 5 layers, 200 nodes\n    - Joint softmax over (time, cause) for PMF\n    \n    Outputs joint probability mass function P(T=t, K=k) where:\n    - T is discrete time\n    - K is cause (0=prepay, 1=default)\n    \"\"\"\n    def __init__(\n        self,\n        in_features: int,\n        num_time_bins: int,\n        num_causes: int = 2,\n        shared_layers: int = 3,\n        shared_nodes: int = 300,\n        head_layers: int = 5,\n        head_nodes: int = 200,\n        dropout: float = 0.2,\n        batch_norm: bool = True,\n    ):\n        super().__init__()\n        \n        self.in_features = in_features\n        self.num_time_bins = num_time_bins\n        self.num_causes = num_causes\n        \n        # === Shared FFN (3 layers, 300 nodes) ===\n        shared = []\n        prev_dim = in_features\n        for _ in range(shared_layers):\n            shared.append(torch.nn.Linear(prev_dim, shared_nodes))\n            if batch_norm:\n                shared.append(torch.nn.BatchNorm1d(shared_nodes))\n            shared.append(torch.nn.ReLU())\n            shared.append(torch.nn.Dropout(dropout))\n            prev_dim = shared_nodes\n        self.shared = torch.nn.Sequential(*shared)\n        \n        # Residual projection: project input to shared_nodes for addition\n        self.residual_proj = torch.nn.Linear(in_features, shared_nodes)\n        \n        # === Cause-specific heads (5 layers, 200 nodes each) ===\n        self.heads = torch.nn.ModuleList()\n        for _ in range(num_causes):\n            head = []\n            prev_dim = shared_nodes  # Input from shared + residual\n            for layer_idx in range(head_layers):\n                head.append(torch.nn.Linear(prev_dim, head_nodes))\n                if batch_norm:\n                    head.append(torch.nn.BatchNorm1d(head_nodes))\n                head.append(torch.nn.ReLU())\n                head.append(torch.nn.Dropout(dropout))\n                prev_dim = head_nodes\n            # Output layer for this head: num_time_bins\n            head.append(torch.nn.Linear(prev_dim, num_time_bins))\n            self.heads.append(torch.nn.Sequential(*head))\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass.\n        \n        Args:\n            x: Input features [batch_size, in_features]\n            \n        Returns:\n            Joint PMF over (time, cause) [batch_size, num_causes, num_time_bins]\n        \"\"\"\n        batch_size = x.shape[0]\n        \n        # Shared layers\n        shared_out = self.shared(x)  # [batch, shared_nodes]\n        \n        # Residual connection: add projected input\n        residual = self.residual_proj(x)  # [batch, shared_nodes]\n        combined = shared_out + residual  # [batch, shared_nodes]\n        \n        # Cause-specific heads\n        head_outputs = []\n        for head in self.heads:\n            head_out = head(combined)  # [batch, num_time_bins]\n            head_outputs.append(head_out)\n        \n        # Stack: [batch, num_causes, num_time_bins]\n        logits = torch.stack(head_outputs, dim=1)\n        \n        # Joint softmax over ALL (cause, time) combinations\n        # Reshape to [batch, num_causes * num_time_bins], softmax, reshape back\n        logits_flat = logits.view(batch_size, -1)\n        pmf_flat = torch.softmax(logits_flat, dim=-1)\n        pmf = pmf_flat.view(batch_size, self.num_causes, self.num_time_bins)\n        \n        return pmf\n    \n    def predict_cif(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Predict cause-specific cumulative incidence functions (CIF).\n        \n        CIF_k(t) = P(T <= t, K = k) = sum_{s <= t} P(T=s, K=k)\n        \n        Args:\n            x: Input features [batch_size, in_features]\n            \n        Returns:\n            CIF for each cause [batch_size, num_causes, num_time_bins]\n        \"\"\"\n        pmf = self.forward(x)  # [batch, num_causes, num_time_bins]\n        cif = torch.cumsum(pmf, dim=-1)\n        return cif\n    \n    def predict_survival(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Predict overall survival function S(t) = P(T > t).\n        \n        S(t) = 1 - sum_k CIF_k(t)\n        \n        Args:\n            x: Input features [batch_size, in_features]\n            \n        Returns:\n            Survival probabilities [batch_size, num_time_bins]\n        \"\"\"\n        cif = self.predict_cif(x)  # [batch, num_causes, num_time_bins]\n        # Sum CIF over all causes\n        total_cif = cif.sum(dim=1)  # [batch, num_time_bins]\n        survival = 1 - total_cif\n        return survival\n    \n    def predict_cause_specific_hazard(self, x: torch.Tensor, cause: int) -> torch.Tensor:\n        \"\"\"\n        Predict cause-specific hazard for a given cause.\n        \n        h_k(t) = P(T=t, K=k | T >= t)\n        \n        Args:\n            x: Input features [batch_size, in_features]\n            cause: Cause index (0=prepay, 1=default)\n            \n        Returns:\n            Hazard probabilities [batch_size, num_time_bins]\n        \"\"\"\n        pmf = self.forward(x)  # [batch, num_causes, num_time_bins]\n        survival = self.predict_survival(x)  # [batch, num_time_bins]\n        \n        # Shift survival by 1 for S(t-1)\n        survival_prev = torch.cat([\n            torch.ones(survival.shape[0], 1, device=survival.device),\n            survival[:, :-1]\n        ], dim=-1)\n        \n        # h_k(t) = f_k(t) / S(t-1)\n        hazard = pmf[:, cause, :] / (survival_prev + 1e-7)\n        return hazard\n\n\n# Print architecture summary\nprint(\"DeepHitNetwork (Blumenstock et al. 2022 architecture)\")\nprint(\"=\" * 60)\nprint(f\"Shared FFN: 3 layers x 300 nodes\")\nprint(f\"Residual: shared_output + proj(input)\")\nprint(f\"Head 1 (Prepay):  5 layers x 200 nodes -> {NUM_DURATIONS} time bins\")\nprint(f\"Head 2 (Default): 5 layers x 200 nodes -> {NUM_DURATIONS} time bins\")\nprint(f\"Output: Joint softmax over {2 * NUM_DURATIONS} (cause, time) combinations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "o4enx984on",
   "source": "# === DeepHit Loss Function (Competing Risks) ===\n\nclass DeepHitLoss(torch.nn.Module):\n    \"\"\"\n    DeepHit loss function for competing risks.\n    \n    Combines:\n    1. Negative log-likelihood loss (L1) for joint distribution P(T=t, K=k)\n    2. Ranking loss (L2) for each cause\n    \n    Loss = L1 + alpha * L2\n    \n    Reference: Lee et al. (2018) - DeepHit\n    \"\"\"\n    def __init__(self, alpha: float = 0.2, sigma: float = 0.1):\n        \"\"\"\n        Args:\n            alpha: Weight for ranking loss (default 0.2)\n            sigma: Smoothing parameter for ranking loss (default 0.1)\n        \"\"\"\n        super().__init__()\n        self.alpha = alpha\n        self.sigma = sigma\n        \n    def forward(\n        self,\n        pmf: torch.Tensor,\n        durations: torch.Tensor,\n        events: torch.Tensor,\n        time_bins: torch.Tensor,\n    ) -> tuple:\n        \"\"\"\n        Compute DeepHit loss for competing risks.\n        \n        Args:\n            pmf: Predicted joint PMF [batch_size, num_causes, num_time_bins]\n            durations: Observed durations [batch_size]\n            events: Event codes (0=censored, 1=prepay, 2=default) [batch_size]\n            time_bins: Discrete time bin edges [num_time_bins + 1]\n            \n        Returns:\n            Tuple of (total_loss, nll_loss, ranking_loss)\n        \"\"\"\n        batch_size = pmf.shape[0]\n        num_causes = pmf.shape[1]\n        num_bins = pmf.shape[2]\n        device = pmf.device\n        \n        # Map durations to bin indices\n        bin_indices = torch.bucketize(durations, time_bins[1:])\n        bin_indices = torch.clamp(bin_indices, 0, num_bins - 1)\n        \n        # === Negative Log-Likelihood Loss (L1) ===\n        # For uncensored (event > 0): -log(P(T=t, K=k))\n        # For censored (event = 0): -log(S(t)) where S(t) = 1 - sum_k CIF_k(t)\n        \n        eps = 1e-7\n        \n        # Compute CIF for survival calculation\n        cif = torch.cumsum(pmf, dim=-1)  # [batch, num_causes, num_time_bins]\n        total_cif = cif.sum(dim=1)  # [batch, num_time_bins]\n        survival = 1 - total_cif  # [batch, num_time_bins]\n        \n        # Get survival at observed time for censored samples\n        survival_at_time = survival[torch.arange(batch_size, device=device), bin_indices]\n        \n        # For uncensored: get PMF at (observed_time, observed_cause)\n        # Event codes: 0=censored, 1=prepay (cause 0), 2=default (cause 1)\n        # Map event codes to cause indices: event - 1 (but only for event > 0)\n        cause_indices = (events - 1).long()\n        cause_indices = torch.clamp(cause_indices, 0, num_causes - 1)  # Safety clamp\n        \n        # Get PMF at observed (time, cause)\n        pmf_at_event = pmf[\n            torch.arange(batch_size, device=device),\n            cause_indices,\n            bin_indices\n        ]\n        \n        # Masks\n        is_censored = (events == 0).float()\n        is_uncensored = (events > 0).float()\n        \n        # NLL components\n        nll_uncensored = -torch.log(pmf_at_event + eps) * is_uncensored\n        nll_censored = -torch.log(survival_at_time + eps) * is_censored\n        nll_loss = (nll_uncensored + nll_censored).mean()\n        \n        # === Ranking Loss (L2) ===\n        if self.alpha > 0 and is_uncensored.sum() > 0:\n            ranking_loss = self._compute_ranking_loss(\n                pmf, cif, bin_indices, events, num_causes\n            )\n        else:\n            ranking_loss = torch.tensor(0.0, device=device)\n        \n        total_loss = nll_loss + self.alpha * ranking_loss\n        \n        return total_loss, nll_loss, ranking_loss\n    \n    def _compute_ranking_loss(\n        self,\n        pmf: torch.Tensor,\n        cif: torch.Tensor,\n        bin_indices: torch.Tensor,\n        events: torch.Tensor,\n        num_causes: int,\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute ranking loss for competing risks.\n        \n        For each cause k, compare pairs where subject i experienced cause k\n        before subject j (who either experienced k later, different cause, or censored).\n        \"\"\"\n        device = pmf.device\n        batch_size = pmf.shape[0]\n        \n        ranking_loss = torch.tensor(0.0, device=device)\n        n_pairs = 0\n        \n        # For each cause\n        for k in range(num_causes):\n            event_code = k + 1  # Map cause index to event code\n            \n            # Find subjects who experienced this cause\n            cause_mask = (events == event_code)\n            cause_indices = torch.where(cause_mask)[0]\n            \n            if len(cause_indices) < 1:\n                continue\n            \n            # Sample subjects for efficiency\n            sample_size = min(100, len(cause_indices))\n            if len(cause_indices) > sample_size:\n                perm = torch.randperm(len(cause_indices), device=device)[:sample_size]\n                cause_indices = cause_indices[perm]\n            \n            for idx in cause_indices:\n                t_i = bin_indices[idx]\n                \n                # Compare with subjects who have later event time\n                # (regardless of their cause or censoring status)\n                later_mask = bin_indices > t_i\n                later_indices = torch.where(later_mask)[0]\n                \n                if len(later_indices) == 0:\n                    continue\n                \n                # Sample if too many\n                if len(later_indices) > 10:\n                    perm = torch.randperm(len(later_indices), device=device)[:10]\n                    later_indices = later_indices[perm]\n                \n                # CIF_k at time t_i for subject i and subjects j\n                cif_i_at_ti = cif[idx, k, t_i]\n                cif_j_at_ti = cif[later_indices, k, t_i]\n                \n                # Penalize if CIF_j > CIF_i (j predicted higher risk at t_i)\n                diff = cif_j_at_ti - cif_i_at_ti\n                pair_loss = torch.exp(diff / self.sigma)\n                \n                ranking_loss = ranking_loss + pair_loss.sum()\n                n_pairs += len(later_indices)\n        \n        if n_pairs > 0:\n            ranking_loss = ranking_loss / n_pairs\n        \n        return ranking_loss\n\n\nprint(\"DeepHitLoss (Competing Risks) defined\")\nprint(f\"  - Joint NLL over P(T=t, K=k)\")\nprint(f\"  - Cause-specific ranking loss\")\nprint(f\"  - alpha={DEEPHIT_PARAMS['alpha']}, sigma={DEEPHIT_PARAMS['sigma']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2cak13lzqev",
   "source": "# === Create Time Bins for Discretization ===\n\nprint(\"=== Creating Time Bins ===\")\n\n# Get min/max durations\nall_durations = np.concatenate([duration_train, duration_val, duration_test])\nmin_dur, max_dur = all_durations.min(), all_durations.max()\n\n# Create evenly spaced time bins\ntime_bins = np.linspace(min_dur, max_dur, NUM_DURATIONS + 1)\ntime_bins_tensor = torch.tensor(time_bins, dtype=torch.float32)\n\nprint(f\"Duration range: {min_dur:.0f} - {max_dur:.0f} months\")\nprint(f\"Number of bins: {NUM_DURATIONS}\")\nprint(f\"Bin width: {(max_dur - min_dur) / NUM_DURATIONS:.2f} months\")\n\n# Store time points (bin centers) for later use\ntime_points = (time_bins[:-1] + time_bins[1:]) / 2\nprint(f\"Time points: {time_points[:5]}... {time_points[-5:]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z7i4mb3i8g8",
   "source": "# === Training Function (Competing Risks) ===\n\ndef train_deephit(\n    model: torch.nn.Module,\n    criterion: DeepHitLoss,\n    X_train: np.ndarray,\n    y_train: np.ndarray,  # durations\n    e_train: np.ndarray,  # event codes (0=censored, 1=prepay, 2=default)\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n    e_val: np.ndarray,\n    time_bins: torch.Tensor,\n    batch_size: int = 256,\n    epochs: int = 100,\n    learning_rate: float = 0.01,\n    patience: int = 10,\n    device: torch.device = torch.device('cpu'),\n) -> dict:\n    \"\"\"\n    Train DeepHit competing risks model with early stopping.\n    \n    Returns:\n        Dictionary with training history\n    \"\"\"\n    # Move model and time bins to device\n    model = model.to(device)\n    time_bins = time_bins.to(device)\n    \n    # Create tensors\n    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n    y_train_t = torch.tensor(y_train, dtype=torch.float32)\n    e_train_t = torch.tensor(e_train, dtype=torch.float32)\n    \n    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n    e_val_t = torch.tensor(e_val, dtype=torch.float32).to(device)\n    \n    # Create DataLoader\n    train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t, e_train_t)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n    \n    # Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_nll': [],\n        'train_rank': [],\n        'val_nll': [],\n        'val_rank': [],\n    }\n    \n    # Early stopping\n    best_val_loss = float('inf')\n    best_epoch = 0\n    best_state = None\n    epochs_no_improve = 0\n    \n    print(f\"Training on {device}\")\n    print(f\"{'Epoch':>6} | {'Train Loss':>10} | {'Val Loss':>10} | {'NLL':>8} | {'Rank':>8}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_losses = []\n        train_nlls = []\n        train_ranks = []\n        \n        for batch_X, batch_y, batch_e in train_loader:\n            batch_X = batch_X.to(device)\n            batch_y = batch_y.to(device)\n            batch_e = batch_e.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            pmf = model(batch_X)\n            loss, nll, rank = criterion(pmf, batch_y, batch_e, time_bins)\n            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            train_losses.append(loss.item())\n            train_nlls.append(nll.item())\n            train_ranks.append(rank.item())\n        \n        avg_train_loss = np.mean(train_losses)\n        avg_train_nll = np.mean(train_nlls)\n        avg_train_rank = np.mean(train_ranks)\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            pmf_val = model(X_val_t)\n            val_loss, val_nll, val_rank = criterion(pmf_val, y_val_t, e_val_t, time_bins)\n            val_loss = val_loss.item()\n            val_nll = val_nll.item()\n            val_rank = val_rank.item()\n        \n        # Update scheduler\n        scheduler.step(val_loss)\n        \n        # Save history\n        history['train_loss'].append(avg_train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_nll'].append(avg_train_nll)\n        history['train_rank'].append(avg_train_rank)\n        history['val_nll'].append(val_nll)\n        history['val_rank'].append(val_rank)\n        \n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        \n        # Print progress\n        if epoch % 10 == 0 or epoch == epochs - 1:\n            print(f\"{epoch:>6} | {avg_train_loss:>10.4f} | {val_loss:>10.4f} | {val_nll:>8.4f} | {val_rank:>8.4f}\")\n        \n        # Check early stopping\n        if epochs_no_improve >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch}\")\n            break\n    \n    # Restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n        model.to(device)\n    \n    print(f\"\\nBest epoch: {best_epoch} (val_loss={best_val_loss:.4f})\")\n    \n    return history\n\n\nprint(\"train_deephit function defined (with gradient clipping)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fit-header",
   "metadata": {},
   "source": "---\n\n## Train Joint DeepHit Model\n\nTrain a single model that jointly predicts prepayment and default risks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-prepay",
   "metadata": {},
   "outputs": [],
   "source": "# === Train Joint DeepHit Model ===\nprint(\"=== Training Joint DeepHit Model (Competing Risks) ===\")\n\n# Get event codes (0=censored, 1=prepay, 2=default)\nevent_train = train_df[event_col].values.astype('float32')\nevent_val = val_df[event_col].values.astype('float32')\nevent_test = test_df[event_col].values.astype('float32')\n\nprint(f\"\\nEvent distribution (training):\")\nfor code, name in event_names.items():\n    count = (event_train == code).sum()\n    print(f\"  {name} (k={code}): {count:,.0f}\")\n\n# Create model (Blumenstock architecture)\nin_features = X_train.shape[1]\nmodel = DeepHitNetwork(\n    in_features=in_features,\n    num_time_bins=NUM_DURATIONS,\n    num_causes=2,\n    shared_layers=3,\n    shared_nodes=300,\n    head_layers=5,\n    head_nodes=200,\n    dropout=DEEPHIT_PARAMS['dropout'],\n    batch_norm=DEEPHIT_PARAMS['batch_norm'],\n)\n\n# Count parameters\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel parameters: {n_params:,}\")\n\n# Create loss function\ncriterion = DeepHitLoss(\n    alpha=DEEPHIT_PARAMS['alpha'],\n    sigma=DEEPHIT_PARAMS['sigma'],\n)\n\n# Train model\nhistory = train_deephit(\n    model=model,\n    criterion=criterion,\n    X_train=X_train,\n    y_train=duration_train,\n    e_train=event_train,\n    X_val=X_val,\n    y_val=duration_val,\n    e_val=event_val,\n    time_bins=time_bins_tensor,\n    batch_size=DEEPHIT_PARAMS['batch_size'],\n    epochs=DEEPHIT_PARAMS['epochs'],\n    learning_rate=DEEPHIT_PARAMS['learning_rate'],\n    patience=10,\n    device=DEVICE,\n)\n\nprint(\"\\nJoint model training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Total loss\nax = axes[0]\nax.plot(history['train_loss'], label='Train')\nax.plot(history['val_loss'], label='Validation')\nax.set_xlabel('Epoch')\nax.set_ylabel('Total Loss')\nax.set_title('DeepHit: Total Loss')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# NLL component\nax = axes[1]\nax.plot(history['train_nll'], label='Train')\nax.plot(history['val_nll'], label='Validation')\nax.set_xlabel('Epoch')\nax.set_ylabel('NLL Loss')\nax.set_title('DeepHit: NLL Component')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Ranking component\nax = axes[2]\nax.plot(history['train_rank'], label='Train')\nax.plot(history['val_rank'], label='Validation')\nax.set_xlabel('Epoch')\nax.set_ylabel('Ranking Loss')\nax.set_title('DeepHit: Ranking Component')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / 'deephit_training_curves.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Evaluate using time-dependent C-index at 24, 48, and 72 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-prepay",
   "metadata": {},
   "outputs": [],
   "source": "# === Evaluate Joint Model ===\nprint(\"=== DeepHit Joint Model Evaluation ===\")\n\n# Get predictions from joint model\nmodel.eval()\nwith torch.no_grad():\n    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n    \n    # Get CIF for each cause\n    cif_tensor = model.predict_cif(X_test_t)  # [batch, num_causes, num_time_bins]\n    cif_np = cif_tensor.cpu().numpy()\n    \n    # Get overall survival\n    surv_tensor = model.predict_survival(X_test_t)  # [batch, num_time_bins]\n    surv_np = surv_tensor.cpu().numpy()\n\n# Extract cause-specific CIFs\ncif_prepay = cif_np[:, 0, :]   # Prepay CIF [batch, time]\ncif_default = cif_np[:, 1, :]  # Default CIF [batch, time]\n\nprint(f\"CIF shapes: prepay={cif_prepay.shape}, default={cif_default.shape}\")\nprint(f\"Survival shape: {surv_np.shape}\")\n\n# Convert survival to DataFrame for EvalSurv\nsurv_df = pd.DataFrame(surv_np.T, index=time_points)\n\n# Create cause-specific event indicators for evaluation\nevent_prepay_test = (event_test == 1).astype('float32')\nevent_default_test = (event_test == 2).astype('float32')\n\nduration_test_np = np.array(duration_test).astype('float64')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-default",
   "metadata": {},
   "outputs": [],
   "source": "# === Compute C-index for both causes ===\n\n# For C-index, we use CIF as risk score (higher CIF = higher risk)\n# Risk at a specific time horizon\ndef get_risk_at_horizon(cif, time_points, tau):\n    \"\"\"Get CIF value at time horizon tau.\"\"\"\n    idx = np.searchsorted(time_points, tau)\n    idx = min(idx, len(time_points) - 1)\n    return cif[:, idx]\n\n# === PREPAYMENT Evaluation ===\nprint(\"=== PREPAYMENT C-index ===\")\nprint(\"-\" * 50)\n\n# Create sksurv structured arrays\ny_train_prepay_sk = Surv.from_arrays(\n    (train_df[event_col] == 1).values.astype(bool),\n    duration_train\n)\ny_test_prepay_sk = Surv.from_arrays(\n    event_prepay_test.astype(bool),\n    duration_test\n)\n\ncindex_prepay_ipcw = {}\nfor tau in TIME_HORIZONS:\n    try:\n        risk_prepay = get_risk_at_horizon(cif_prepay, time_points, tau)\n        c_tau = concordance_index_ipcw(\n            y_train_prepay_sk,\n            y_test_prepay_sk,\n            risk_prepay,\n            tau=tau\n        )\n        cindex_prepay_ipcw[tau] = c_tau[0]\n        print(f\"  tau = {tau:3d} months: C-index (IPCW) = {c_tau[0]:.4f}\")\n    except Exception as e:\n        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n\n# Overall C-index (using mean CIF as risk)\nrisk_prepay_overall = cif_prepay.mean(axis=1)\nc_index_prepay = concordance_index_censored(\n    event_prepay_test.astype(bool),\n    duration_test,\n    risk_prepay_overall\n)\nprint(f\"\\nOverall C-index (Harrell): {c_index_prepay[0]:.4f}\")\n\n# === DEFAULT Evaluation ===\nprint(\"\\n=== DEFAULT C-index ===\")\nprint(\"-\" * 50)\n\ny_train_default_sk = Surv.from_arrays(\n    (train_df[event_col] == 2).values.astype(bool),\n    duration_train\n)\ny_test_default_sk = Surv.from_arrays(\n    event_default_test.astype(bool),\n    duration_test\n)\n\ncindex_default_ipcw = {}\nfor tau in TIME_HORIZONS:\n    try:\n        risk_default = get_risk_at_horizon(cif_default, time_points, tau)\n        c_tau = concordance_index_ipcw(\n            y_train_default_sk,\n            y_test_default_sk,\n            risk_default,\n            tau=tau\n        )\n        cindex_default_ipcw[tau] = c_tau[0]\n        print(f\"  tau = {tau:3d} months: C-index (IPCW) = {c_tau[0]:.4f}\")\n    except Exception as e:\n        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n\n# Overall C-index\nrisk_default_overall = cif_default.mean(axis=1)\nc_index_default = concordance_index_censored(\n    event_default_test.astype(bool),\n    duration_test,\n    risk_default_overall\n)\nprint(f\"\\nOverall C-index (Harrell): {c_index_default[0]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-cindex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-dependent C-index comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use IPCW results for plotting\n",
    "horizons = sorted(set(cindex_prepay_ipcw.keys()) & set(cindex_default_ipcw.keys()))\n",
    "prepay_cindex = [cindex_prepay_ipcw[h] for h in horizons]\n",
    "default_cindex = [cindex_default_ipcw[h] for h in horizons]\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, prepay_cindex, width, label='Prepayment', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, default_cindex, width, label='Default', color='indianred', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, prepay_cindex):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "for bar, val in zip(bars2, default_cindex):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random (0.5)')\n",
    "\n",
    "ax.set_xlabel('Time Horizon (months)', fontsize=12)\n",
    "ax.set_ylabel('C-index (IPCW)', fontsize=12)\n",
    "ax.set_title('DeepHit: Time-Dependent Concordance Index by Event Type', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'tau = {h}' for h in horizons])\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_time_dependent_cindex.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Figure saved to: {FIGURES_DIR / 'deephit_time_dependent_cindex.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "survival-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Survival Curves\n",
    "\n",
    "Plot predicted survival curves for sample loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-survival",
   "metadata": {},
   "outputs": [],
   "source": "# Plot CIF curves for sample loans\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Sample 5 random loans\nnp.random.seed(42)\nsample_idx = np.random.choice(len(X_test), size=5, replace=False)\n\n# Prepayment CIF\nax = axes[0]\nfor i, idx in enumerate(sample_idx):\n    ax.plot(time_points, cif_prepay[idx], label=f'Loan {idx}', alpha=0.7)\nax.set_xlabel('Time (months)')\nax.set_ylabel('Cumulative Incidence')\nax.set_title('DeepHit: Prepayment CIF')\nax.legend(loc='lower right', fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\n# Default CIF\nax = axes[1]\nfor i, idx in enumerate(sample_idx):\n    ax.plot(time_points, cif_default[idx], label=f'Loan {idx}', alpha=0.7)\nax.set_xlabel('Time (months)')\nax.set_ylabel('Cumulative Incidence')\nax.set_title('DeepHit: Default CIF')\nax.legend(loc='lower right', fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\n# Overall Survival\nax = axes[2]\nfor i, idx in enumerate(sample_idx):\n    ax.plot(time_points, surv_np[idx], label=f'Loan {idx}', alpha=0.7)\nax.set_xlabel('Time (months)')\nax.set_ylabel('Survival Probability')\nax.set_title('DeepHit: Overall Survival S(t)')\nax.legend(loc='lower left', fontsize=8)\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / 'deephit_survival_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Also plot stacked CIF to show competing risks\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Average CIF across all test samples\nmean_cif_prepay = cif_prepay.mean(axis=0)\nmean_cif_default = cif_default.mean(axis=0)\nmean_survival = surv_np.mean(axis=0)\n\nax.fill_between(time_points, 0, mean_cif_prepay, alpha=0.7, label='Prepay', color='steelblue')\nax.fill_between(time_points, mean_cif_prepay, mean_cif_prepay + mean_cif_default, \n                alpha=0.7, label='Default', color='indianred')\nax.fill_between(time_points, mean_cif_prepay + mean_cif_default, 1, \n                alpha=0.3, label='Survival', color='gray')\n\nax.set_xlabel('Time (months)', fontsize=12)\nax.set_ylabel('Probability', fontsize=12)\nax.set_title('DeepHit: Average Stacked CIF (Test Set)', fontsize=14)\nax.legend(loc='center right')\nax.set_ylim(0, 1)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "importance-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance (Permutation)\n",
    "\n",
    "Since DeepHit is a neural network, we use permutation importance to assess feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": "# Compute permutation importance for joint model\nprint(\"=== Feature Importance (Permutation) ===\")\nprint(\"Computing permutation importance for each cause...\")\n\ndef permutation_importance_deephit_joint(model, X, duration, event, feature_names, \n                                         cause_idx, device, n_repeats=5):\n    \"\"\"\n    Compute permutation importance for DeepHit joint model.\n    \n    Args:\n        cause_idx: 0 for prepay, 1 for default\n    \"\"\"\n    model.eval()\n    event_code = cause_idx + 1  # Map cause index to event code\n    event_binary = (event == event_code).astype(bool)\n    \n    # Baseline score using CIF as risk\n    with torch.no_grad():\n        X_t = torch.tensor(X, dtype=torch.float32).to(device)\n        cif_baseline = model.predict_cif(X_t).cpu().numpy()[:, cause_idx, :]\n    risk_baseline = cif_baseline.mean(axis=1)\n    \n    baseline_cindex = concordance_index_censored(\n        event_binary, duration, risk_baseline\n    )[0]\n    \n    importances = []\n    importances_std = []\n    \n    for i, feat in enumerate(feature_names):\n        scores = []\n        for _ in range(n_repeats):\n            X_perm = X.copy()\n            np.random.shuffle(X_perm[:, i])\n            \n            with torch.no_grad():\n                X_perm_t = torch.tensor(X_perm, dtype=torch.float32).to(device)\n                cif_perm = model.predict_cif(X_perm_t).cpu().numpy()[:, cause_idx, :]\n            risk_perm = cif_perm.mean(axis=1)\n            \n            perm_cindex = concordance_index_censored(\n                event_binary, duration, risk_perm\n            )[0]\n            \n            scores.append(baseline_cindex - perm_cindex)\n        \n        importances.append(np.mean(scores))\n        importances_std.append(np.std(scores))\n        \n    return np.array(importances), np.array(importances_std)\n\n# Prepayment importance\nprint(\"\\nPrepayment cause...\")\nimp_prepay, imp_prepay_std = permutation_importance_deephit_joint(\n    model, X_test, duration_test, event_test, feature_cols, \n    cause_idx=0, device=DEVICE, n_repeats=5\n)\n\nimportance_prepay = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': imp_prepay,\n    'std': imp_prepay_std\n}).sort_values('importance', ascending=False)\n\n# Default importance\nprint(\"Default cause...\")\nimp_default, imp_default_std = permutation_importance_deephit_joint(\n    model, X_test, duration_test, event_test, feature_cols, \n    cause_idx=1, device=DEVICE, n_repeats=5\n)\n\nimportance_default = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': imp_default,\n    'std': imp_default_std\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Features - PREPAYMENT:\")\nprint(importance_prepay.head(10).to_string(index=False))\n\nprint(\"\\nTop 10 Features - DEFAULT:\")\nprint(importance_default.head(10).to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Prepayment\n",
    "ax = axes[0]\n",
    "top_n = 15\n",
    "plot_df = importance_prepay.head(top_n).iloc[::-1]\n",
    "ax.barh(plot_df['feature'], plot_df['importance'], xerr=plot_df['std'],\n",
    "        color='steelblue', alpha=0.7, capsize=3)\n",
    "ax.set_xlabel('Importance (decrease in C-index)')\n",
    "ax.set_title('DeepHit Permutation Importance: Prepayment', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Default\n",
    "ax = axes[1]\n",
    "plot_df = importance_default.head(top_n).iloc[::-1]\n",
    "ax.barh(plot_df['feature'], plot_df['importance'], xerr=plot_df['std'],\n",
    "        color='indianred', alpha=0.7, capsize=3)\n",
    "ax.set_xlabel('Importance (decrease in C-index)')\n",
    "ax.set_title('DeepHit Permutation Importance: Default', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": "# Save joint model (PyTorch native format)\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'in_features': in_features,\n    'num_time_bins': NUM_DURATIONS,\n    'num_causes': 2,\n    'shared_layers': 3,\n    'shared_nodes': 300,\n    'head_layers': 5,\n    'head_nodes': 200,\n    'dropout': DEEPHIT_PARAMS['dropout'],\n    'batch_norm': DEEPHIT_PARAMS['batch_norm'],\n}, MODELS_DIR / 'deephit_joint.pt')\n\n# Save scaler for inference\nwith open(MODELS_DIR / 'deephit_scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Save time bins\nnp.save(MODELS_DIR / 'deephit_time_bins.npy', time_bins)\n\n# Save feature importance\nimportance_prepay.to_csv(MODELS_DIR / 'deephit_importance_prepay.csv', index=False)\nimportance_default.to_csv(MODELS_DIR / 'deephit_importance_default.csv', index=False)\n\n# Save feature columns\nwith open(MODELS_DIR / 'deephit_feature_cols.pkl', 'wb') as f:\n    pickle.dump(feature_cols, f)\n\n# Save training history\nwith open(MODELS_DIR / 'deephit_history.pkl', 'wb') as f:\n    pickle.dump(history, f)\n\nprint(f\"Model saved to {MODELS_DIR}:\")\nprint(f\"  - deephit_joint.pt (joint competing risks model)\")\nprint(f\"  - deephit_scaler.pkl\")\nprint(f\"  - deephit_time_bins.npy\")\nprint(f\"  - deephit_importance_prepay.csv\")\nprint(f\"  - deephit_importance_default.csv\")\nprint(f\"  - deephit_feature_cols.pkl\")\nprint(f\"  - deephit_history.pkl\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"DEEPHIT (BLUMENSTOCK ARCHITECTURE) - SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nArchitecture:\")\nprint(f\"  Shared FFN: 3 layers x 300 nodes\")\nprint(f\"  Residual: shared_output + proj(input)\")\nprint(f\"  Prepay head: 5 layers x 200 nodes -> {NUM_DURATIONS} time bins\")\nprint(f\"  Default head: 5 layers x 200 nodes -> {NUM_DURATIONS} time bins\")\nprint(f\"  Output: Joint softmax over {2 * NUM_DURATIONS} (cause, time)\")\nprint(f\"  Total parameters: {n_params:,}\")\n\nprint(f\"\\nData:\")\nprint(f\"  Training loans: {len(train_df):,}\")\nprint(f\"  Validation loans: {len(val_df):,}\")\nprint(f\"  Test loans: {len(test_df):,}\")\n\nprint(f\"\\nFeatures: {len(feature_cols)}\")\nprint(f\"  Static: {len([f for f in STATIC_FEATURES if f in feature_cols or 'log_upb' in feature_cols])}\")\nprint(f\"  Behavioral: {len([f for f in BEHAVIORAL_FEATURES if f in feature_cols or 'bal_repaid_lag1' in feature_cols])}\")\nprint(f\"  Macro: {len([f for f in MACRO_FEATURES if f in feature_cols])}\")\n\nprint(f\"\\nTraining Parameters:\")\nprint(f\"  Batch size: {DEEPHIT_PARAMS['batch_size']}\")\nprint(f\"  Learning rate: {DEEPHIT_PARAMS['learning_rate']}\")\nprint(f\"  Alpha (ranking): {DEEPHIT_PARAMS['alpha']}\")\nprint(f\"  Sigma: {DEEPHIT_PARAMS['sigma']}\")\nprint(f\"  Dropout: {DEEPHIT_PARAMS['dropout']}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"MODEL PERFORMANCE (Test Set)\")\nprint(\"=\" * 70)\n\nprint(f\"\\nPREPAYMENT:\")\nprint(f\"  Overall C-index (Harrell): {c_index_prepay[0]:.4f}\")\nprint(f\"  Time-Dependent C-index (IPCW):\")\nfor tau, c in cindex_prepay_ipcw.items():\n    print(f\"    tau = {tau:3d} months: {c:.4f}\")\n\nprint(f\"\\nDEFAULT:\")\nprint(f\"  Overall C-index (Harrell): {c_index_default[0]:.4f}\")\nprint(f\"  Time-Dependent C-index (IPCW):\")\nfor tau, c in cindex_default_ipcw.items():\n    print(f\"    tau = {tau:3d} months: {c:.4f}\")\n\nprint(f\"\\nTop 3 Important Features:\")\nprint(f\"  Prepayment: {', '.join(importance_prepay['feature'].head(3).tolist())}\")\nprint(f\"  Default: {', '.join(importance_default['feature'].head(3).tolist())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Notebook 09**: Model Comparison\n",
    "\n",
    "Compare all models:\n",
    "- Cause-Specific Cox (notebook 05)\n",
    "- Fine-Gray (notebook 06)\n",
    "- Random Survival Forest (notebook 07)\n",
    "- DeepHit (this notebook)\n",
    "\n",
    "Key comparisons:\n",
    "- Time-dependent C-index at multiple horizons\n",
    "- Calibration assessment\n",
    "- Cumulative incidence predictions\n",
    "- Computational cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}