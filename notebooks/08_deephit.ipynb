{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# DeepHit for Competing Risks\n",
    "\n",
    "This notebook implements **DeepHit** (Lee et al., 2018) for competing risks analysis. DeepHit is a deep learning approach that directly models the probability mass function of survival times without assuming any particular form for the underlying stochastic process.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Model** | DeepHit with competing risks |\n",
    "| **Base** | pycox DeepHitSingle / DeepHit |\n",
    "| **Features** | Blumenstock et al. (2022) - 21 variables |\n",
    "| **Evaluation** | Time-dependent C-index at 24, 48, 72 months |\n",
    "\n",
    "## Key Advantages of DeepHit\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **No PH assumption** | Does not assume proportional hazards |\n",
    "| **Competing risks** | Native support for multiple event types |\n",
    "| **Flexible** | Can capture complex non-linear relationships |\n",
    "| **Direct modeling** | Models PMF directly, not hazard function |\n",
    "\n",
    "## References\n",
    "\n",
    "- Lee, C., Zame, W., Yoon, J., & van der Schaar, M. (2018). DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks. AAAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.models import DeepHitSingle\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "# Survival analysis (for comparison metrics)\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored, concordance_index_ipcw\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Time horizons for evaluation (matching previous notebooks)\n",
    "TIME_HORIZONS = [24, 48, 72]\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Time horizons for C-index evaluation: {TIME_HORIZONS} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "DATA_DIR = Path('../data/processed')\n",
    "FIGURES_DIR = Path('../reports/figures')\n",
    "MODELS_DIR = Path('../models')\n",
    "\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cross-validation folds (Blumenstock methodology)\n",
    "TRAIN_FOLDS = list(range(10))  # Folds 0-9 for training\n",
    "VAL_FOLDS = [9]                 # Use fold 9 for validation (early stopping)\n",
    "TEST_FOLD = 10                  # Fold 10 for testing\n",
    "\n",
    "# DeepHit hyperparameters\n",
    "DEEPHIT_PARAMS = {\n",
    "    'num_nodes': [64, 64],      # Hidden layer sizes\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.2,\n",
    "    'activation': torch.nn.ReLU,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'alpha': 0.2,               # Weight for ranking loss\n",
    "    'sigma': 0.1,               # Smoothing parameter for ranking loss\n",
    "}\n",
    "\n",
    "# Number of discrete time points\n",
    "NUM_DURATIONS = 100\n",
    "\n",
    "print(f\"Training folds: {[f for f in TRAIN_FOLDS if f not in VAL_FOLDS]}\")\n",
    "print(f\"Validation fold: {VAL_FOLDS}\")\n",
    "print(f\"Test fold: {TEST_FOLD}\")\n",
    "print(f\"\\nDeepHit parameters:\")\n",
    "for k, v in DEEPHIT_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Loan-Month Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan-month panel data\n",
    "print(\"Loading loan-month panel data...\")\n",
    "panel_df = pd.read_parquet(DATA_DIR / 'loan_month_panel.parquet')\n",
    "\n",
    "print(f\"Loaded {len(panel_df):,} loan-months\")\n",
    "print(f\"Unique loans: {panel_df['loan_sequence_number'].nunique():,}\")\n",
    "print(f\"Folds: {sorted(panel_df['fold'].unique())}\")\n",
    "print(f\"Vintages: {panel_df['vintage_year'].min()} - {panel_df['vintage_year'].max()}\")\n",
    "\n",
    "print(\"\\nEvent distribution (terminal observations):\")\n",
    "event_names = {0: 'Censored', 1: 'Prepay', 2: 'Default'}\n",
    "terminal_events = panel_df[panel_df['event'] == 1].groupby('event_code').size()\n",
    "for code, count in terminal_events.items():\n",
    "    print(f\"  {event_names.get(code, 'Other')} (k={code}): {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define Features (Blumenstock et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups (Blumenstock et al. 2022, Table 2)\n",
    "# Matching previous notebooks exactly\n",
    "\n",
    "# Static covariates (fixed at origination) - 5 variables\n",
    "STATIC_FEATURES = [\n",
    "    'int_rate',      # Initial interest rate\n",
    "    'orig_upb',      # Original unpaid balance\n",
    "    'fico_score',    # Initial FICO score\n",
    "    'dti_r',         # Initial debt-to-income ratio\n",
    "    'ltv_r',         # Initial loan-to-value ratio\n",
    "]\n",
    "\n",
    "# Behavioral covariates (time-varying) - 4 variables\n",
    "BEHAVIORAL_FEATURES = [\n",
    "    'bal_repaid',      # Current repaid balance in percent\n",
    "    't_act_12m',       # No. of times not being delinquent in last 12 months\n",
    "    't_del_30d_12m',   # No. of times being 30 days delinquent in last 12 months\n",
    "    't_del_60d_12m',   # No. of times being 60 days delinquent in last 12 months\n",
    "]\n",
    "\n",
    "# Macro covariates (time-varying) - 12 variables\n",
    "MACRO_FEATURES = [\n",
    "    'hpi_st_d_t_o',    # HPI difference between origination and today (state)\n",
    "    'ppi_c_FRMA',      # Current prepayment incentive\n",
    "    'TB10Y_d_t_o',     # Treasury rate difference\n",
    "    'FRMA30Y_d_t_o',   # 30Y FRM difference\n",
    "    'ppi_o_FRMA',      # Prepayment incentive at origination\n",
    "    'hpi_st_log12m',   # HPI 12-month log return (state)\n",
    "    'hpi_r_st_us',     # Ratio of state HPI to national HPI\n",
    "    'st_unemp_r12m',   # Unemployment 12-month log return (state)\n",
    "    'st_unemp_r3m',    # Unemployment 3-month log return (state)\n",
    "    'TB10Y_r12m',      # Treasury rate 12-month return\n",
    "    'T10Y3MM',         # Yield spread (10Y - 3M)\n",
    "    'T10Y3MM_r12m',    # Yield spread 12-month return\n",
    "]\n",
    "\n",
    "ALL_FEATURES = STATIC_FEATURES + BEHAVIORAL_FEATURES + MACRO_FEATURES\n",
    "\n",
    "# Filter to available features\n",
    "feature_cols = [f for f in ALL_FEATURES if f in panel_df.columns]\n",
    "missing_features = [f for f in ALL_FEATURES if f not in panel_df.columns]\n",
    "\n",
    "print(\"=== Feature Groups (Blumenstock et al. 2022) ===\")\n",
    "print(f\"Static features: {len([f for f in STATIC_FEATURES if f in feature_cols])}/5\")\n",
    "print(f\"Behavioral features: {len([f for f in BEHAVIORAL_FEATURES if f in feature_cols])}/4\")\n",
    "print(f\"Macro features: {len([f for f in MACRO_FEATURES if f in feature_cols])}/12\")\n",
    "print(f\"\\nTotal available: {len(feature_cols)}/21\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nMissing features ({len(missing_features)}):\")\n",
    "    for f in missing_features:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare Data for DeepHit\n",
    "\n",
    "DeepHit requires terminal observations (one per loan) with discretized time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DeepHit, we need terminal observations (one per loan)\n",
    "# Get the last observation for each loan\n",
    "print(\"=== Preparing Terminal Observations ===\")\n",
    "\n",
    "time_col = 'loan_age'\n",
    "event_col = 'event_code'\n",
    "\n",
    "# Sort and get last observation per loan\n",
    "panel_df = panel_df.sort_values(['loan_sequence_number', time_col])\n",
    "terminal_df = panel_df.groupby('loan_sequence_number').last().reset_index()\n",
    "\n",
    "print(f\"Terminal observations: {len(terminal_df):,} loans\")\n",
    "\n",
    "# Lag bal_repaid to avoid data leakage (matching previous notebooks)\n",
    "if 'bal_repaid' in feature_cols:\n",
    "    print(\"\\nLagging bal_repaid to avoid data leakage...\")\n",
    "    \n",
    "    def get_lagged_bal_repaid(group):\n",
    "        if len(group) >= 2:\n",
    "            return group['bal_repaid'].iloc[-2]\n",
    "        else:\n",
    "            return group['bal_repaid'].iloc[-1]\n",
    "    \n",
    "    bal_repaid_lag = panel_df.groupby('loan_sequence_number').apply(get_lagged_bal_repaid)\n",
    "    terminal_df['bal_repaid_lag1'] = terminal_df['loan_sequence_number'].map(bal_repaid_lag)\n",
    "    \n",
    "    feature_cols = [f if f != 'bal_repaid' else 'bal_repaid_lag1' for f in feature_cols]\n",
    "    print(\"  Created bal_repaid_lag1\")\n",
    "\n",
    "# Log transform UPB\n",
    "if 'orig_upb' in terminal_df.columns:\n",
    "    terminal_df['log_upb'] = np.log(terminal_df['orig_upb'])\n",
    "    feature_cols = [f if f != 'orig_upb' else 'log_upb' for f in feature_cols]\n",
    "    print(\"  Created log_upb\")\n",
    "\n",
    "# Drop rows with missing features\n",
    "n_before = len(terminal_df)\n",
    "terminal_df = terminal_df.dropna(subset=feature_cols)\n",
    "n_after = len(terminal_df)\n",
    "print(f\"\\nAfter dropping NaN: {n_after:,} loans (dropped {n_before - n_after:,})\")\n",
    "\n",
    "print(f\"\\nFeatures ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by folds (matching previous notebooks)\n",
    "print(\"=== Splitting Data by Folds ===\")\n",
    "\n",
    "# Training uses folds 0-8, validation uses fold 9, test uses fold 10\n",
    "train_folds_actual = [f for f in TRAIN_FOLDS if f not in VAL_FOLDS]\n",
    "\n",
    "train_df = terminal_df[terminal_df['fold'].isin(train_folds_actual)].copy()\n",
    "val_df = terminal_df[terminal_df['fold'].isin(VAL_FOLDS)].copy()\n",
    "test_df = terminal_df[terminal_df['fold'] == TEST_FOLD].copy()\n",
    "\n",
    "print(f\"Training set (folds {train_folds_actual}): {len(train_df):,} loans\")\n",
    "print(f\"Validation set (fold {VAL_FOLDS}): {len(val_df):,} loans\")\n",
    "print(f\"Test set (fold {TEST_FOLD}): {len(test_df):,} loans\")\n",
    "\n",
    "# Event distribution\n",
    "print(\"\\nTraining set event distribution:\")\n",
    "for code, name in event_names.items():\n",
    "    count = (train_df[event_col] == code).sum()\n",
    "    print(f\"  {name}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for neural networks)\n",
    "print(\"=== Standardizing Features ===\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df[feature_cols]).astype('float32')\n",
    "X_val = scaler.transform(val_df[feature_cols]).astype('float32')\n",
    "X_test = scaler.transform(test_df[feature_cols]).astype('float32')\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Durations\n",
    "duration_train = train_df[time_col].values\n",
    "duration_val = val_df[time_col].values\n",
    "duration_test = test_df[time_col].values\n",
    "\n",
    "print(f\"\\nDuration range: {duration_train.min()} - {duration_train.max()} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fit DeepHit Models\n",
    "\n",
    "We fit cause-specific DeepHit models for prepayment and default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discretize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize time for DeepHit\n",
    "# DeepHit models discrete time, so we need to create time bins\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "\n",
    "print(\"=== Discretizing Time ===\")\n",
    "\n",
    "# Create discrete time cuts based on training data\n",
    "cuts = np.linspace(0, duration_train.max(), NUM_DURATIONS + 1)\n",
    "labtrans = LabTransDiscreteTime(cuts)\n",
    "\n",
    "print(f\"Number of time intervals: {NUM_DURATIONS}\")\n",
    "print(f\"Time range: {cuts[0]:.1f} - {cuts[-1]:.1f} months\")\n",
    "print(f\"Interval width: ~{(cuts[-1] - cuts[0]) / NUM_DURATIONS:.2f} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-prepay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DeepHit for PREPAYMENT (event=1)\n",
    "print(\"=== Fitting DeepHit for PREPAYMENT ===\")\n",
    "\n",
    "# Create cause-specific event indicator\n",
    "event_prepay_train = (train_df[event_col] == 1).astype('float32').values\n",
    "event_prepay_val = (val_df[event_col] == 1).astype('float32').values\n",
    "event_prepay_test = (test_df[event_col] == 1).astype('float32').values\n",
    "\n",
    "# Transform labels for DeepHit\n",
    "y_train_prepay = labtrans.fit_transform(duration_train, event_prepay_train)\n",
    "y_val_prepay = labtrans.transform(duration_val, event_prepay_val)\n",
    "\n",
    "# Create network\n",
    "in_features = X_train.shape[1]\n",
    "out_features = labtrans.out_features\n",
    "\n",
    "net_prepay = tt.practical.MLPVanilla(\n",
    "    in_features,\n",
    "    DEEPHIT_PARAMS['num_nodes'],\n",
    "    out_features,\n",
    "    DEEPHIT_PARAMS['batch_norm'],\n",
    "    DEEPHIT_PARAMS['dropout'],\n",
    "    activation=DEEPHIT_PARAMS['activation']\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model_prepay = DeepHitSingle(\n",
    "    net_prepay,\n",
    "    tt.optim.Adam,\n",
    "    alpha=DEEPHIT_PARAMS['alpha'],\n",
    "    sigma=DEEPHIT_PARAMS['sigma'],\n",
    "    duration_index=labtrans.cuts\n",
    ")\n",
    "model_prepay.optimizer.set_lr(DEEPHIT_PARAMS['learning_rate'])\n",
    "\n",
    "print(f\"Network architecture: {in_features} -> {DEEPHIT_PARAMS['num_nodes']} -> {out_features}\")\n",
    "print(f\"Training events: {event_prepay_train.sum():,.0f}\")\n",
    "\n",
    "# Fit with early stopping\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=10)]\n",
    "log_prepay = model_prepay.fit(\n",
    "    X_train, y_train_prepay,\n",
    "    DEEPHIT_PARAMS['batch_size'],\n",
    "    DEEPHIT_PARAMS['epochs'],\n",
    "    callbacks,\n",
    "    val_data=(X_val, y_val_prepay),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete. Best epoch: {log_prepay.to_pandas()['val_loss'].idxmin()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DeepHit for DEFAULT (event=2)\n",
    "print(\"=== Fitting DeepHit for DEFAULT ===\")\n",
    "\n",
    "# Create cause-specific event indicator\n",
    "event_default_train = (train_df[event_col] == 2).astype('float32').values\n",
    "event_default_val = (val_df[event_col] == 2).astype('float32').values\n",
    "event_default_test = (test_df[event_col] == 2).astype('float32').values\n",
    "\n",
    "# Transform labels for DeepHit\n",
    "y_train_default = labtrans.transform(duration_train, event_default_train)\n",
    "y_val_default = labtrans.transform(duration_val, event_default_val)\n",
    "\n",
    "# Create network\n",
    "net_default = tt.practical.MLPVanilla(\n",
    "    in_features,\n",
    "    DEEPHIT_PARAMS['num_nodes'],\n",
    "    out_features,\n",
    "    DEEPHIT_PARAMS['batch_norm'],\n",
    "    DEEPHIT_PARAMS['dropout'],\n",
    "    activation=DEEPHIT_PARAMS['activation']\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model_default = DeepHitSingle(\n",
    "    net_default,\n",
    "    tt.optim.Adam,\n",
    "    alpha=DEEPHIT_PARAMS['alpha'],\n",
    "    sigma=DEEPHIT_PARAMS['sigma'],\n",
    "    duration_index=labtrans.cuts\n",
    ")\n",
    "model_default.optimizer.set_lr(DEEPHIT_PARAMS['learning_rate'])\n",
    "\n",
    "print(f\"Training events: {event_default_train.sum():,.0f}\")\n",
    "\n",
    "# Fit with early stopping\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=10)]\n",
    "log_default = model_default.fit(\n",
    "    X_train, y_train_default,\n",
    "    DEEPHIT_PARAMS['batch_size'],\n",
    "    DEEPHIT_PARAMS['epochs'],\n",
    "    callbacks,\n",
    "    val_data=(X_val, y_val_default),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete. Best epoch: {log_default.to_pandas()['val_loss'].idxmin()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Prepayment\n",
    "ax = axes[0]\n",
    "log_df = log_prepay.to_pandas()\n",
    "ax.plot(log_df['train_loss'], label='Train')\n",
    "ax.plot(log_df['val_loss'], label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DeepHit Prepayment: Training Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Default\n",
    "ax = axes[1]\n",
    "log_df = log_default.to_pandas()\n",
    "ax.plot(log_df['train_loss'], label='Train')\n",
    "ax.plot(log_df['val_loss'], label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DeepHit Default: Training Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Evaluate using time-dependent C-index at 24, 48, and 72 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-prepay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PREPAYMENT model\n",
    "print(\"=== DeepHit Prepayment Model Evaluation ===\")\n",
    "\n",
    "# Get survival predictions\n",
    "surv_prepay = model_prepay.predict_surv_df(X_test)\n",
    "\n",
    "# Create EvalSurv object for pycox metrics\n",
    "ev_prepay = EvalSurv(\n",
    "    surv_prepay,\n",
    "    duration_test,\n",
    "    event_prepay_test,\n",
    "    censor_surv='km'\n",
    ")\n",
    "\n",
    "# Time-dependent C-index using pycox\n",
    "print(\"\\nTime-Dependent C-index for PREPAYMENT:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cindex_prepay_results = {}\n",
    "for tau in TIME_HORIZONS:\n",
    "    try:\n",
    "        c_tau = ev_prepay.concordance_td(method='antolini')\n",
    "        cindex_prepay_results[tau] = c_tau\n",
    "        print(f\"  tau = {tau:3d} months: C-index = {c_tau:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n",
    "\n",
    "# Also compute using scikit-survival for comparison\n",
    "print(\"\\nUsing scikit-survival IPCW:\")\n",
    "\n",
    "# Get risk scores (negative mean survival = higher risk)\n",
    "risk_prepay_test = -surv_prepay.mean(axis=0).values\n",
    "\n",
    "# Create structured arrays for sksurv\n",
    "y_train_prepay_sk = Surv.from_arrays(event_prepay_train.astype(bool), duration_train)\n",
    "y_test_prepay_sk = Surv.from_arrays(event_prepay_test.astype(bool), duration_test)\n",
    "\n",
    "cindex_prepay_ipcw = {}\n",
    "for tau in TIME_HORIZONS:\n",
    "    try:\n",
    "        c_tau = concordance_index_ipcw(\n",
    "            y_train_prepay_sk,\n",
    "            y_test_prepay_sk,\n",
    "            risk_prepay_test,\n",
    "            tau=tau\n",
    "        )\n",
    "        cindex_prepay_ipcw[tau] = c_tau[0]\n",
    "        print(f\"  tau = {tau:3d} months: C-index (IPCW) = {c_tau[0]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n",
    "\n",
    "# Overall C-index (Harrell's)\n",
    "c_index_prepay = concordance_index_censored(\n",
    "    event_prepay_test.astype(bool),\n",
    "    duration_test,\n",
    "    risk_prepay_test\n",
    ")\n",
    "print(f\"\\nOverall C-index (Harrell): {c_index_prepay[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DEFAULT model\n",
    "print(\"=== DeepHit Default Model Evaluation ===\")\n",
    "\n",
    "# Get survival predictions\n",
    "surv_default = model_default.predict_surv_df(X_test)\n",
    "\n",
    "# Create EvalSurv object\n",
    "ev_default = EvalSurv(\n",
    "    surv_default,\n",
    "    duration_test,\n",
    "    event_default_test,\n",
    "    censor_surv='km'\n",
    ")\n",
    "\n",
    "# Time-dependent C-index\n",
    "print(\"\\nTime-Dependent C-index for DEFAULT:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cindex_default_results = {}\n",
    "for tau in TIME_HORIZONS:\n",
    "    try:\n",
    "        c_tau = ev_default.concordance_td(method='antolini')\n",
    "        cindex_default_results[tau] = c_tau\n",
    "        print(f\"  tau = {tau:3d} months: C-index = {c_tau:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n",
    "\n",
    "# Using scikit-survival IPCW\n",
    "print(\"\\nUsing scikit-survival IPCW:\")\n",
    "\n",
    "risk_default_test = -surv_default.mean(axis=0).values\n",
    "\n",
    "y_train_default_sk = Surv.from_arrays(event_default_train.astype(bool), duration_train)\n",
    "y_test_default_sk = Surv.from_arrays(event_default_test.astype(bool), duration_test)\n",
    "\n",
    "cindex_default_ipcw = {}\n",
    "for tau in TIME_HORIZONS:\n",
    "    try:\n",
    "        c_tau = concordance_index_ipcw(\n",
    "            y_train_default_sk,\n",
    "            y_test_default_sk,\n",
    "            risk_default_test,\n",
    "            tau=tau\n",
    "        )\n",
    "        cindex_default_ipcw[tau] = c_tau[0]\n",
    "        print(f\"  tau = {tau:3d} months: C-index (IPCW) = {c_tau[0]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  tau = {tau:3d} months: Error - {str(e)[:50]}\")\n",
    "\n",
    "# Overall C-index\n",
    "c_index_default = concordance_index_censored(\n",
    "    event_default_test.astype(bool),\n",
    "    duration_test,\n",
    "    risk_default_test\n",
    ")\n",
    "print(f\"\\nOverall C-index (Harrell): {c_index_default[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-cindex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-dependent C-index comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use IPCW results for plotting\n",
    "horizons = sorted(set(cindex_prepay_ipcw.keys()) & set(cindex_default_ipcw.keys()))\n",
    "prepay_cindex = [cindex_prepay_ipcw[h] for h in horizons]\n",
    "default_cindex = [cindex_default_ipcw[h] for h in horizons]\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, prepay_cindex, width, label='Prepayment', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, default_cindex, width, label='Default', color='indianred', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, prepay_cindex):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "for bar, val in zip(bars2, default_cindex):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random (0.5)')\n",
    "\n",
    "ax.set_xlabel('Time Horizon (months)', fontsize=12)\n",
    "ax.set_ylabel('C-index (IPCW)', fontsize=12)\n",
    "ax.set_title('DeepHit: Time-Dependent Concordance Index by Event Type', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'tau = {h}' for h in horizons])\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_time_dependent_cindex.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Figure saved to: {FIGURES_DIR / 'deephit_time_dependent_cindex.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "survival-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Survival Curves\n",
    "\n",
    "Plot predicted survival curves for sample loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival curves for sample loans\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sample 5 random loans\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(len(X_test), size=5, replace=False)\n",
    "\n",
    "# Prepayment survival\n",
    "ax = axes[0]\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    surv = surv_prepay.iloc[:, idx]\n",
    "    ax.plot(surv.index, surv.values, label=f'Loan {idx}', alpha=0.7)\n",
    "ax.set_xlabel('Time (months)')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_title('DeepHit: Prepayment Survival Curves')\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Default survival\n",
    "ax = axes[1]\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    surv = surv_default.iloc[:, idx]\n",
    "    ax.plot(surv.index, surv.values, label=f'Loan {idx}', alpha=0.7)\n",
    "ax.set_xlabel('Time (months)')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_title('DeepHit: Default Survival Curves')\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_survival_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance (Permutation)\n",
    "\n",
    "Since DeepHit is a neural network, we use permutation importance to assess feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance\n",
    "print(\"=== Feature Importance (Permutation) ===\")\n",
    "print(\"Computing permutation importance...\")\n",
    "\n",
    "def permutation_importance_deephit(model, X, duration, event, feature_names, n_repeats=10):\n",
    "    \"\"\"Compute permutation importance for DeepHit model.\"\"\"\n",
    "    # Baseline score (negative mean survival as risk score)\n",
    "    surv_baseline = model.predict_surv_df(X)\n",
    "    risk_baseline = -surv_baseline.mean(axis=0).values\n",
    "    \n",
    "    y_sk = Surv.from_arrays(event.astype(bool), duration)\n",
    "    baseline_cindex = concordance_index_censored(\n",
    "        event.astype(bool), duration, risk_baseline\n",
    "    )[0]\n",
    "    \n",
    "    importances = []\n",
    "    importances_std = []\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_perm = X.copy()\n",
    "            np.random.shuffle(X_perm[:, i])\n",
    "            \n",
    "            surv_perm = model.predict_surv_df(X_perm)\n",
    "            risk_perm = -surv_perm.mean(axis=0).values\n",
    "            \n",
    "            perm_cindex = concordance_index_censored(\n",
    "                event.astype(bool), duration, risk_perm\n",
    "            )[0]\n",
    "            \n",
    "            scores.append(baseline_cindex - perm_cindex)\n",
    "        \n",
    "        importances.append(np.mean(scores))\n",
    "        importances_std.append(np.std(scores))\n",
    "        \n",
    "    return np.array(importances), np.array(importances_std)\n",
    "\n",
    "# Prepayment model\n",
    "print(\"\\nPrepayment model...\")\n",
    "imp_prepay, imp_prepay_std = permutation_importance_deephit(\n",
    "    model_prepay, X_test, duration_test, event_prepay_test, feature_cols, n_repeats=5\n",
    ")\n",
    "\n",
    "importance_prepay = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': imp_prepay,\n",
    "    'std': imp_prepay_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Default model\n",
    "print(\"Default model...\")\n",
    "imp_default, imp_default_std = permutation_importance_deephit(\n",
    "    model_default, X_test, duration_test, event_default_test, feature_cols, n_repeats=5\n",
    ")\n",
    "\n",
    "importance_default = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': imp_default,\n",
    "    'std': imp_default_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features - PREPAYMENT:\")\n",
    "print(importance_prepay.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Features - DEFAULT:\")\n",
    "print(importance_default.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Prepayment\n",
    "ax = axes[0]\n",
    "top_n = 15\n",
    "plot_df = importance_prepay.head(top_n).iloc[::-1]\n",
    "ax.barh(plot_df['feature'], plot_df['importance'], xerr=plot_df['std'],\n",
    "        color='steelblue', alpha=0.7, capsize=3)\n",
    "ax.set_xlabel('Importance (decrease in C-index)')\n",
    "ax.set_title('DeepHit Permutation Importance: Prepayment', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Default\n",
    "ax = axes[1]\n",
    "plot_df = importance_default.head(top_n).iloc[::-1]\n",
    "ax.barh(plot_df['feature'], plot_df['importance'], xerr=plot_df['std'],\n",
    "        color='indianred', alpha=0.7, capsize=3)\n",
    "ax.set_xlabel('Importance (decrease in C-index)')\n",
    "ax.set_title('DeepHit Permutation Importance: Default', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'deephit_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "model_prepay.save_net(MODELS_DIR / 'deephit_prepay_net.pt')\n",
    "model_default.save_net(MODELS_DIR / 'deephit_default_net.pt')\n",
    "\n",
    "# Save scaler and labtrans for inference\n",
    "with open(MODELS_DIR / 'deephit_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(MODELS_DIR / 'deephit_labtrans.pkl', 'wb') as f:\n",
    "    pickle.dump(labtrans, f)\n",
    "\n",
    "# Save feature importance\n",
    "importance_prepay.to_csv(MODELS_DIR / 'deephit_importance_prepay.csv', index=False)\n",
    "importance_default.to_csv(MODELS_DIR / 'deephit_importance_default.csv', index=False)\n",
    "\n",
    "# Save feature columns\n",
    "with open(MODELS_DIR / 'deephit_feature_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "\n",
    "print(f\"Models saved to {MODELS_DIR}:\")\n",
    "print(f\"  - deephit_prepay_net.pt\")\n",
    "print(f\"  - deephit_default_net.pt\")\n",
    "print(f\"  - deephit_scaler.pkl\")\n",
    "print(f\"  - deephit_labtrans.pkl\")\n",
    "print(f\"  - deephit_importance_prepay.csv\")\n",
    "print(f\"  - deephit_importance_default.csv\")\n",
    "print(f\"  - deephit_feature_cols.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEEPHIT - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Training loans: {len(train_df):,}\")\n",
    "print(f\"  Validation loans: {len(val_df):,}\")\n",
    "print(f\"  Test loans: {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"  Static: {len([f for f in STATIC_FEATURES if f in feature_cols or 'log_upb' in feature_cols])}\")\n",
    "print(f\"  Behavioral: {len([f for f in BEHAVIORAL_FEATURES if f in feature_cols or 'bal_repaid_lag1' in feature_cols])}\")\n",
    "print(f\"  Macro: {len([f for f in MACRO_FEATURES if f in feature_cols])}\")\n",
    "\n",
    "print(f\"\\nDeepHit Parameters:\")\n",
    "for param, value in DEEPHIT_PARAMS.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MODEL PERFORMANCE (Test Set)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nPREPAYMENT MODEL:\")\n",
    "print(f\"  Training events: {event_prepay_train.sum():,.0f}\")\n",
    "print(f\"  Overall C-index (Harrell): {c_index_prepay[0]:.4f}\")\n",
    "print(f\"  Time-Dependent C-index (IPCW):\")\n",
    "for tau, c in cindex_prepay_ipcw.items():\n",
    "    print(f\"    tau = {tau:3d} months: {c:.4f}\")\n",
    "\n",
    "print(f\"\\nDEFAULT MODEL:\")\n",
    "print(f\"  Training events: {event_default_train.sum():,.0f}\")\n",
    "print(f\"  Overall C-index (Harrell): {c_index_default[0]:.4f}\")\n",
    "print(f\"  Time-Dependent C-index (IPCW):\")\n",
    "for tau, c in cindex_default_ipcw.items():\n",
    "    print(f\"    tau = {tau:3d} months: {c:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 3 Important Features:\")\n",
    "print(f\"  Prepayment: {', '.join(importance_prepay['feature'].head(3).tolist())}\")\n",
    "print(f\"  Default: {', '.join(importance_default['feature'].head(3).tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Notebook 09**: Model Comparison\n",
    "\n",
    "Compare all models:\n",
    "- Cause-Specific Cox (notebook 05)\n",
    "- Fine-Gray (notebook 06)\n",
    "- Random Survival Forest (notebook 07)\n",
    "- DeepHit (this notebook)\n",
    "\n",
    "Key comparisons:\n",
    "- Time-dependent C-index at multiple horizons\n",
    "- Calibration assessment\n",
    "- Cumulative incidence predictions\n",
    "- Computational cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
