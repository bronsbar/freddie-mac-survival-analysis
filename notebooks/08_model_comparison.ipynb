{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Model Comparison: Blumenstock et al. (2022) Replication\n\nThis notebook replicates the experiments from:\n\n> Blumenstock, G., Lessmann, S., & Seow, H-V. (2022). Deep learning for survival and competing risk modelling. *Journal of the Operational Research Society*, 73(1), 26-38.\n\n## Models\n1. **CSC**: Cause-Specific Cox\n2. **FGR**: Fine-Gray Model\n3. **RSF**: Random Survival Forest\n4. **DeepHit**: Deep Learning approach (Lee et al., 2018)\n\n## Experiments (Dataset 2: 2010-2025)\n- **Exp 4.1**: Loan-level variables only (9 features)\n- **Exp 4.2**: Macroeconomic variables only (13 features)\n- **Exp 4.3**: All variables (22 features)\n\n## Evaluation\n- Time-dependent concordance index at 24, 48, 72 months\n- Separate evaluation for prepayment (k=1) and default (k=2)\n- Results averaged across cross-validation folds"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Models\nfrom lifelines import CoxPHFitter\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Competing risks module\nimport sys\nsys.path.insert(0, '..')\nfrom src.competing_risks import (\n    CompetingRisksRSF,\n    CompetingRisksDeepHit,\n    time_dependent_concordance_index,\n    evaluate_all_events,\n    format_results_table,\n    plot_concordance_comparison,\n    EVAL_TIMES,\n)\n\n# Check if PyTorch/pycox available for DeepHit\ntry:\n    import torch\n    import pycox\n    DEEPHIT_AVAILABLE = True\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"pycox available: {DEEPHIT_AVAILABLE}\")\nexcept ImportError:\n    DEEPHIT_AVAILABLE = False\n    print(\"DeepHit not available (install pycox: pip install pycox torch torchtuples)\")\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\nprint(\"\\nImports complete.\")\nprint(f\"Evaluation times: {EVAL_TIMES} months\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/processed')\n",
    "\n",
    "# Load the Blumenstock-style dataset\n",
    "data_path = DATA_DIR / 'blumenstock_dataset2.parquet'\n",
    "if data_path.exists():\n",
    "    df = pd.read_parquet(data_path)\n",
    "    print(f\"Loaded {len(df):,} observations\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please run 03_data_preparation_blumenstock.ipynb first.\")\n",
    "    # Fall back to standard survival data\n",
    "    df = pd.read_parquet(DATA_DIR / 'survival_data.parquet')\n",
    "    print(f\"Loaded fallback data: {len(df):,} observations\")\n",
    "\n",
    "# Load variable config\n",
    "var_config_path = DATA_DIR / 'blumenstock_variables.json'\n",
    "if var_config_path.exists():\n",
    "    with open(var_config_path) as f:\n",
    "        var_config = json.load(f)\n",
    "    LOAN_LEVEL_VARS = var_config['loan_level_vars']\n",
    "    MACRO_VARS = var_config['macro_vars']\n",
    "    ALL_VARS = var_config['all_vars']\n",
    "else:\n",
    "    # Default variables\n",
    "    LOAN_LEVEL_VARS = ['int_rate', 'orig_upb', 'fico_score', 'dti_r', 'ltv_r']\n",
    "    MACRO_VARS = []\n",
    "    ALL_VARS = LOAN_LEVEL_VARS\n",
    "\n",
    "print(f\"\\nLoan-level variables: {len(LOAN_LEVEL_VARS)}\")\n",
    "print(f\"Macro variables: {len(MACRO_VARS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "print(\"=== Data Summary ===\")\n",
    "print(f\"Observations: {len(df):,}\")\n",
    "print(f\"Folds: {df['fold'].nunique() if 'fold' in df.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nEvent distribution:\")\n",
    "event_counts = df['event_code'].value_counts().sort_index()\n",
    "for code, count in event_counts.items():\n",
    "    name = {0: 'Censored', 1: 'Prepay', 2: 'Default'}.get(code, 'Other')\n",
    "    print(f\"  {name} (k={code}): {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDuration statistics:\")\n",
    "print(df['duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup Experiments\n",
    "\n",
    "Define variable sets for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to available variables\n",
    "available_cols = df.columns.tolist()\n",
    "\n",
    "LOAN_VARS_AVAIL = [v for v in LOAN_LEVEL_VARS if v in available_cols]\n",
    "MACRO_VARS_AVAIL = [v for v in MACRO_VARS if v in available_cols]\n",
    "ALL_VARS_AVAIL = [v for v in ALL_VARS if v in available_cols]\n",
    "\n",
    "# Experiment configurations\n",
    "EXPERIMENTS = {\n",
    "    'Exp 4.1': {\n",
    "        'name': 'Loan-level only',\n",
    "        'features': LOAN_VARS_AVAIL,\n",
    "    },\n",
    "    'Exp 4.2': {\n",
    "        'name': 'Macro only',\n",
    "        'features': MACRO_VARS_AVAIL,\n",
    "    },\n",
    "    'Exp 4.3': {\n",
    "        'name': 'All variables',\n",
    "        'features': ALL_VARS_AVAIL,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"=== Experiment Setup ===\")\n",
    "for exp_name, config in EXPERIMENTS.items():\n",
    "    print(f\"\\n{exp_name}: {config['name']}\")\n",
    "    print(f\"  Features ({len(config['features'])}): {config['features'][:5]}...\" \n",
    "          if len(config['features']) > 5 else f\"  Features ({len(config['features'])}): {config['features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Model Classes\n",
    "\n",
    "Wrapper classes for consistent interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseSpecificCoxModel:\n",
    "    \"\"\"Cause-Specific Cox model for competing risks.\"\"\"\n",
    "    \n",
    "    def __init__(self, penalizer=0.01):\n",
    "        self.penalizer = penalizer\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_cols = []\n",
    "        \n",
    "    def fit(self, X, duration, event_code, feature_cols):\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit model for each event\n",
    "        for event in [1, 2]:  # Prepay, Default\n",
    "            # Create cause-specific event indicator\n",
    "            event_indicator = (event_code == event).astype(int)\n",
    "            \n",
    "            # Create DataFrame for lifelines\n",
    "            train_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "            train_df['duration'] = duration\n",
    "            train_df['event'] = event_indicator\n",
    "            \n",
    "            # Fit Cox model\n",
    "            cph = CoxPHFitter(penalizer=self.penalizer)\n",
    "            cph.fit(train_df, duration_col='duration', event_col='event')\n",
    "            self.models[event] = cph\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_risk(self, X, event):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_df = pd.DataFrame(X_scaled, columns=self.feature_cols)\n",
    "        return self.models[event].predict_partial_hazard(X_df).values.flatten()\n",
    "\n",
    "\n",
    "class FineGrayModel:\n",
    "    \"\"\"Discrete-time Fine-Gray model using logistic regression.\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0):\n",
    "        self.C = C\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, duration, event_code, feature_cols):\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit model for each event\n",
    "        for event in [1, 2]:  # Prepay, Default\n",
    "            # Fine-Gray: primary event = 1, competing events stay in risk set with y=0\n",
    "            y = (event_code == event).astype(int)\n",
    "            \n",
    "            # Fit logistic regression\n",
    "            model = LogisticRegression(C=self.C, penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "            model.fit(X_scaled, y)\n",
    "            self.models[event] = model\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_risk(self, X, event):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.models[event].predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Model classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "Following paper's cross-validation design with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment-func",
   "metadata": {},
   "outputs": [],
   "source": "def run_single_experiment(train_df, test_df, feature_cols, exp_name, val_df=None):\n    \"\"\"\n    Run experiment with all four models (CSC, FGR, RSF, DeepHit).\n    \n    Returns results DataFrame with C-index at 24, 48, 72 months.\n    \"\"\"\n    results = []\n    \n    # Prepare data\n    X_train = train_df[feature_cols].values\n    X_test = test_df[feature_cols].values\n    \n    duration_train = train_df['duration'].values\n    duration_test = test_df['duration'].values\n    \n    event_train = train_df['event_code'].values\n    event_test = test_df['event_code'].values\n    \n    # === 1. Cause-Specific Cox ===\n    print(\"  Training CSC...\")\n    csc = CauseSpecificCoxModel(penalizer=0.01)\n    csc.fit(X_train, duration_train, event_train, feature_cols)\n    \n    risk_prepay_csc = csc.predict_risk(X_test, event=1)\n    risk_default_csc = csc.predict_risk(X_test, event=2)\n    \n    csc_results = evaluate_all_events(\n        duration_test, event_test, risk_prepay_csc, risk_default_csc, EVAL_TIMES\n    )\n    csc_results['Model'] = 'CSC'\n    csc_results['Experiment'] = exp_name\n    results.append(csc_results)\n    \n    # === 2. Fine-Gray ===\n    print(\"  Training FGR...\")\n    fgr = FineGrayModel(C=1.0)\n    fgr.fit(X_train, duration_train, event_train, feature_cols)\n    \n    risk_prepay_fgr = fgr.predict_risk(X_test, event=1)\n    risk_default_fgr = fgr.predict_risk(X_test, event=2)\n    \n    fgr_results = evaluate_all_events(\n        duration_test, event_test, risk_prepay_fgr, risk_default_fgr, EVAL_TIMES\n    )\n    fgr_results['Model'] = 'FGR'\n    fgr_results['Experiment'] = exp_name\n    results.append(fgr_results)\n    \n    # === 3. Random Survival Forest ===\n    print(\"  Training RSF...\")\n    rsf = CompetingRisksRSF(\n        n_estimators=100,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        n_jobs=-1,\n        random_state=42\n    )\n    rsf.fit(X_train, duration_train, event_train, event_types=[1, 2])\n    \n    risk_prepay_rsf = rsf.predict_risk(X_test, event=1)\n    risk_default_rsf = rsf.predict_risk(X_test, event=2)\n    \n    rsf_results = evaluate_all_events(\n        duration_test, event_test, risk_prepay_rsf, risk_default_rsf, EVAL_TIMES\n    )\n    rsf_results['Model'] = 'RSF'\n    rsf_results['Experiment'] = exp_name\n    results.append(rsf_results)\n    \n    # === 4. DeepHit ===\n    if DEEPHIT_AVAILABLE:\n        print(\"  Training DeepHit...\")\n        \n        # Prepare validation data\n        if val_df is not None:\n            val_data = (\n                val_df[feature_cols],\n                val_df['duration'].values,\n                val_df['event_code'].values,\n            )\n        else:\n            # Use a portion of training data for validation\n            val_data = None\n        \n        deephit = CompetingRisksDeepHit(\n            num_durations=100,\n            num_nodes_shared=[64, 64],\n            num_nodes_indiv=[32],\n            batch_norm=True,\n            dropout=0.1,\n            alpha=0.2,\n            sigma=0.1,\n            lr=0.01,\n            weight_decay=0.01,\n            batch_size=256,\n            epochs=100,  # Reduced for speed\n            patience=5,\n            verbose=False,\n            random_state=42,\n        )\n        \n        try:\n            deephit.fit(\n                train_df[feature_cols],\n                duration_train,\n                event_train,\n                event_types=[1, 2],\n                val_data=val_data,\n            )\n            \n            risk_prepay_dh = deephit.predict_risk(test_df[feature_cols], event=1)\n            risk_default_dh = deephit.predict_risk(test_df[feature_cols], event=2)\n            \n            dh_results = evaluate_all_events(\n                duration_test, event_test, risk_prepay_dh, risk_default_dh, EVAL_TIMES\n            )\n            dh_results['Model'] = 'DeepHit'\n            dh_results['Experiment'] = exp_name\n            results.append(dh_results)\n        except Exception as e:\n            print(f\"    DeepHit failed: {e}\")\n    else:\n        print(\"  Skipping DeepHit (not available)\")\n    \n    return pd.concat(results, ignore_index=True)\n\n\nprint(\"Experiment function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "all_results = []\n",
    "\n",
    "# Use fold-based cross-validation if available\n",
    "if 'fold' in df.columns:\n",
    "    n_folds = df['fold'].nunique()\n",
    "    # Use first 10 folds for CV, last fold reserved for tuning\n",
    "    cv_folds = list(range(min(10, n_folds)))\n",
    "else:\n",
    "    # Simple train/test split\n",
    "    cv_folds = [0]\n",
    "    df['fold'] = 0\n",
    "\n",
    "for exp_name, config in EXPERIMENTS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {exp_name}: {config['name']}\")\n",
    "    print(f\"Features: {len(config['features'])}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    feature_cols = config['features']\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"  No features available, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Filter to complete cases for this feature set\n",
    "    df_exp = df.dropna(subset=feature_cols + ['duration', 'event_code'])\n",
    "    print(f\"Complete cases: {len(df_exp):,}\")\n",
    "    \n",
    "    if len(df_exp) < 1000:\n",
    "        print(\"  Insufficient data, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Cross-validation\n",
    "    for fold in cv_folds[:3]:  # Limit folds for speed\n",
    "        print(f\"\\nFold {fold + 1}/{len(cv_folds[:3])}\")\n",
    "        \n",
    "        # Split by fold\n",
    "        test_df = df_exp[df_exp['fold'] == fold]\n",
    "        train_df = df_exp[df_exp['fold'] != fold]\n",
    "        \n",
    "        if len(test_df) < 100 or len(train_df) < 500:\n",
    "            print(\"  Insufficient data in fold, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Train: {len(train_df):,}, Test: {len(test_df):,}\")\n",
    "        \n",
    "        # Run experiment\n",
    "        fold_results = run_single_experiment(train_df, test_df, feature_cols, exp_name)\n",
    "        fold_results['Fold'] = fold\n",
    "        all_results.append(fold_results)\n",
    "\n",
    "# Combine all results\n",
    "if all_results:\n",
    "    results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\n\\nTotal results: {len(results_df)} rows\")\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Format results following paper's Table 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-results",
   "metadata": {},
   "outputs": [],
   "source": "if 'results_df' in dir() and len(results_df) > 0:\n    # Aggregate across folds\n    agg_results = results_df.groupby(['Experiment', 'Model', 'Metric']).agg({\n        'Prepay (k=1)': 'mean',\n        'Default (k=2)': 'mean',\n        'Combined': 'mean'\n    }).reset_index()\n    \n    print(\"=\" * 70)\n    print(\"RESULTS SUMMARY (Mean across folds)\")\n    print(\"=\" * 70)\n    \n    for exp_name in EXPERIMENTS.keys():\n        exp_data = agg_results[agg_results['Experiment'] == exp_name]\n        if len(exp_data) == 0:\n            continue\n            \n        print(f\"\\n{exp_name}: {EXPERIMENTS[exp_name]['name']}\")\n        print(\"-\" * 70)\n        \n        # Format as table similar to paper\n        model_order = ['CSC', 'FGR', 'RSF', 'DeepHit']\n        for model in model_order:\n            model_data = exp_data[exp_data['Model'] == model]\n            if len(model_data) == 0:\n                continue\n                \n            c_values = {}\n            for _, row in model_data.iterrows():\n                metric = row['Metric']\n                c_values[metric] = {\n                    'prepay': row['Prepay (k=1)'] * 100,\n                    'default': row['Default (k=2)'] * 100,\n                    'combined': row['Combined'] * 100,\n                }\n            \n            # Print row\n            c24 = c_values.get('C(24)', {}).get('combined', 0)\n            c48 = c_values.get('C(48)', {}).get('combined', 0)\n            c72 = c_values.get('C(72)', {}).get('combined', 0)\n            oc = c_values.get('ØC', {}).get('combined', 0)\n            \n            print(f\"{model:10} C(24)={c24:5.2f}  C(48)={c48:5.2f}  C(72)={c72:5.2f}  ØC={oc:5.2f}\")\nelse:\n    print(\"No results to display.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'agg_results' in dir() and len(agg_results) > 0:\n",
    "    # Create detailed table\n",
    "    print(\"\\n=== Detailed Results (C-index × 100) ===\")\n",
    "    \n",
    "    for exp_name in EXPERIMENTS.keys():\n",
    "        exp_data = agg_results[agg_results['Experiment'] == exp_name]\n",
    "        if len(exp_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{exp_name}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = exp_data.pivot(index='Model', columns='Metric')[['Prepay (k=1)', 'Default (k=2)', 'Combined']]\n",
    "        pivot = pivot * 100  # Convert to percentage\n",
    "        \n",
    "        # Reorder columns\n",
    "        cols_order = []\n",
    "        for metric in ['C(24)', 'C(48)', 'C(72)', 'ØC']:\n",
    "            for col in ['Prepay (k=1)', 'Default (k=2)', 'Combined']:\n",
    "                if (col, metric) in pivot.columns:\n",
    "                    cols_order.append((col, metric))\n",
    "        \n",
    "        pivot = pivot[cols_order]\n",
    "        print(pivot.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-results",
   "metadata": {},
   "outputs": [],
   "source": "if 'agg_results' in dir() and len(agg_results) > 0:\n    # Plot comparison\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n    \n    # Define colors for each model\n    model_colors = {'CSC': 'C0', 'FGR': 'C1', 'RSF': 'C2', 'DeepHit': 'C3'}\n    \n    for idx, exp_name in enumerate(EXPERIMENTS.keys()):\n        ax = axes[idx]\n        exp_data = agg_results[agg_results['Experiment'] == exp_name]\n        \n        if len(exp_data) == 0:\n            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n            ax.set_title(exp_name)\n            continue\n        \n        # Filter to time-specific metrics\n        time_data = exp_data[exp_data['Metric'].str.startswith('C(')]\n        \n        # Plot bars\n        models = ['CSC', 'FGR', 'RSF', 'DeepHit']\n        models = [m for m in models if m in time_data['Model'].values]\n        metrics = sorted(time_data['Metric'].unique())\n        x = np.arange(len(metrics))\n        width = 0.8 / len(models) if models else 0.2\n        \n        for i, model in enumerate(models):\n            model_data = time_data[time_data['Model'] == model].set_index('Metric')\n            values = [model_data.loc[m, 'Combined'] * 100 if m in model_data.index else 0 \n                      for m in metrics]\n            ax.bar(x + i * width, values, width, label=model, \n                   color=model_colors.get(model, f'C{i}'), alpha=0.8)\n        \n        ax.set_xlabel('Time Point')\n        ax.set_ylabel('C-index (×100)')\n        ax.set_title(f\"{exp_name}\\n{EXPERIMENTS[exp_name]['name']}\")\n        ax.set_xticks(x + width * (len(models) - 1) / 2)\n        ax.set_xticklabels(metrics)\n        ax.legend()\n        ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n        ax.set_ylim(40, 100)\n        ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('../reports/figures/model_comparison_blumenstock.png', dpi=150)\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-by-event",
   "metadata": {},
   "outputs": [],
   "source": "if 'agg_results' in dir() and len(agg_results) > 0:\n    # Plot by event type\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Define colors for each model\n    model_colors = {'CSC': 'C0', 'FGR': 'C1', 'RSF': 'C2', 'DeepHit': 'C3'}\n    \n    # Best experiment (4.3 if available)\n    exp_name = 'Exp 4.3' if 'Exp 4.3' in agg_results['Experiment'].values else agg_results['Experiment'].iloc[0]\n    exp_data = agg_results[agg_results['Experiment'] == exp_name]\n    time_data = exp_data[exp_data['Metric'].str.startswith('C(')]\n    \n    for idx, (col, title) in enumerate([('Prepay (k=1)', 'Prepayment'), ('Default (k=2)', 'Default')]):\n        ax = axes[idx]\n        \n        models = ['CSC', 'FGR', 'RSF', 'DeepHit']\n        models = [m for m in models if m in time_data['Model'].values]\n        metrics = sorted(time_data['Metric'].unique())\n        x = np.arange(len(metrics))\n        width = 0.8 / len(models) if models else 0.2\n        \n        for i, model in enumerate(models):\n            model_data = time_data[time_data['Model'] == model].set_index('Metric')\n            values = [model_data.loc[m, col] * 100 if m in model_data.index else 0 \n                      for m in metrics]\n            ax.bar(x + i * width, values, width, label=model,\n                   color=model_colors.get(model, f'C{i}'), alpha=0.8)\n        \n        ax.set_xlabel('Time Point')\n        ax.set_ylabel('C-index (×100)')\n        ax.set_title(f'{title} Risk Prediction ({exp_name})')\n        ax.set_xticks(x + width * (len(models) - 1) / 2)\n        ax.set_xticklabels(metrics)\n        ax.legend()\n        ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n        ax.set_ylim(40, 100)\n        ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('../reports/figures/model_comparison_by_event.png', dpi=150)\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    # Save detailed results\n",
    "    results_df.to_csv('../reports/blumenstock_results_detailed.csv', index=False)\n",
    "    print(\"Saved detailed results to reports/blumenstock_results_detailed.csv\")\n",
    "    \n",
    "    # Save aggregated results\n",
    "    agg_results.to_csv('../reports/blumenstock_results_summary.csv', index=False)\n",
    "    print(\"Saved summary results to reports/blumenstock_results_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## Conclusions\n\n### Key Findings (Expected based on paper)\n\n1. **DeepHit and RSF should outperform statistical models** - Deep learning and ML models typically achieve higher C-index\n\n2. **Default prediction is easier** - ØC₂ > ØC₁ typically (loan-level variables are strong predictors of default)\n\n3. **Macro variables help prepayment more than default** - Interest rate environment drives prepayment\n\n4. **Combined variables perform best** - Exp 4.3 should show highest ØC\n\n### Model Ranking (Paper's findings)\n\nFrom the paper, the expected ranking for Dataset 2 is:\n- **DeepHit** ≈ **RSF** > **FGR** > **CSC**\n\n### Implementation Notes\n\n- **DeepHit**: Uses pycox library with cause-specific network architecture\n- **RSF**: Cause-specific approach using scikit-survival\n- **FGR**: Discrete-time approximation with logistic regression\n- **CSC**: Semi-parametric Cox model from lifelines\n\n### References\n\n- Blumenstock et al. (2022): Main paper being replicated\n- Lee et al. (2018): DeepHit original paper\n- Ishwaran et al. (2014): RSF for competing risks"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}